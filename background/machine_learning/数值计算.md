<!-- markdownlint-disable MD033 MD041 MD045 -->

### 稳定数值

#### 下溢(underflow)

当一个接近0的很小的正数被四舍五入为0时发生下溢。

#### 上溢(overflow)

当一个很大的数被近似为$$\infty$$或$$-\infty$$时发生上溢。

#### softmax

社会科学家邓肯·卢斯于1959年在选择模型（choice model）的理论基础上发明的softmax函数正是这样做的：softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。
为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。

$$
softmax(x)_i = \frac{exp(x_i)}{\sum_{j=1}^n exp(x_j)}
$$

$$exp(x)$$：**指数函数**通常特指以e为底数的指数函数（即$$e^x$$）

$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$

对于所有的 $j$ 总有 $0 \leq \hat{y}_j \leq 1$。 因此，$\hat{\mathbf{y}}$可以视为一个正确的概率分布。softmax运算不会改变未规范化的预测之间的大小次序，只会确定分配给每个类别的概率。 因此，在预测过程中，我们仍然可以用下式来选择最有可能的类别。

$\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.$

尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个线性模型（linear model）。

### 优化方法

优化：改变$$x$$，以最小化或最大化函数$$f(x)$$的任务。这个函数被称为**目标函数(object function)**或**准则(criterion)**、**代价函数(cost function)**、**损失函数(loss function)**、**误差函数(error function)**。

**偏导数$$\frac{\partial}{\partial x_i}f(x)$$**：衡量点$$x$$处只有$$x_i$$增加时$$f(x)$$如何变化。

一个多变量的函数的偏导数（partial derivative）是它关于其中一个变量的导数，而保持其他变量恒定（相对于全导数，在其中所有变量都允许变化）。

**梯度(gradient)**：对一个向量求导，记作$$\nabla x f(x)$$

牛顿法 基本思想 通过使用$$f$$在$$x = a$$处的线性化来改善估算

假设$$a$$是方程$$f(x) = 0$$的解的一个近似，如果令$$b = a - \frac{f(a)}{f’(a)}$$，则在很多情况下，b是一个比a更好地近似

**无穷数列** $${a_n} = a_1,a_2,a_3,...$$

**无穷级数** $$\sum\limits_{n=1}^{\infty} {a_n} = a_1+a_2+a_3+...$$
