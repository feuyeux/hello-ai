<!-- markdownlint-disable MD033 MD041 -->

### 人工智能简史

#### 达特茅斯1956

约翰·麦肯锡(John McCarthy)

马文·明斯基(Marvin L. Minsky)

纳撒尼尔·罗切斯特(Nathaniel Rochester)

克劳德·香农(Cloud E. Shannon) 信息论之父

赫伯特·西蒙(Herbert Simon)

艾伦·纽维尔(Allen Newell)

#### 鸟飞派

斯图亚特·罗素、诺威格《人工智能：一种现代的方法》

明斯基《语义信息处理》分析了人工智能的局限性。The box was in the pen(钢笔/小孩玩耍的栅栏)。

#### 统计+数据

##### 费里德里克·贾里尼克(Frederick Jelinek)

数据驱动 语音识别

*Statistical Methods for Speech Recognition(Language,Speech,and Communication)*

##### 弗朗兹·奥科(Franz Och)

机器翻译

##### 米奇·马库斯(Mitch Marcus)

语料库组织(Linguistic Data Consortium, LDC)

数据的产生

- 计算机日志 用户产生的数据(UGC)
- 传感器 射频识别芯片(RFID)
- 过去存在的非数字化形式存储的信息

#### 机器学习学派(The five tribes of machine learning)

| 学派|描述             | 主算法     |
| --------|-------- | ---------- |
| 符号学派(symbolists) | 信息简化为操作符号。专家知识->编码->**规则** | 归纳法(induction)/逆向演绎(inverse deduction) |
| 联结学派(connectionists) | 学习参数。模拟大脑(**神经网络**/深度学习)，逐层改变神经元之间的连接，使输出的东西接近想要的。 | 反向传播(back propagation) |
| 进化学派(evolutionaries)\|鸟飞派 | 学习进化结构。 | 基因/遗传编程(genetic programming) |
| 贝叶斯学派(Bayesians) | 所有已掌握的知识都有不确定性，运用概率推理在不破坏信息的情况下，实现处理嘈杂、不完整、自相矛盾的信息。 | 贝叶斯定理及其衍生定理(Bayes’ theorem and its derivates) |
| 类推学派(analogizers) |在不同场景中，认识相似性。统计学习(statistical learning)，迁移学习(Transfer Learning)。| 支持向量机(support vector machine) |

#### 符号学派

**理性主义者**认为：**逻辑推理**是通往知识的唯一可靠的道路。代表：笛卡尔、斯宾诺莎、莱布尼茨

**经验主义者**认为：所有推理都不可靠，知识必须来源于**观察和实验**。代表：洛克、贝克莱、大卫·休谟

从专家身上提取知识，然后将其编码成规则。

#### 联结学派

##### 感知器(perceptroms)

弗兰克·罗森布拉特(Frank Rosenblatt) 1950年代发明

缺点：感知器只能学习线性问题，而XOR线性不可分。

##### NP完全

算法分析之父高德纳：算法复杂度与问题大小无关。

- P类问题：所有可以在多项式(Polynomial)时间内求解的判定问题构成P类问题。P类问题是计算机有效可解的。
- NP类问题：所有的非确定性多项式时间可解的判定问题构成NP类问题。非确定性算法：非确定性算法将问题分解成猜测和验证两个阶段。算法的猜测阶段是非确定性的，算法的验证阶段是确定性的，它验证猜测阶段给出解的正确性。找出围棋每一步的最佳走法就是NP问题。
- NP完全(NPC)/库克(Stephen Cook)-李文(Leonid Levin)定理：是NP(非决定性多项式时间)中最难的决定性问题。因此NP完备问题应该是最不可能被化简为P（多项式时间可决定）的决定性问题的集合。

##### 反向传播 1986

反向传播(Back Propagation,BP/Error Back Propagation)由心理学家大卫·鲁梅尔哈特在杰夫·辛顿、罗纳德·威廉斯的协助下发明。BP基于梯度下降(gradient descent)策略，以目标的负梯度方向对参数进行调整。

**杰弗里·辛顿(Geoffrey Hinton)**  

盖茨比计算神经科学中心(Gatsby Computational Neuroscience Unit)创始人 乔治·布尔(George Boole)的曾孙

反向传播算法和对比散度算法，深度信念网络(Deep Belief Network, DBN)

信号正向传播，误差反向传播

**伊恩·勒坤(Yann LeCun)**

卷积神经网络(CNN)推动者 第一个将BP算法用在CNN上

##### Bengio

**哈萨比斯(Hassabis)**

伦敦大学学院(UCL, University College London)神经科学博士 DeepMind创始人

##### 谷歌大脑(Google Brain) 2010

机器学习理论上没有突破，但从工程上，把人工神经网络并行地实现了。

2012年，Google收购DeepMind

AlphaGo

#### 进化学派

DNA4个基本碱基

- 腺嘌呤
- 胸腺嘧啶
- 胞嘧啶
- 鸟嘌呤

##### 遗传编程 1987

约翰·霍兰德的学生约翰·科扎发明遗传编程。1992年约翰·科扎出版《遗传编程》(Genetic Programming)。

- 结构学习：利用爬山法决定神经元之间如何连接
- 权值学习：利用反向传播学习连接的权值

#### [贝叶斯Bayesians](Bayesians.md)学派

#### 类推学派

《逍遥法外》弗兰克·阿巴内尔的骗术：找到和现在病人症状最相似的病人资料，然后做出相同诊断。

##### 类比推理法

- 最近邻算法/懒惰学习算法，k最近邻算法(k-nearest-neighbor, KNN algorithm)
- 高维度下，相似性会无效。维数灾难——**理查德·贝尔曼**
- 降维：摆脱不相关维度，丢弃所有信息增益低于阈值的属性。

##### 支持向量机(support vector machine)

弗拉基米尔·万普尼克

一种二分类模型。它的**基本模型**是定义在特征空间上的**间隔最大(maximum margin)**的线性分类器，间隔最大使它有别于感知机；支持向量机还包括**核技巧(kernel trick)**，这使其成为实质上的非线性分类器。

##### 支持向量

- 训练数据集中与分离超平面距离最近的样本点的实例称为支持向量
- 更通俗的解释：
  - 数据集种的某些点，位置比较特殊。比如 `x+y-2=0` 这条直线，假设出现在直线上方的样本记为 A 类，下方的记为 B 类。
  - 在寻找找这条直线的时候，一般只需看两类数据，它们各自最靠近划分直线的那些点，而其他的点起不了决定作用。
  - 这些点就是所谓的“支持点”，在数学中，这些点称为**向量**，所以更正式的名称为“**支持向量**”。

#### 迁移学习(Transfer Learning)

运用已有知识，对不同但相关领域问题进行求解的机器学习方法

#### 主成分分析(principal components analysis, PCA)
