<!-- markdownlint-disable MD033 MD041 -->

# 多层感知机(multilayer perceptron, MLP)

线性意味着单调假设:任何特征的增大都会导致模型输出的增大(如果对应的权重为正)，或者导致模型输出的减小(如果对应的权重为负)。

我们的数据可能会有一种表示，这种表示会考虑到我们在特征之间的相关交互作用。在此表示的基础上建立一个线性模型可能会是合适的，但我们不知道如何手动计算这么一种表示。对于深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。

我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。要做到这一点，最简单的方法是将许多全连接层堆叠在一起。每一层都输出到上面的层，直到生成最后的输出。我们可以把前L − 1层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机(multilayer perceptron)，通常缩写为MLP。

MLP(multilayer perceptron)

An MLP is composed of one input layer, one or more layers of TLUs called *hidden layers*, and one final layer of TLUs called the *output layer* (see Figure 10-7). The layers close to the input layer are usually called the *lower layers*, and the ones close to the outputs are usually called the *upper layers*.

Backpropagation

reverse-mode automatic differentiation+gradient descent

- *The hyperbolic tangent function: tanh(z) = 2σ(2z) – 1*
- The rectified linear unit function: ReLU(z) = max(0, z)
