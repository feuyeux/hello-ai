# 激活函数(Activation Functions)

## 修正线性单元(Rectified linear unit，ReLU)

$\operatorname{ReLU}(x) = \max(x, 0).$

ReLU函数有许多变体，包括参数化ReLU（Parameterized ReLU，pReLU）函数。 该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：

$\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).$

## Sigmoid or Logistic 函数

sigmoid通常称为挤压函数（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值

$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$

在最早的神经网络中，科学家们感兴趣的是对“激发”或“不激发”的生物神经元进行建模。 因此，这一领域的先驱可以一直追溯到人工神经元的发明者麦卡洛克和皮茨，他们专注于阈值单元。 阈值单元在其输入低于某个阈值时取值0，当输入超过阈值时取值1。

当人们逐渐关注到到基于梯度的学习时， sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。 当我们想要将输出视作二元分类问题的概率时， sigmoid仍然被广泛用作输出单元上的激活函数 （sigmoid可以视为softmax的特例）。 然而，sigmoid在隐藏层中已经较少使用， 它在大部分时候被更简单、更容易训练的ReLU所取代。

## 双曲正切函数(Tanh or hyperbolic tangent)

将其输入压缩转换到区间(-1, 1)上

$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.$

注意，当输入在0附近时，tanh函数接近线性变换。 函数的形状类似于sigmoid函数， 不同的是tanh函数关于坐标系原点中心对称。

tanh函数的导数是：

$\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).$

tanh函数的导数图像如下所示。 当输入接近0时，tanh函数的导数接近最大值1。 与我们在sigmoid函数图像中看到的类似， 输入在任一方向上越远离0点，导数越接近0。
