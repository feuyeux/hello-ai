<!-- markdownlint-disable MD033 MD036 MD041 MD056 -->

# Machine Learning

## 统计学习方法三要素

方法=模型+策略+算法

### 模型(model)

在监督学习过程中，模型就是所要学习的**条件概率分布**或**决策函数**。

模型的假设空间（hypothesis space） 包含所有可能的条件概率分布或决策函数。

|   模型           | 假设空间$\cal F$                                             | 输入空间$\cal X$ | 输出空间$\cal Y$ | 参数空间      |
| ------------ | ------------------------------------------------------------ | ---------------- | ---------------- | ------------- |
| 非概率模型 决策函数     | $\cal F\it =\{f_{\theta} |Y=f_{\theta}(x), \theta \in \bf R \it ^n\}$ | 变量             | 变量             | $\bf R\it ^n$ |
| 概率模型 条件概率分布 | $\cal F\it =\{P|P_{\theta}(Y|X),\theta\in \bf R \it ^n\}$    | 随机变量         | 随机变量        | $\bf R\it ^n$ |

### 策略(strategy)

#### 损失函数与风险函数

- **损失函数(loss function)或代价函数(cost function)**度量模型**一次预测**的好坏。损失函数是给定输入$X$的**预测值$f(X)$**和**真实值$Y$**之间的**非负实值**函数，记作$L(Y,f(X))$。
- **风险函数(risk function)或期望损失(expected loss)**度量**平均意义**下模型预测的好坏。和模型的泛化误差的形式一样：
  $$
  R_{exp}(f)=E_p[L(Y, f(X))]=\int_{\mathcal X\times\mathcal Y}L(y,f(x))P(x,y)\, {\rm d}x{\rm d}y
  $$
  
  模型$f(X)$关于联合分布$P(X,Y)$的**平均意义下的**损失(**期望**损失)，因为$P(X,Y)$是未知的，所以需要学习，用词是(平均意义下的)**期望**。

#### 经验风险

模型$f(X)$关于训练数据集的平均损失称为**经验风险(empirical risk)**或**经验损失 (empirical loss)**，记作
$$
R_{emp}(f)=\frac{1}{N}\sum^{N}_{i=1}L(y_i,f(x_i))
$$

- $R_{exp}(f)$是模型关于联合分布的期望损失
- $R_{emp}(f)$是模型关于训练样本集的平均损失

#### 经验风险最小化

经验风险最小化(empirical risk minimization, **ERM**)的策略认为，经验风险最小的模型是最优的模型。根据这一策略，按照经验风险最小化求最优模型就是求解最优化问题：

$$
\min_{f\in\mathcal{F}}\frac{1}{N}\sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)
$$

**极大似然估计**(maximum likelihood estimation)就是经验风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。

#### 结构风险最小化

结构风险最小化(structural risk minimization, **SRM**)是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化(regularization)。结构风险在经验风险上加上表示模型复杂度的正则化项(regularizer)或罚项(penalty term)。在假设空间、损失函数以及训练数据集确定的情况下，结构风险的定义是：

$$
R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)
$$

其中$J(f)$为模型复杂度，是定义在假设空间$\cal F$上的泛函。$\lambda \geqslant 0$是系数，用以权衡经验风险和模型复杂度。

**贝叶斯估计**中的最大后验概率估计(maximum posterior probability esti­mation, MAP)就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。

结构风险最小化的策略认为结构风险最小的模型是最优的模型。所以求最优模型，就是求解最优化问题：
$$
\min_{f\in\mathcal{F}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)
$$

监督学习问题就变成了经验风险或结构风险函数的最优化问题。这时经验或结构风险函数是最优化的目标函数。

### 算法(algorithm)

基于训练数据集，根据学习策略,
从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。

## 监督学习

- 生成模型(Generative model)
- 判别模型(Discriminative model)

| 方法                     |           | 适用问题         | 模型特点                             | 模型类型 | 学习策略                               | 学习的损失函数     | 学习算法             |
| ------------------------ | --------- | ---------------- | ------------------------------------ | -------- | -------------------------------------- | :----------------- | -------------------- |
| 感知机                   | Peceptron | 二类分类         | 分离超平面                           | 判别模型 | 极小化误分点到超平面距离               | 误分点到超平面距离 | 随机梯度下降(SGD)          |
| k近邻法                  | KNN       | 多类分类, 回归   | 特征空间, 样本点                     | 判别模型 |                                        |                    |                      |
| 朴素贝叶斯法             | NB        | 多类分类         | 特征与类别的联合耨率分布，条件独立假设 | 生成模型 | 极大似然估计(MLE)，最大后验概率估计(MAP) | 对数似然损失       | 概率计算公式, EM算法 |
| 决策树                   | DT        | 多类分类，回归    | 分类树，回归树 | 判别模型 | 正则化的极大似然估计                   | 对数似然损失       | 特征选择, 生成, 剪枝 |
| 逻辑斯谛回归与最大熵模型 | LR Maxent | 多类分类         | 特征条件下类别的条件概率分布,对数线形模型 | 判别模型 | 极大似然估计，正则化的极大似然估计 | 逻辑斯谛损失 | 改进的迭代尺度算法，梯度下降，拟牛顿法 |
| 支持向量机               | SVM       | 二类分类         | 分离超平面，核技巧 | 判别模型 | 极小化正则化合页损失，软间隔最大化 | 合页损失 | 序列最小最优化算法（SMO） |
| 提升方法                 | AdaBoost  | 二类分类         | 弱分类器的线性组合 | 判别模型 | 极小化加法模型的指数损失 | 指数损失 | 前向分步加法算法 |
| EM算法                   | EM        | 概率模型参数估计 | 含隐变量的概率模型                   |          | 极大似然估计(MLE)，最大后验概率估计(MAP) | 对数似然损失 | 迭代算法 |
| 隐马尔可夫模型           | HMM       | 标注             | 观测序列与状态序列的联合概率分布模型 | 生成模型 | 极大似然估计(MLE)，最大后验概率估计(MAP) | 对数似然损失 | 概率计算公式，EM算法 |
| 条件随机场               | CRF       | 标注             | 状态序列条件下观测序列的条件概率分布，对数线性模型 | 判别模型 | 极大似然估计，正则化的极大似然估计 | 对数似然损失 | 改进的迭代尺度算法，梯度下降，拟牛顿法 |

> EM(Expectation-Maximization 期望最大化)算法的基本思想是通过迭代E步骤（计算隐变量期望）和M步骤（根据期望优化参数），逐步提高模型似然性，直至参数稳定。

### 1 回归(Regression)

由果索因的过程，是一种归纳思想。**预测一个连续值**。也就是说我们试图将输入变量和输出用一个连续函数对应起来。

把平面上一系列的点，用一条光滑曲线连接起来的过程，叫拟合。

过拟合(over fitting)：在归纳过程中，为了迎合所有样本向量点甚至是噪声点而使模型描述过于复杂，失去泛化能力(Generalization ability)。

欠拟合(under fitting)：模型拟合程度不高，数据距离拟合曲线较远，或指模型没有很好地捕捉到数据特征，不能够很好地拟合数据。

### 2 分类(Classification)

预测一个离散值，我们试图将输入变量与离散的类别对应起来。

- Bayes
- 决策树
- 随机森林
- HMM
- SVM

## 非监督学习

| 方法     |              | 模型                 | 策略                 | 算法                |
| -------- | ------------ | -------------------- | -------------------- | ------------------- |
| 聚类     | 层次聚类     | 聚类树               | 类内样本距离最小     | 启发式算法          |
|          | k均值聚类    | k中心聚类            | 样本与类中心距离最小 | 迭代算法            |
|          | 高斯混合模型 | 高斯混合模型         | 似然函数最大         | EM算法              |
| 降维     | PCA          | 低维正交空间         | 方差最大             | SVD                 |
| 话题分析 | LSA          | 矩阵分解模型         | 平方损失最小         | SVD                 |
|          | NMF          | 矩阵分解模型         | 平方损失最小         | 非负矩阵分解        |
|          | PLSA         | PLSA模型             | 似然函数最大         | EM算法              |
|          | LDA          | LDA模型              | 后验概率估计         | 吉布斯抽样,变分推理 |
| 图分析   | PageRank     | 有向图上的马尔可夫链 | 平稳分布求解         | 幂法                |

- 聚类 *Clustering*
- *Anomaly detection (also called outlier detection)*
- *Density estimation*

## 强化学习

RL不给定标注，而是给定回报函数，回报函数决定当前状态的结果。数学本质是一个马尔可夫决策过程

<https://www.coursera.org/learn/machine-learning/lecture/db3jS/model-representation>

Cost Function

$$
J(\theta_0, \theta_1) = \frac {1}{2m} \sum _{i=1}^m ( \hat{y}_{i}- y_{i})^2 = \dfrac {1}{2m}  \sum _{i=1}^m (h_\theta (x_{i}) - y_{i})^2
$$
