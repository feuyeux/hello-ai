<!-- markdownlint-disable MD033 MD041 MD045 -->

### Mean|Median|Mode

```python
from scipy import stats
import numpy
speed = [99, 86, 87, 88, 111, 86, 103, 87, 94, 78, 77, 85, 86]
x = numpy.mean(speed)
print('Mean(均值、算数平均数)', x)
# 77, 78, 85, 86, 86, 86, 87, 87, 88, 94, 99, 103, 111
x = numpy.median(speed)
print('Median(中值)', x)
# Mode(众数) 众数值是出现次数最多的值
x = stats.mode(speed)
print('Mode(众数)', x)

# # # #
# Mean(均值、算数平均数) 89.76923076923077
# Median(中值) 87.0
# Mode(众数) ModeResult(mode=array([86]), count=array([3]))

```

众数(Mode)是一组数据中出现次数最多的数值。

中位数(Median)又称中值，对于有限的数集，可以通过把所有观察值高低排序后找出正中间的一个作为中位数。如果观察值有偶数个，通常取最中间的两个数值的平均数作为中位数。

### 同比和环比

- 同比(year-on-year/year-over-year)是指上年同期，是**一个单位周期内**和另一个更大周期的**同一个段周期内**的变化比

- 环比(month-on-month) 是指本年上期，是**连续2个单位周期内**的量的变化比

举例来说，2021.9月，同比是与2020.9月比，环比是与2021.8月比。

### 正态分布

正态分布(Normal distribution)，也称“常态分布”，高斯分布(Gaussian distribution)。最早由德国数学家高斯（Gauss）应用于天文学研究。 简单的说，若随机变量$x$具有均值$\mu$和方差$\sigma^2$（标准差$\sigma$）

$$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).$$

### 平方损失

改变均值会产生沿$x$轴的偏移，增加方差将会分散分布、降低其峰值。

均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是：我们假设了观测中包含噪声，其中噪声服从正态分布。噪声正态分布如下式:

$y = \mathbf{w}^\top \mathbf{x} + b + \epsilon,$

其中，$\epsilon \sim \mathcal{N}(0, \sigma^2)$。

因此，我们现在可以写出通过给定的$\mathbf{x}$观测到特定$y$的似然（likelihood）：

$P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).$

现在，根据极大似然估计法，参数$\mathbf{w}$和$b$的最优值是使整个数据集的似然最大的值：

$P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)}).$

根据极大似然估计法选择的估计量称为极大似然估计量。 虽然使许多指数函数的乘积最大化看起来很困难， 但是我们可以在不改变目标的前提下，通过最大化似然对数来简化。 由于历史原因，优化通常是说最小化而不是最大化。 我们可以改为最小化负对数似然$-\log P(\mathbf y \mid \mathbf X)$。 由此可以得到的数学公式是：

$-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.$

现在我们只需要假设$\sigma$是某个固定常数就可以忽略第一项， 因为第一项不依赖于$\mathbf{w}$和$b$。 现在第二项除了常数$\frac{1}{\sigma^2}$外，其余部分和前面介绍的均方误差是一样的。 幸运的是，上面式子的解并不依赖于$\sigma$。 因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。

### 泊松分布

Poisson分布(Poisson distribution)，又称泊松小数法则（Poisson law of small numbers），是一种统计与概率学里常见到的离散概率分布，由法国数学家西莫恩·德尼·泊松（Siméon-Denis Poisson）在1838年时发表。

$$ P(X=k)=\frac{e^{-\lambda}\lambda^k}{k!} $$

### 伯努利分布

伯努利分布(Bernoulli distribution)，又名两点分布或者0-1分布，是一个离散型概率分布。
$$  f_{X}(x)=p^{x}(1-p)^{1-x}=\left\{{\begin{matrix}p&{\mbox{if }}x=1,\\q\ &{\mbox{if }}x=0.\\\end{matrix}}\right.$$

### Error(误差)

Error(误差)：实际预测输出与样本的真实输出之间的差异

训练集上的误差称为训练误差(training error)，或经验误差(empirial error)

新样本上的误差称为泛化误差(generalization error)

泛化误差=偏差+方差+噪声

### 偏差与方差

#### 1 偏差(Error due to Bias)

> Bias measures how far off in general these models' predictions are from the correct value.

**Bias(偏差)**：度量了学习算法的**期望预测**与**真实结果**的**偏离程度**，即刻画了**学习算法本身的拟合能力**。【描述准确度】

偏差越低(low bias)表现越好，模型越复杂化，要增加模型的参数，越容易导致**过拟合 (overfitting)**。

#### 2 方差(Error due to Variance)

> The variance is how much the predictions for a given point vary between different realizations of the model.

**Variance(方差)**：度量了同样大小的**训练集的变动**所导致的**学习性能**的变化，即刻画了**数据扰动所造成的影响**。【描述分散度】

方差越低(low varience)表现越好，模型越简化，要减少模型的参数，越容易**欠拟合(unfitting)**。

<img src="../img/bias_variance.png" width="500"/>

右上过拟合：high variance，点很分散；low bias，都打在靶心附近，瞄得准但手不**稳**。

左下欠拟合：high bias，点偏离中心；low variance，打得很集中但不在靶心附近，手很稳但瞄不**准**。

#### 3 误差与模型复杂度

<img src="../img/bias_variance_2.png" width="500"/>

泛化性能(Generalization Performance)好：**偏差较小，既能够充分拟合数据**，且**方差较小，即使得数据扰动产生的影响小**。

### 方差

$$
\sigma^2=\frac{1}{N}{\sum_{i=1}^N(x_i-\mu)^2}
$$
σ<sup>2</sup>为总体方差，X为变量，μ为总体**均值**，N为总体例数

实际工作中，总体均数难以得到时，应用样本统计量代替总体参数，经校正后，样本方差计算公式：
$$
S^2=\frac{1}{N}{\sum_{i=1}^N(x_i-\bar x)^2}
$$
S<sup>2</sup>为样本方差，X为变量，$\bar X$为样本均值，n为样本例数

### 标准差

$$
\sigma=\sqrt{\frac{1}{N}{\sum_{i=1}^N(x_i-\bar{x})^2}}
$$

**标准差**(Standard Deviation)又常称**均方差**，是方差的算术平方根。标准差能反映一个数据集内<u>个体间的离散程度</u>。

例如，两组数的集合{0,5,9,14}和{5,6,8,9}其平均值都是7，但第二个集合具有较小的标准差。

方差与均方差：一个班男生的平均身高是170cm，均方差是10cm，可以简便的描述为本班男生身高分布在170±10cm，方差就无法做到这点。

### 均方误差

$$
MSE=\frac{1}{n}{\sum_{i=1}^{n}(f(x_i)-y_i)^2}
$$

**均方误差**(Mean Square Error, **MSE**)是预测值与真实值之差的平方和的平均值。

均方误差的开方叫**均方根误差（Root Mean Square Error）**，**均方根误差**和标准差形式上接近。

方差和均方误差形式上接近，但物理意义不同：

- 方差：是各数据偏离**平均值（期望）** 差值的平方和 的平均数

- 均方误差：是各数据偏离**真实值**  差值的平方和 的平均数

### MSE与偏差、方差

$$
MSE(\tilde\theta_m)=E[(\tilde\theta_m-\theta)^2]={Bias}(\tilde\theta_m)^2+{Var}(\tilde\theta_m)
$$
$$
{Bias}(\tilde\theta_m)=E(\tilde\theta_m)-\theta
$$
$$
{Var}(\tilde\theta_m)=E[(\tilde\theta_m-E(\tilde\theta_m))^2]
$$

$\tilde\theta_m$是估计值，$\theta$是真实值

### 混淆矩阵

m个样本中有a个样本分类错误

错误率 E = a/m

准确率（accuracy） acc = 1 - a/m

|真实情况|预测结果|预测结果|
|:--|:--|:--|
||正例|反例|
|正例(Positive)|真正例(TP)|假反例(FN) 漏报|
|反例(Negative)|假证例(FP) 误报|真反例(TN)|

#### P-R曲线

##### **精确率**（precision）查准率

$$
P = \frac {TP}{TP+FP}
$$

##### **召回率**（recall, sensitivity, true positive rate）查全率

$$
R = \frac {TP} {TP+FN}
$$

##### **F1值**——精确率和召回率的调和均值

> 只有当精确率和召回率都很高时，F1值才会高

$$
F1 = \frac {2 \times  P \times R} {P + R} = \frac {2 \times TP} { 样例总数 + TP - TN}
$$

### ROC

ROC(Receiver Operating Characteristic)受试者工作特征

AUC(Area Under ROC Cure) ROC下曲线面积

#### TPR(True Positive Rate)真正例率

$$
TPR = \frac {TP}{TP+FN}
$$

#### FPR(False Positive Rate) 假正例率

$$
FPR = \frac {FP}{TN+FP}
$$
