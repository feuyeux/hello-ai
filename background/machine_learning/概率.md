<!-- markdownlint-disable MD033 MD036 MD041 MD045 -->

#### 贝叶斯[Bayesians]

托马斯·贝叶斯(Thomas Bayes，1701-1761)

皮埃尔-西蒙·拉普拉斯
$$P(A|B) = \frac{P(A)P(B)}{P(B)}$$
$$P(原因|结果) = \frac{P(原因)P(结果|原因)}{P(结果)}$$

把假设当作可能的原因，把数据当作观察到的效果
$$P(假设|数据) = \frac{P(假设)P(数据|假设)}{P(数据)}$$
$$P(感冒|发烧) = \frac{P(感冒)P(发烧|感冒)}{P(发烧)}$$

#### 极大释然法(Maximum likelihood principle)

抛100次硬币，正面朝上70次，则正面朝上的概率为0.7

- 大卫-麦凯 如何通过贝叶斯法来学习多层感知器
- 雷德福·尼尔 MCMC
- 迈克尔·乔丹 变分推理

#### 似然

是对likelihood 的一种较为贴近文言文的翻译，“似然”用现代的中文来说即“可能性”。

| 符号 | 含义 |
| ---- | ---- |
| $P (O \| \theta)$ | 概率 |
| $L(\theta \| O) $ | 释然 |

- **“概率”描述了给定模型参数后，描述结果的合理性，而不涉及任何观察到的数据。**

> 抛一枚均匀的硬币，拋20次，问15次拋得正面的可能性有多大？ 这里的可能性就是”概率”，均匀的硬币就是给定参数$\theta=0.5$，“拋20次15次正面”是观测值$O$。求概率$P (H=15 | \theta=0.5) = ？$的概率。

- **“似然”描述了给定了特定观测值后，描述模型参数是否合理。**

> 拋一枚硬币，拋20次，结果15次正面向上，问其为均匀的可能性？ 这里的可能性就是”似然”，“拋20次15次正面”为观测值$O$为已知，参数$\theta=?$并不知道，求$L(\theta | H=15) = P (H=15 | \theta=0.5)$的最大化下的$\theta$ 值。

输入有两个：$O$表示某一个具体的数据；$\theta$表示模型的参数。

- 如果$\theta$是已知确定的，$O$是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本$O$，其出现概率是多少。
- 如果$O$是已知确定的，$\theta$是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现x这个样本点的概率是多少。

#### 先验概率与后验概率

**条件概率**（似然概率）

- 一个事件发生后另一个事件发生的概率。
- 一般的形式为 `P(X|Y)`，表示 y 发生的条件下 x 发生的概率。
- 有时为了区分一般意义上的**条件概率**，也称**似然概率**

**先验概率**

- 事件发生前的预判概率
- 可以是基于历史数据的统计，可以由背景常识得出，也可以是人的主观观点给出。
- 一般都是**单独事件**发生的概率，如 `P(A)`、`P(B)`。

**后验概率**

- 基于先验概率求得的**反向条件概率**，形式上与条件概率相同（若 `P(X|Y)` 为正向，则 `P(Y|X)` 为反向）

#### 马尔可夫链(Markov Chain)

随机过程：在任何一个时刻t，对应的状态$$s_t$$都是随机的。

马尔可夫提出一种简化的假设：随机过程中各个状态$$s_t$$的概率分布，只与它的前序状态$$s_{t-1}$$有关，即$$P(s_t|s_1,s_2,s_3,\cdots,s_{t-1})=P(s_t|s_{t-1})$$。

符合马尔可夫假设的随机过程成为**马尔可夫链**。

#### 隐马尔可夫模型(Hidden Markov Model, HMM)

是马尔可夫链的扩展：任一时刻t的状态$$s_t$$是不可见的，但每个时刻t会输出一个$$o_t$$，$$o_t$$与$$s_t$$相关且仅与$$s_t$$相关(独立输出假设)。

$$P(s_1,s_2,s_3,\cdots,o_1,o_2,o_3,\cdots) = \displaystyle \prod_t{P(s_t|s_{t-1}) \cdot P(o_t|s_t)}$$

##### **转移概率(Transition Probability)**

从前一个状态$s_{t-1}$进入当前状态$s_t$的概率$P(s_t|s_{t-1})$
$$
P(s_t|s_{t-1}) = \frac {P(s_{t-1},s_t)}{P(s_{t-1})}
$$

##### **生成概率(Generation Probability)**

每个状态$s_t$产生相应输出符号$o_t$的概率$P(o_t|s_t)$

有监督训练：通过人工标注(Human Annotated)数据。
$$
P(o_t|s_t) = \frac {P(o_t,s_t)}{P(s_t)} \approx \frac{\#(o_t,s_t)}{\#(s_t)}
$$

无监督训练：通过大量观测$o_1,o_2,o_3,\cdots$，使用鲍姆-韦尔奇算法(Baum-Welch Algorithm)推算$P(s_t|s_{t-1})$和$P(o_t|s_t)$。

#### 马尔可夫决策过程(Markov Decision Process, MDP)

也具有马尔可夫性，与上面不同的是MDP考虑了动作，即系统下个状态不仅和当前的状态有关，也和当前采取的动作有关。还是举下棋的例子，当我们在某个局面（状态s）走了一步(动作a)，这时对手的选择（导致下个状态s’）我们是不能确定的，但是他的选择只和s和a有关，而不用考虑更早之前的状态和动作，即s’是根据s和a随机生成的。

我们用一个二维表格表示一下，各种马尔可夫子模型的关系就很清楚了：

|                | 不考虑动作          | 考虑动作                                |
| :------------- | :------------------ | --------------------------------------- |
| 状态完全可见   | 马尔科夫链(MC)      | 马尔可夫决策过程(MDP)                   |
| 状态不完全可见 | 隐马尔可夫模型(HMM) | 不完全可观察马尔可夫决策过程(**POMDP**) |

一个马尔可夫决策过程由一个四元组构成M = (S, A, Psa, 𝑅)

- S: 表示状态集(states)，有s∈S，si表示第i步的状态。
- A:表示一组动作(actions)，有a∈A，ai表示第i步的动作。
- 𝑃sa: 表示状态转移概率。𝑃s𝑎 表示的是在当前s ∈ S状态下，经过a ∈ A作用后，会转移到的其他状态的概率分布情况。比如，在状态s下执行动作a，转移到s'的概率可以表示为p(s'|s,a)。
- R: S×A⟼ℝ ，R是回报函数(reward function)。有些回报函数状态S的函数，可以简化为R: S⟼ℝ。如果一组(s,a)转移到了下个状态s'，那么回报函数可记为r(s'|s, a)。如果(s,a)对应的下个状态s'是唯一的，那么回报函数也可以记为r(s,a)。

MDP 的动态过程如下：某个智能体(agent)的初始状态为s0，然后从 A 中挑选一个动作a0执行，执行后，agent 按Psa概率随机转移到了下一个s1状态，s1∈ Ps0a0。然后再执行一个动作a1，就转移到了s2，接下来再执行a2…，我们可以用下面的图表示状态转移的过程。
![](https://cdn.nlark.com/lark/0/2018/jpeg/9038/1538068746480-4f009c33-ccc7-4b07-aa75-0cde0124323b.jpeg)

如果回报r是根据状态s和动作a得到的，则MDP还可以表示成下图：
![](https://cdn.nlark.com/lark/0/2018/jpeg/9038/1538068754829-1d6d68aa-f8dc-4e69-8cc7-202244b21c54.jpeg)

#### 部分可观察马尔可夫决策过程POMDP

POMDP是一种通用化的马尔可夫决策过程。POMDP模拟代理人决策程序是假设系统动态由MDP决定，但是代理人无法直接观察目前的状态。相反的，它必须要根据模型的全域与部分区域观察结果来推断状态的分布。
因为POMDP架构的通用程度足以模拟不同的真实世界的连续过程，应用于机器人导航问题、机械维护和不定性规划。架构最早由研究机构所建立，随后人工智能与自动规划社群继续发展。
离散时间POMDP模拟代理与其环境之间的关系。 形式上，POMDP是7元组`(S,A,T,R,Ω,Ο,Υ)`，其中

- `S`是一组状态，
- `A`是一组动作，
- `T`是状态之间的一组条件转移概率，
- `R:SxA-IR`是奖励函数。
- `Ω`是一组观察，
- `Ο`是一组条件观察概率，
- `Υε[0,1]`是折扣因子。
  在每个时间段，环境处于某种状态s ε S.The agent在A中采取动作a ε A，这会导致转换到状态s'的环境概率为T(s'|s,a)。同时，代理接收观察ο ε Ω，它取决于环境的新状态，概率为Ο(ο|s',a)。最后，代理接收奖励r等于R(s,a)。然后重复该过程。目标是让代理人在每个时间步骤选择最大化其预期未来折扣奖励的行动：![](https://cdn.nlark.com/lark/0/2018/jpeg/9038/1538069381292-8f7a2b51-1744-4540-a6d8-e0acae81dc70.jpeg) 。折扣系数γ决定了对更远距离的奖励有多大的直接奖励。当γ=0时，代理人只关心哪个动作会产生最大的预期即时奖励;当γ=1时，代理人关心最大化未来奖励的预期总和。

#### 马尔可夫链蒙特卡洛理论(Markov chain Monte Carlo,MCMC)

随机模拟(或者统计模拟)方法。

蒙特卡洛方法也称蒙特卡洛抽样方法，其基本思想是通过大量取样来近似得到想要的答案。有一个经典的试验就是计算圆周率

#### EM 1977

k均值算法(k-means algorithm) 大多数的熊是这样，那这应该是熊。适用于群体易于区分的情况下。

亚瑟·邓普斯特

南·莱尔德

唐纳德·鲁宾

##### 期望最大化演算法(Expectation Maximization,EM)

E 期望(推断预期的概率)

M 最大化(估算可能性最大的参数)

不能保证找到全局最优点

#### 贝叶斯网络(Beyesian Network, BN)

也被称为信念网络(Belif Networks)或者因果网络(Causal Networks)

##### 有向无环图(Directed Acyclic Graph, DAG)

![20151123212843008.png](https://cdn.nlark.com/lark/0/2018/png/9038/1538048924744-d82aebda-e82b-41e2-b7cf-b74ec65a7095.png)

##### 条件概率表

![](https://cdn.nlark.com/lark/0/2018/png/9038/1538049101224-d9ac1dea-bd50-43b0-bd53-2c8c692e2fc4.png)

#### 条件随机场(Conditional Random Fields, CRF)

无向图(Directed Acyclic Graph, DAG)

是马尔可夫模型的扩展，与前后状态都有关。是一种特殊的概率图模型(Probabilistic Graphical Model, PGM)。

#### 分类器

朴素贝叶斯网络(Naive Bayesian Networks, NBN)

通用贝叶斯网络（General Bayesian Networks, GBN）

增强型朴素贝叶斯网络（Tree-Augmented Naive Bayes, TAN）

马尔科夫毯贝叶斯网络（Markov Blanket Bayesian Networks, MBBN）

动态贝叶斯网络(Dynamic Bayesian Network, DBN)
