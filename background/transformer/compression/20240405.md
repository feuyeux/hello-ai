# Interence Optimization

<https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/>

<https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices>

<https://lilianweng.github.io/posts/2023-01-10-inference-optimization/>

- Prefill phase or processing the input
- Decode phase or generating the output

## LLM Benchmark

- Thoughtput 吞吐量
- First Token Latency 首字延迟
- Latency 延迟
- QPS 每秒请求数

分组量化 group quantize
