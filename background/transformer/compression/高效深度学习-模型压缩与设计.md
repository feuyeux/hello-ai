<!-- markdownlint-disable MD033 MD036 MD045 -->

# 高效深度学习:模型压缩与设计

## 1 绪论

### 硬件性能指标

可用性、实用性：

- 延时(Latency)

- 吞吐率(Throughput)

散热能力：

- 功率(Power)
- 热设计功耗(Thermal Design Power, TDP)，即最大负荷的能量释放

成本：

- 能耗(Energy)

- 存储(Storage)

### 模型延时公式

$$
L=\frac{W}{PU}
$$

> $Latency=\frac{Workload}{Peak\,computing\,performance \times Utilization}$
>
> $延时=\frac{计算负载}{硬件峰值计算性能\times硬件计算单元实际利用率}$

**示例** 在6张 RTX 3090 Ti (半精度峰值 184TFLOPS)上，解码一个词块的延时：$L\approx\frac{0.137 \, TFLOPs}{6\times184\times0.1\%}$

### 优化模型延时

- 降低 计算负载 **W**
- 提升 实际利用率 **U**

**数据表示压缩**，使用低位宽数据格式(低精度)表示权重和激活值，降低计算、访存、存储的代价 —— 模型量化(Model Quatization)

**模型结构压缩**

- 实现更低的计算负载，更高的运行时利用率
  - 模型剪枝(Model Pruning)
  - 近似低秩分解(Low-Rank Factorization)
  - 神经网络架构搜索(Neural Architecture Search, NAS)
- 保持算法性能
  - 动态推理(Dynamic Inference)
  - 结构重参数化(Structual Reparametrization)
  - 知识蒸馏(Knowledge Distillation)

## 2 基础知识

### 优化问题

$$
x^*=\mathop{\arg\min}\limits_{x \in A}f(x) \\
s.t.\quad g_i(x)\leq0,\, i=1,2,\dots,K
$$

- $x$：优化变量(Optimization Variable)
- A：优化空间(Optimization Space)
- $f(\cdot):A\rightarrow\mathbb{R}$：目标函数(Objective Function)
- $g_i(\cdot):A\rightarrow\mathbb{R},i=1,2,\dots,K$：K个约束函数(Constraint Function)

优化问题的可行域(Feasible Set) M=\{x|g_i(x)\leq0,1,\dots,K\} \subseteq A$：满足约束函数的$x$构成的集合。

优化问题的求解目标是在可行域里，找到让$f(x)$值最小的$x^*$

> - `arg min`：最小值的参数(argument of the minimum)。使函数取得最小值的自变量值。
> - `s.t.` ：约束条件(subject to)

### 卷积神经网络模型

| 模型                                                         | 作者                                                         | 年代                                                         | 贡献                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| AlexNet                                                      | 多伦多大学<br/>Alex Krizhevsky<br/>Ilya Sutskever<br/>Geoffrey Hinton | 2012年 ImageNet 图像识别 冠军                                | ReLU激活函数<br/>随机舍弃(dropout)<br/>局部响应归一化(Local Response Normalization,LRN) |
| VGGNet                                                       | 牛津大学<br/>视觉几何组(Visual Geomethry Group,VGG)          | 2014年 ImageNet 图像识别 冠军                                | 使用小型卷积核串联代替大卷积核，显著较少参数量<br/>增加模型深度，显著提升性能 |
| [GoogLeNet](https://github.com/deep-learning-algorithm/GoogLeNet)<br/>Inception-V1 | Google研究员Christian Szegedy等                              | 2014年 ImageNet 图像分类 冠军                                | 引入Inception模块，包含多个冰箱卷积网络分支，每个分支有不同的感受野，是模型可以同时学习不同尺度的特征 |
| ResNet                                                       | 微软亚洲研究院<br/>何恺明、孙剑、任少卿和张祥雨              | 2015年 ImageNet 图像识别 冠军<br/>2016年 CVPR 最佳论文奖<br/>《Deep Residual Learning for Image Recognition》 | 引入残差连接(Residual Connection)，有效解决了深层网络训练中的梯度消失问题 |
| DenseNet                                                     | Gao Huang<br/>Zhuang Liu<br/>Laurens van der Maaten<br/>Kilian Q. Weinberger | 2017年 CVPR 最佳论文奖<br/>《Densely Connected Convolutional Networks》 | DenseNet的设计灵感来源于ResNet，但与ResNet的跳跃连接(Skip Connection)不同，DenseNet通过特征重用和稠密连接方式，有效地减少了模型的参数量和计算量，同时提高了模型的泛化能力 |
| [Deformable ConvNet](https://github.com/msracver/Deformable-ConvNets) | [Jifeng Dai等](https://arxiv.org/abs/1703.06211)                                             | 2017年 ICCV 口头报告                                         | 引入可变形卷积层来增强网络对图像中物体形状变化的适应能力     |

## 3 高效模块设计

存储开销：参数量

计算开销：计算量

- SqueezeNet：减少参数量。提出高效模块Fire Module，包括压缩(Squeeze)和扩展(Expand)，将输入特征压缩到更少的通道数目。使用AlexNet 2%的参数量，达到相同的准确率。
- MobileNet 系列：减少参数量和计算量。通过聚合降低资源开销——提出深度可分离卷积，将卷积层分解为逐深度(空间维度聚合)卷积和逐点(深度维度聚合)卷积。
- ShuffleNet 系列：减少计算量。分组卷积，讲输出通道与输入通道间的计算对应关系从全连接改成部分连接。通道重排，打乱各层通道顺序，使信息组各分组间交互。
- GhostNet：减少计算量。可以让通道间相似的特征通过变换相互得到，就行是其他通道的鬼影。
- ConvNeXt：对ResNet架构进行优化，改变训练配置、调整架构。
- VoVNet 系列：优化帧率(Frame Per Second,FPS)和内存访问开销。基于DenseNet进行改进，引入单次聚合(One-Shot Aggregation,OSA)，降低访存率，提升GPU利用率、降低能耗。
- RepVGG：结构重参数化改进，提升任务性能。在RepVGG分支之外，增加了一条1x1卷积分支和Identity分支，缓解梯度消失问题。

<img src="20240824153416.jpg" alt="20240824153416" style="width:800px" />

模块级别设计

- D1 连接结构优化
- D2 卷积通道配置

卷积层级别设计

- D3 通道间分组
- D4 维度间分解
- D5 激活值通道间变换

## 4 模型剪枝

模型剪枝识别并删除模型中冗余权重。

问题定义：寻找最优的模型子结构$\alpha^*$和相应的权重$w^*$，在特定约束条件下，最优化减之后的模型性能。

优化空间：剪枝后模型的结构空间$A(\alpha^a \in A)$和剪枝后模型的权重空间$W(w^* \in W)$。

优化目标：尽量降低剪枝造成的任务性能损失。

约束条件：硬件指标(延时、功耗)、与推理效率相关的代理指标(参数量、计算量)。

<img src="20240824193605.jpg" style="width:800px" />

4.2 模型敏感度分析方法 34
4.2.1 层内和层间敏感度分析 34
4.2.2 层内敏感度分析指标 35
4.3 结构化剪枝方法 37
4.3.1 基于权重正则的结构化剪枝方法 37
4.3.2 基于搜索的结构化剪枝方法 39
4.3.3 给定资源限制的条件下的结构化剪枝方法 44
4.4 近似低秩分解方法 47
4.5 非结构化剪枝方法 48
4.6 半结构化剪枝方法 51
4.7 针对激活值的剪枝方法 53
4.8 剪枝方法的经验性选择 55
4.8.1 剪枝流程的选择 55
4.8.2 剪枝稀疏模式的选择 56
4.8.3 关于任务性能的经验 56
4.9 Group Lasso 结构化剪枝的实践案例 57

## 5 模型量化

5.1 模型量化的定义和分类 61
5.2 模型量化过程和量化推理过程 64
5.3 量化格式和操作 65
5.3.1 均匀量化格式 66
5.3.2 非均匀量化格式 68
5.3.3 三种量化操作 71
5.4 量化参数 73
5.4.1 缩放系数 73
5.4.2 零点位置 74
5.4.3 量化位宽 74
5.5 训练后量化75
5.5.1 训练后量化的流程 75
5.5.2 重参数化 76
5.5.3 缩放系数的选取方法 80
5.5.4 量化值调整 83
5.6 量化感知训练 87
5.6.1 基础与流程 87
5.6.2 调整模型架构的方法 90
5.6.3 量化器设计的技巧 92
5.6.4 常用的训练技巧 97
5.7 混合位宽量化 97
5.7.1 基于敏感度指标的混合位宽量化 97
5.7.2 基于搜索的混合位宽量化 99
5.8 量化方法的经验性选择 100
5.8.1 量化流程的选择 100
5.8.2 数据表示的设计和决定 100
5.8.3 算子的选择与处理和计算图的调整 102
5.8.4 关于任务性能的经验 104
5.9 拓展：低比特推理硬件实现 104
5.9.1 定点计算的硬件效率 104
5.9.2 浮点计算转定点计算的原理 105
5.9.3 非均匀量化格式的计算 111
5.9.4 典型的计算单元和加速器架构 112
5.10 拓展：低比特训练简介 115
5.10.1 应用背景 115
5.10.2 挑战分析 116
5.10.3 相关工作 116

## 6 模型二值化

6.1 模型二值化的定义和分类 118
6.2 模型二值化的基础：以XNOR-Net 为例 120
6.3 二值化方式 122
6.3.1 朴素二值化方式 123
6.3.2 间接二值化方式 127
6.4 训练技巧 131
6.4.1 修改损失函数 132
6.4.2 降低梯度估计误差 133
6.4.3 多阶段的训练方法 135
6.4.4 训练经验 136
6.5 架构设计 137
6.5.1 模型架构的调整 138
6.5.2 模型架构搜索 141
6.5.3 集成方法与动态模型 142
6.6 模型二值化在其他任务与架构中的应用 142

# 7 神经网络架构搜索

7.1 神经网络架构搜索的定义和分类 146
7.2 搜索空间 149
7.2.1 人工设计搜索空间 150
7.2.2 自动设计搜索空间 154
7.2.3 总结 156
7.3 搜索策略 157
7.3.1 基于强化学习的搜索策略 157
7.3.2 基于进化算法的搜索策略 159
7.3.3 随机搜索策略 160
7.3.4 基于架构性能预测器的搜索策略 160
7.3.5 总结 164
7.4 评估策略 165
7.4.1 独立训练策略 166
7.4.2 基于权重共享的单次评估策略 167
7.4.3 基于权重生成的单次评估策略 172
7.4.4 零次评估策略 172
7.5 可微分神经网络架构搜索 175
7.5.1 连续松弛方法 175
7.5.2 优化方法 176
7.5.3 搜索坍缩问题 177
7.5.4 更高效的可微分搜索算法 179
7.6 考虑硬件效率的神经网络架构搜索 180
7.6.1 考虑硬件效率的搜索空间设计 181
7.6.2 硬件效率指标的加速评估方法 182
7.6.3 考虑多种硬件效率目标的搜索策略 184
7.6.4 面向多种硬件设备及约束的神经网络架构搜索方法 186

## 8 知识蒸馏

8.1 知识蒸馏的定义和分类 190
8.2 知识类型和知识分量：“学什么” 192
8.2.1 基于响应的知识 192
8.2.2 基于特征的知识 194
8.2.3 基于关系的知识 197
8.3 知识来源：“向谁学” 199
8.3.1 离线蒸馏 199
8.3.2 互学习 199
8.3.3 自蒸馏200
8.4 本章小结 201

第3 部分拓展和前沿
9 相关领域：高效灵活的AI 推理硬件
和系统 203
9.1 概述 203
9.2 硬件加速器设计和软硬件协同优化 204
9.2.1 从CPU 到硬件加速器 204
9.2.2 AI 加速器中的软硬件协同优化 206
9.2.3 Roofline 分析模型 207
9.2.4 基于指令集的AI 加速器 210
9.3 神经网络计算资源虚拟化 211
9.3.1 虚拟化的概念 211
9.3.2 AI 加速器的时分复用与空分复用虚拟化 212
9.3.3 相关工作简介 214
9.4 本章小结 215
10 前沿应用：大语言模型的加速和压缩 218
10.1 大语言模型的发展 218
10.2 大语言模型的架构和推理过程 219
10.3 大语言模型的高效性分析 220
10.3.1 效率瓶颈 220
10.3.2 优化路径 221
10.4 典型的大语言模型的压缩方法：量化 223
