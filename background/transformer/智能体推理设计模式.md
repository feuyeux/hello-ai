# 智能体推理设计模式

**人工智能**已经发展出更复杂、更有能力的智能体，如虚拟助手(virtual assistants)、自主机器人(autonomous robots)和会话大语言模型(conversational LLM)智能体。这些智能体可以思考、行动和协作以实现复杂的目标。**智能体推理设计模式(Agentic Reasoning Design Patterns)**解释了这些智能体的工作原理，概述了**AI智能体**用于推理(reasoning)、决策(decision-making)和与环境交互( interacting with their environment)的基本策略(essential strategies)。

**人工智能体**是一种自主的软件实体(autonomous software entity)，能够感知周围环境(perceiving its environment)，做出决策(making decisions)，并采取行动(taking actions)以实现特定目标。大型语言模型使这些智能体能够理解自然语言并通过问题进行推理。它们还可以与各种工具和其他智能体互动，有效地解决复杂挑战。例如，客户支持人工智能智能体可能会使用大型语言模型来理解用户的查询，搜索知识库以找到适当的解决方案，并生成有用的回应，根据用户反馈调整其方法，以改善未来的互动。

本文将探讨四种关键设计模式： **Reflection(反思)**, **Tool Use(工具使用)**, **Planning(规划)**和**Multi-agent(多智能体)**。

## 1. Reflection Pattern

Reflection是基于LLM的智能体通过自我评估和迭代改进其自身推理能力的能力。这种方法特别适用于提高客户支持或诊断场景中的决策准确性。

模型对提示生成初始响应，评估此输出的质量和正确性，然后根据自身的反馈精炼内容。模型实质上扮演着创作者和批评家的双重角色。这个过程涉及多个迭代，AI在这两个角色之间交替，直到输出达到一定的质量水平或预设的停止标准。它评估自己的工作，检查错误、不一致之处，或输出的可提升领域，然后进行修订。这种生成和自我评估的周期使AI通过迭代精炼其响应，随着时间的推移，导致结果更加准确和有用。

这种模式对于大型语言模型尤其有价值，因为语言可能复杂且微妙。通过**反思自己的输出**，AI可以捕捉错误，澄清含糊不清的短语，并确保其响应更好地与预期意义或任务要求保持一致。就像我们的课程开发者为了提高学习成果而精炼课程一样，反思模式使AI系统能够持续提高生成内容的质量。

![Agentic AI Reflection Pattern](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/Agentic-AI-Reflection-Pattern.webp)

Reflection Pattern之所以有效，是因为它通过迭代的反馈允许**逐步改进**。通过反复反思输出，识别改进领域，并精炼文本，你可以获得比单次生成步骤更高的质量结果。

举个例子，在**撰写研究摘要**时使用这种模式。

- **提示(Prompt)：**“概括这篇关于气候变化的研究论文的关键点。”
- **生成(Generate)：**AI提供了一个简短的摘要。
- **反思(Reflect)：**你注意到摘要中缺少了一些重要的论文内容，比如研究发现的含义。
- **反思后的文本(Reflected Text)：**你更新了摘要，包括了这些细节，并精炼了语言以增强清晰度。
- **迭代(Iterate)：**你重复这个过程，直到摘要准确捕捉到所有关键点。

Reflection Pattern主要由以下三部分组成：

1. **Generation** Step(生成步骤)：从用户提供一个初始提示开始，这可能是生成文本、编写代码或解决复杂问题的请求。例如，提示可能要求AI生成一篇关于历史人物的论文，或者用特定的编程语言实现一个算法。

   - **零样本提示(Zero-Shot Prompting)**：第一次生成通常采用零样本风格，即AI在没有先前的示例或迭代的情况下生成响应。

   - 初始输出(Initial Output)：产生的输出被视为第一稿。虽然它可能相关且连贯，但可能仍然包含错误或缺乏必要的细节。

     生成步骤的目标是产生一个候选输出，该输出可以在后续步骤中进一步评估和精炼。

2. **Reflection** Step(反思步骤)：这是AI模型审查自身生成内容的关键阶段。这一步骤包括：

   - **自我批判(Self-Critique)**：模型批判自己的工作，识别改进领域，如事实错误、风格问题或逻辑不一致。

   - **反馈生成(Feedback Generation)**：AI生成具体反馈，可能包括重组内容的建议、添加细节或纠正错误。

   - **评估标准(Evaluation Criteria)**：批判可能基于预定义的标准，如语法准确性、连贯性、与提示的相关性或遵守特定的格式指南。


   反思过程可以模拟学科专家的风格，以提供更深入的反馈。例如，AI可能采取软件工程师的身份来审查一段代码，或者作为历史学家批评一篇论文。

3. Iteration and **Refinement**(迭代与精炼)：在这个阶段，反思步骤中生成的反馈被用来指导下一轮输出的生成。AI将建议的更改和改进纳入内容的新版本。这个周期重复多次，每次迭代都使输出更接近所需的质量。

   - 适应性学习(Adaptive Learning)：通过这个迭代过程，AI学会识别自己错误中的模式，并精炼对任务要求的理解。

   - 多次迭代(Multiple Iterations)：这个过程可以重复固定次数的步骤（例如，10次迭代），或者直到满足特定的停止条件，比如达到一定的内容质量水平或遇到“停止”关键词。

![How the Reflection Pattern Works](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/image-1-1-1.webp)

上图中的组成部分：

1. **Prompt (Input):** 给模型的初始输入，作为文本生成过程的起点。
2. **Generate:** AI模型根据提示创建响应的过程。
3. **Output Text:** 模型生成的响应。
4. **Reflect:** 生成的输出被分析、审查或修改以提升质量的一步。
5. **Reflected Text:** 在初次生成后经过修改或调整的输出。
6. **Iterate:** 过程重复进行，使用反思后的文本生成新的输出，进一步精炼结果。

流程解释

1. **Step 1 – Generate:** 用户首先向AI模型提供一个提示。例如，提示可能是：“写一个关于一只猫旅行到太空的短篇故事。”
2. **Step 2 – Output Text:** 模型根据提示生成一个响应，例如：“从前，有一只叫胡须的猫，在后院发现了一艘神奇的火箭船。胡须跳了进去，发射到太空中，在那里他遇到了来自喵塔星球的异星猫。”
3. **Step 3 – Reflect:** 在这个阶段，你审查生成的输出以评估质量。你可能会注意到故事缺乏关于胡须情感或旅途中面临的挑战的细节。
4. **Step 4 – Reflected Text:** 根据反思修订文本或提出改进建议。反思后的版本可能包括额外的细节：
   “胡须既兴奋又害怕地踏进火箭船。随着引擎轰鸣着启动，他紧紧抓住座位，不知道自己是否还能再见到家。穿越太空的旅程充满了奇异的景象和危险，如流星雨和宇宙风暴，这些都考验着胡须的勇气。”
5. **Step 5 – Iterate:** 现在这个精炼的文本可以反馈到生成过程中，可能作为新的提示或进一步文本生成的改进基础。根据反思后的文本，模型可以生成故事的更加精致版本。

**Self-RAG**或者**CRAG**都是这种模式的真实案例。

![Self-RAG vs. Traditional RAG](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/image-49-1.webp)

RAG：理一个提示，例如“美国各州是如何获得它们的名称的？”。

- 步骤1 涉及检索与提示相关的几篇文档（显示为标记为1、2、3的气泡）。检索到的段落被添加到输入提示中。
- 步骤2 显示语言模型根据提示加上检索到的段落生成响应。然而，它可能产生不一致的输出（例如，矛盾的段落或引入未支持的主张）。

该模型缺乏自我反思机制，导致最终生成中可能包含潜在的错误或不相关内容。

Self-RAG：使用Self-RAG处理相同的提示。系统按需检索，意味着只有在需要时才会进行检索，系统动态决定何时检索将有益。

- 步骤1 检索多个相关段落，但它允许模型有选择地与这些信息互动，而不是强制所有检索内容都进入响应。
- 步骤2：并行生成多个输出。每个版本在使用检索段落的方式上都有所不同，确保不相关或矛盾的 信息可以被批判。例如，一些输出被标记为不相关或部分相关。
- 步骤3：Self-RAG批判生成的输出并选择最佳的一个。这包括对每个输出的相关性、事实准确性和整体质量进行评级。在这种情况下，输出1被选为最相关的，从而得到一个更干净、更准确的最终响应。

总之，该图对比了传统RAG如何不经反思地融入检索段落，而Self-RAG则通过选择性检索、生成和批判来实现更高的事实性和连贯性。

体现这种模式的两项突出技术包括：

- **自我精炼：自我反馈的迭代改进 [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/abs/2303.17651)**
  Madaan et al. (2023) 描述了智能体如何使用自己的反馈迭代地改进其响应，以提高推理和决策的质量。例如，智能体可以解决问题，评估其表现，并调整其方法，直到达到期望的结果。
- **反思：具有口语强化学习能力的语言智能体 [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)**
  Shinn et al. (2023) LLM智能体使用强化学习来强化口语正向行为并纠正错误。使智能体能以类似人类导师提供反馈的方式，从正确和错误中学习。

### Reflection Pattern实战

```python
# pip install langchain-ollama==0.2.0 IPython==8.29.0
from langchain_ollama import OllamaLLM
from IPython.display import display_markdown

llm = OllamaLLM(model="llama3.2", temperature=0)

generation_chat_history = [
   {
       "role": "system",
       "content": "You are an experienced Python programmer who generate high quality Python code for users with there explanations"
       "Here's your task: You will Generate the best content for the user's request and give explanation of code line by line. If the user provides critique,"
       "respond with a revised version of your previous attempt."
       "also in the end always ask - Do you have any feedback or would you like me to revise anything?"
       "In each output you will tell me whats new you have added for the user in comparison to earlier output"
   }
]

generation_chat_history.append(
   {
       "role": "user",
       "content": "Generate a Python implementation of the Fibonacci series for beginner students"
   }
)

fibonacci_code = llm.invoke(generation_chat_history)

generation_chat_history.append(
   {
       "role": "assistant",
       "content": fibonacci_code
   }
)

print("Generation Step")
display_markdown(fibonacci_code, raw=True)

#### Reflection Step ####
print("\n\nReflection Step")

reflection_chat_history = [
   {
   "role": "system",
   "content": "You are Nitika Sharma, an experienced Python coder. With this experience in Python generate critique and recommendations for user output on the given prompt",
   }
]
reflection_chat_history.append(
   {
       "role": "user",
       "content": fibonacci_code
   }
)
critique = llm.invoke(reflection_chat_history)
display_markdown(critique, raw=True)

#### Generation Step (2nd Iteration) ####
print("\n\nGeneration Step (2nd Iteration)")

Generation_2 = llm.invoke(generation_chat_history)
display_markdown(Generation_2, raw=True)

#### Reflection Step (2nd Iteration) ####
print("\n\nReflection Step (2nd Iteration)")
reflection_chat_history.append(
   {
       "role": "user",
       "content": Generation_2
   }
)
critique_1 = llm.invoke(reflection_chat_history)
display_markdown(critique_1, raw=True)

#### Generation Step (3rd Iteration) ####
print("\n\nGeneration Step (3rd Iteration)")

generation_chat_history.append(
   {
       "role": "user",
       "content": critique_1
   }
)
Generation_3 = llm.invoke(generation_chat_history)
display_markdown(Generation_3, raw=True)
```

## 2. Tool Use Pattern

LLM智能体不仅限于其内部推理能力；它们还可以利用外部工具来扩展其功能。工具使用对于扩展智能体独立实现的能力至关重要。通过访问专业知识，执行需要外部数据的任务，以及与各种工具互动，智能体显著增强了其问题解决能力。例如，LLM智能体可以使用工具从网络上检索最新信息，执行计算，翻译语言，或与专业数据库互动。

![tool use pattern](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/image-89.webp)


工具使用模式背后的核心思想

1. **任务模块化**：系统不是依赖一个试图处理所有事情的单一AI模型，而是将用户提示**分解**并将它们**分配**给**特定工具**（在此表示为工具A、工具B、工具C）。每个工具都专注于一种独特的能力，这使得整个系统更加高效和可扩展。
2. 用于多样化任务的专业工具：
   - **工具A**：例如，这可能是一个事实核查工具，用于查询数据库或互联网以验证信息。
   - **工具B**：这可能是一个数学求解器或代码执行环境，用于处理计算或运行模拟。
   - **工具C**：另一个专业工具，可能是用于语言翻译或图像识别。
3. 图表中的每个工具都被可视化为能够根据需要查询**信息源**（例如，数据库、网络API等），这表明了一个模块化架构，其中不同的子智能体或专业组件处理不同的任务。
4. **顺序处理**：模型可能通过工具运行**顺序查询**，这意味着可以逐个处理多个提示，每个工具独立查询其各自的数据源。这允许快速、响应性的结果，特别是当与在特定领域表现出色的工具结合使用时。

大型语言模型如何识别对工具的需求？
![](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/image-88.webp)
端到端过程：

- 输入：用户问“2乘以3等于多少？”
- 解释：LLM识别这是一个数学运算。
- 工具选择：LLM选择乘法工具。
- 创建有效载荷：它提取相关参数（a: 2 和 b: 3），准备一个有效载荷，并调用工具。
- 执行：工具执行操作（2 * 3 = 6)，并将结果返回给LLM以呈现给用户。

这在智能体AI中为什么重要？

上图捕捉了智能体AI的一个核心特征，即模型可以根据用户的查询自主决定使用哪个外部工具。LLM不仅仅是提供一个静态的响应，而是作为一个智能体动态选择工具，格式化数据，并返回处理后的结果，这是智能体AI系统中的工具使用模式的核心部分。这种工具集成允许LLM将其能力扩展到简单的语言处理之外，使它们成为更多功能的智能体，能够高效地执行结构化任务。

### CrewAI 内置内容生成工具

CrewAI 内置了博客研究和内容生成智能体(Blog Research and Content Generation Agent, BRCGA)，BRCGA能够自动化研究AI行业最新趋势并撰写高质量博客文章的过程。这个智能体利用专门工具从网络搜索、目录和文件中收集信息，最终生产出吸引人且信息丰富的内容。
BRCGA分为两个核心角色：

- **Researcher Agent**: 专注于收集洞察和市场分析。
- **Writer Agent**: 负责根据研究撰写文笔流畅的博客文章。

![CrewAI in-built Tools](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/CrewAI-in-built-Tools_-1.webp)

https://github.com/feuyeux/hello-crewai/blob/main/hello_tool_use.py

**智能体AI的工具使用模式**允许大型语言模型通过与外部工具互动来超越其固有的局限性，使它们能够执行基于预训练知识的简单文本生成之外的 tasks。这种模式将AI从仅依赖静态数据转变为动态访问实时信息并执行专业操作，如运行模拟、检索实时数据或执行代码。

核心思想是通过对专业工具（例如，事实核查、解方程或语言翻译）分配任务来**模块化任务**，这导致了更高的**效率、灵活性和可扩展性**。智能体AI不是依赖单一的整体AI处理所有任务，而是利用多个工具，每个工具都针对特定的功能设计，从而实现更快的处理和更有效的多任务处理。

**工具使用模式**突出了智能体AI的关键特性，如决策制定、自主行动、从工具使用中学习和多工具协调。这些能力增强了AI的自主性和问题解决潜力，使其能够独立处理日益复杂的任务。系统甚至可以通过学习成功的工具使用并优化其性能来随时间适应其行为。随着AI的不断发展，其整合和创造新工具的能力将进一步深化其自主性和智能体性。

工具使用的例子包括：

- **大猩猩：连接大量API的大型语言模型 [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/abs/2305.15334)**
  Patil et al. (2023) 提出了一种连接到众多API的模型，使其能够执行检索数据或进行复杂操作等任务。这种设计使语言模型成为广泛服务接口。
- **MM-REACT：ChatGPT多模态推理和行动的提示工程 [MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action](https://arxiv.org/abs/2303.11381)**
  Yang et al. (2023) 创建了一个系统，其中LLM智能体基于不同输入类型（如图像、文本和其他数据）进行多模态推理。这种多功能性使智能体更能处理现实世界的应用。

## 3. Planning Pattern

有效的LLM智能体可以通过遵循一系列逻辑步骤(a sequence of logical steps)来创建(create)和执行(execute)计划。规划对于解决需要长期思考(long-term thinking)、策略规划(strategizing)和组织(organizing)的复杂任务至关重要。

**Planning Pattern** 为语言模型提供了将大型任务**分解**为可管理**子目标**的策略，使它们能够逐步解决复杂挑战，同时保持总体目标在焦点上。本文将详细讨论规划模式，包括ReAct和ReWOO技术。

![Agentic AI Planning Pattern](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/Artboard-1-copy-5-2.webp)

**Agentic AI Planning Pattern** 是一个专注于将大问题分解为小任务，有效管理这些任务，并根据任务结果确保持续改进或适应的框架。这个过程是迭代的，依赖于结构化流程，以确保AI系统可以根据需要调整其计划，每次迭代都更接近期望的目标。

**Planning Pattern** 包括如下组成部分：

1. 规划(Planning)
   - 在这个初始阶段，AI智能体解释提示并制定一个总体**计划**。
   - 计划概述了AI打算如何解决问题，包括high-level的目标和策略。
2. 生成任务(Generate Task)
   - 从计划中，AI系统**生成特定的任务**以执行。
   - 每个任务是总体目标的一个更小、可管理的部分，使AI聚焦于特定的步骤。
3. 单任务智能体(Single Task Agent)
   - **单任务智能体**负责完成上一步生成的每个任务。
   - 这个智能体使用预定义的方法执行每个任务，如**ReAct**(**Re**ason + **Act** 推理+行动）或**ReWOO** (**Re**asoning **W**ith**O**ut **O**bservation 无观察推理)。
   - 任务完成后，智能体返回一个**任务结果**，将其发送回规划循环。
4. 重新规划
   - **重新规划**阶段评估任务结果，以确定是否需要任何调整。
   - 如果任务执行没有完全达到期望的结果，系统将**重新规划**，并可能修改任务或策略。
   - 这个反馈循环使AI系统能够迭代学习并改进其方法，使其更能适应变化的要求或意外结果。
5. 迭代
   - 这部分模式是一个循环，连接**生成任务**和**重新规划**。
   - 它标志着过程的迭代性质，其中AI系统持续重新评估和调整其方法，直到达到满意的结果。

智能体AI规划模式利用了**规划、任务生成、执行和重新规划**的结构化循环，确保AI系统能够自主地朝着复杂目标工作。这种模式通过允许AI根据任务结果修改其方法来支持适应性，使其在面对动态环境或变化目标时具有鲁棒性和响应性。


### Planning Pattern示例

[![example of an Agentic AI Planning Pattern](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-07T174155.992.webp)](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-07T174155.992.webp)

上图展示了一个顺序图像理解过程。智能体根据观察和计划好的响应来采取行动，以实现特定的目标。以下是如何将图像中的每个步骤融入到智能体AI框架中的：

#### 1. 目标设定（理解任务）
- **Prompt**： 任务从一个问题开始：“你能描述这张图片并计算图片中有多少个物体吗？”
- **智能体AI元素：** AI智能体将这个目标解释为分析图像以进行对象识别和描述的指令。目标是通过识别、计数和描述对象，来全面回答这个问题。
#### 2. 规划与子目标形成
- 过程分解——为了完成这个目标，智能体将任务分解为特定的子任务：
  - **Object Detection(对象检测)**（识别和定位对象）
  - **Classification(分类)**（识别每个对象是什么）
  - **Caption Generation(字幕生成)**（生成场景的自然语言描述）
- **智能体AI元素：** 智能体通过在智能体AI规划模式中设定中间子目标来规划其行动。在这里，检测对象是完成最终目标（生成包含对象计数的描述性字幕）所需的子目标。
#### 3. 感知(Perception)与行动(Action)（检测与描述）
- 使用的工具和模型：
  - 智能体使用`facebook/detr-resnet-101`模型进行**检测**，该模型识别和定位对象（例如，长颈鹿和斑马）并分配置信度分数。
  - 检测后，智能体使用`nlpconnect/vit-gpt2-image-captioning`生成描述性字幕。
- **智能体AI元素：** 智能体使用特定的知觉模块（预训练模型）来“感知”其环境（图像），以收集必要的信息。在智能体AI中，知觉是一个主动的、目标导向的过程。在这里，模型作为知觉工具，处理视觉信息以实现整体目标。
#### 4. 评估与迭代（合并结果）
- **处理和聚合信息：** 检测（边界框和对象类型）和字幕生成（描述性文本）的结果被合并。智能体评估其输出，确认对象检测置信水平和描述的连贯性。
- **智能体AI元素：** 智能体AI涉及根据反馈和信息聚合不断评估和调整响应。智能体审查其预测（检测分数和边界框），以确保它们符合任务的要求。
#### 5. 目标实现（答案呈现）
- **输出呈现：** 智能体最终提供一个答案，包括检测到的对象计数、带有置信度分数的识别对象列表和描述性字幕。
- **智能体AI元素：** 智能体通过将知觉和规划结果综合成连贯的响应来完成目标。在智能体AI中，这一步是关于实现任务的总目标并生成一个输出，以回答用户的初始问题。

### 智能体AI规划的 任务分解
![智能体AI规划的 任务分解](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-07T174241.828.webp)
来源：[理解LLM智能体的规划：一项调查](https://arxiv.org/pdf/2402.02716)

对于智能体AI规划，有两种不同的**任务分解**方法，专门设计用于在动态和可变的现实世界环境中**处理复杂任务**。鉴于尝试对复杂目标进行单步规划的限制，将目标分解为可管理的部分变得至关重要。这个过程类似于“分而治之”策略，涉及将复杂目标分解为更小、更易实现的子目标。
以下是每种方法的解释：

#### (a) 分解优先方法
1. **分解步骤**：在这种方法中，LLM智能体首先将主要目标完全分解为**子目标**（子目标-1，子目标-2，…，子目标-n），然后开始子任务。这一步见上图**1**。
2. **子计划步骤**：分解任务后，智能体为每个子目标独立创建**子计划**。这些子计划定义了实现每个子目标所需的具体行动。这个规划过程见上图**2**。
3. **顺序执行**：每个子计划按顺序一个接一个地执行，依次完成每个子目标，直到实现主要目标。

本质上，分解优先方法将**分解**和**执行**的阶段分开：在开始任何执行之前，完成所有子目标的规划。这种方法在规划过程中变化最小的稳定环境中可能有效。

#### (b) 交错方法
交错方法中，**分解**和**执行**以更交织的方式进行：
1. **同时规划和执行**：LLM智能体不是在采取行动之前完全分解任务，而是从部分分解（例如，从子目标-1开始）开始，并立即开始规划与这个子目标相关的行动并执行。
2. **适应性分解**：随着每个子目标的处理，可能会识别并规划新的子目标，随着智能体的进展而适应。智能体继续分解、规划和执行，允许灵活性以应对变化或意外的环境复杂性。

3. **动态执行**：这种方法更加**适应性强**和**响应迅速**，因为规划和执行是交错进行的。这允许智能体根据实时反馈进行调整，必要时修改子目标或行动。

简而言之，

- **分解优先**：一种结构化的、逐步的方法，所有子目标都在执行之前规划好。适用于**稳定环境**，其中任务定义明确，执行期间不太可能发生变化。
- **交错**：一种灵活的、适应性的方法，规划和执行同时进行。这种方法适用于**动态环境**，其中实时反馈和调整是必不可少的。

在复杂的AI规划中，选择这两种方法取决于环境和任务的可变性。分解优先方法强调结构和预先规划，而交错方法优先考虑适应性和实时响应性。

这两种方法各有其优势，但在面对高度动态和不可预测的情况时，它们也带来了独特的挑战。为了应对这种复杂性，一个名为**ReAct**（推理与行动）的新兴框架在AI研究中变得越来越受欢迎。ReAct以某种方式综合推理和行动，使智能体能够批判性地思考自己的行动，根据即时反馈调整策略。这个框架将结构化规划与实时调整相结合，允许智能体做出更复杂的决策，并处理多样化环境中的可变性。

### ReAct

#### 什么是ReAct？

正如我们已经知道的，LLMs在提供语言理解和决策能力方面表现出色。然而，它们的推理和行动能力一直是作为单独的主题进行研究的。本节将讨论LLMs如何使用推理和行动规划，通过ReAct方法更好地处理复杂任务。以下是**ReAct（推理+行动）**框架在语言模型系统中的演变和重要性。它将传统的（仅推理和仅行动）方法与结合了推理和行动能力的ReAct进行了对比。让我们分解ReAct架构的每个部分，以了解它传达的内容。

![ReAct的工作流程](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-07T174322.564.webp)

来源：[ReAct: 在语言模型中协同推理和行动](https://react-lm.github.io/)

#### 1. 仅推理(**Reason Only**)
- 这个模型专注于语言模型内的**推理**和**思维处理**。这种方法的一个例子是**[Chain-of-Thought(CoT 思维链) prompting](https://www.analyticsvidhya.com/blog/2023/12/what-is-chain-of-thought-prompting-and-its-benefits/)**，其中语言模型通过逻辑步骤解决问题，但不直接与环境互动。
- 在这个仅推理模式中，模型生成一系列的“思想”或“推理轨迹”，但无法采取行动或从外部环境接收反馈。它仅限于没有互动的内部沉思。
- **局限性**：由于它只进行推理，这个模型无法根据实时反馈调整其行为或与外部系统互动，使得它在需要互动的任务中显得不够动态。
#### 2. 仅行动(**Act Only**)
- 这个模型纯粹是为了在环境中**行动**而设计的。例子包括**WebGPT**和**SayCan**这样的系统，它们可以根据提示执行行动（例如，进行网络搜索和控制机器人）。
- 在这里，语言模型在外部**环境**中行动，采取行动，并观察这些行动的结果。然而，它没有一个推理轨迹来逻辑地指导其行动；它更多地依赖于直接的行动-响应，而没有更深入的规划。
- **局限性**：没有推理，这种方法缺乏处理复杂、多步骤问题解决的能力。行动可能是反应性的，但需要更多的战略思考，这可能会提高长期效果。
#### 3. ReAct
- **ReAct**框架在单个循环中结合了**推理**和**行动**。在这里，语言模型在环境中的**推理轨迹**和**行动**之间交替进行。
- 过程：
  - 模型首先对任务进行**推理**，创建关于接下来应该做什么的“思想”或假设。
  - 然后，它根据其推理在环境中采取**行动**。
  - 执行行动后，模型在环境中**观察**结果，并将其纳入下一步的推理中。
- 这个推理、行动和观察的循环不断迭代，允许模型根据环境的实时反馈**学习和适应**。
- **重要性**：通过整合推理和行动，ReAct允许模型将复杂的、多步骤的任务分解为可管理的步骤，根据结果进行调整，并朝着需要规划和互动的解决方案努力。这种结合使ReAct非常适合动态的、多步骤的任务，在这些任务中，模型必须不断适应和改进其方法。
#### 为什么ReAct强大？
- ReAct框架回答了图底部提出的问题：*如果我们结合推理和行动会怎样？*
- 通过整合这两种能力，ReAct使模型能够以协调的方式**思考和行动**。这增强了它的能力：
  - 解决复杂问题。
  - 根据反馈调整行动。
  - 在需要顺序决策的环境中有效操作。

本质上，ReAct通过将内部推理与外部行动相结合，为任务完成提供了更**全面的方法**，使其在仅靠推理或行动模型不足的真实世界应用中更加灵活和有效。
另外，以下是4种提示方法的比较：(a) Standard(标准)、 (b) Chain-of-thought (CoT 思维链, Reason Only 仅推理)、 (c) Act-only(仅行动)、 (d) ReAct (Reason+Act 推理+行动)，来解决HotpotQA（Yang等人，2018）问题。

以及(a) 仅行动和 (b) ReAct提示词，解决一个AlfWorld（Shridhar等人，2020b）游戏的比较。
![ReAct: 在语言模型中协同推理和行动](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-07T174357.535.webp)来源：[ReAct: 在语言模型中协同推理和行动](https://arxiv.org/pdf/2210.03629)
ReACT（推理+行动)方法通过同时利用推理和行动，超越了其他方法。这允许AI适应动态环境和复杂问题。这个框架导致了更复杂、更准确的结果，非常适合需要思考和互动的任务。
另请阅读：[Implementation of ReAct Agent using LlamaIndex and Gemini](https://www.analyticsvidhya.com/blog/2024/10/implementation-of-react-agent-using-llamaindex-and-gemini/)

### 使用OpenAI API和httpx库的规划模式

本部分旨在概述构建利用OpenAI API和httpx库的AI智能体的过程。它介绍了创建聊天机器人类的基本结构，该类能够处理用户输入并通过OpenAI的语言模型执行响应。本部分解释了实施ReAct模式以启用思考、行动、暂停和观察的循环。它描述了注册自定义动作（例如，维基百科搜索、计算、博客搜索）以增强功能。这促进了动态交互，智能体可以使用外部动作来完善和完成其回答。让我们直接了解构建AI智能体的基本结构：

这段代码定义了一个ChatBot类，用于与OpenAI的GPT模型互动。它使用可选的系统提示初始化，存储对话历史，处理用户输入，并使用OpenAI的API从模型中检索响应，模拟各种应用程序或聊天机器人功能所需的对话能力。


```python
import openai
import re
import httpx
class ChatBot:
    def __init__(self, system=""):
        self.system = system
        self.messages = []
        if self.system:
            self.messages.append({"role": "system", "content": system})
    def __call__(self, message):
        self.messages.append({"role": "user", "content": message})
        result = self.execute()
        self.messages.append({"role": "assistant", "content": result})
        return result
    def execute(self):
        completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=self.messages)
        return completion.choices[0].message.content
```

#### 实现ReAct模式
代码概述了一个结构化过程，用于通过思考、行动、暂停和观察的循环来回答问题。它定义了AI智能体应该如何思考问题，采取适当行动（计算或信息搜索），暂停等待结果，观察结果，并最终提供答案。

```python
prompt = """
You run in a loop of Thought, Action, PAUSE, Observation.
At the end of the loop you output an Answer.
Use Thought to describe your thoughts about the question you have been asked.
Use Action to run one of the actions available to you - then return PAUSE.
Observation will be the result of running those actions.
Your available actions are:
calculate:
e.g. calculate: 4 * 7 / 3
Runs a calculation and returns the number - uses Python so be sure to use floating point
syntax if necessary
wikipedia:
e.g. wikipedia: Django
Returns a summary from searching Wikipedia
simon_blog_search:
e.g. simon_blog_search: Django
Search Simon's blog for that term
Example session:
Question: What is the capital of France?
Thought: I should look up France on Wikipedia
Action: wikipedia: France
PAUSE
You will be called again with this:
Observation: France is a country. The capital is Paris.
You then output:
Answer: The capital of France is Paris
""".strip()
```

*在实现ReAct模式之后，我们将实施以下行动*：
- *行动：维基百科搜索，*
- *行动：博客搜索，*
- *行动：计算。*
#### 向AI智能体添加行动
接下来，我们需要将这些行动注册到一个字典中，以便AI智能体可以使用它们：

```python
known_actions = {
    "wikipedia": wikipedia,
    "calculate": calculate,
    "simon_blog_search": simon_blog_search
}
```

#### 完成集成
这段代码定义了一个函数或查询，它模拟了一个聊天机器人与用户指定问题的互动。它迭代处理响应，直到达到最大回合数，使用已知的处理程序提取并执行特定行动，并根据观察结果更新提示，直到返回或打印最终结果。

```python
def query(question, max_turns=5):
    i = 0
    bot = ChatBot(prompt)
    next_prompt = question
    while i < max_turns:
        i += 1
        result = bot(next_prompt)
        print(result)
        actions = [action_re.match(a) for a in result.split('\n') if action_re.match(a)]
        if actions:
            action, action_input = actions[0].groups()
            if action not in known_actions:
                raise Exception(f"Unknown action: {action}: {action_input}")
            print(" -- running {} {}".format(action, action_input))
            observation = known_actions[action](action_input)
            print("Observation:", observation)
            next_prompt = f"Observation: {observation}"
        else:
            return result
print(query("What does England share borders with?"))
```

![Output](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-07T174441.069.webp)

为了获得完整的代码实现，请参考以下文章：[从头开始构建AI智能体的全面指南](https://www.analyticsvidhya.com/blog/2024/07/build-ai-agents-from-scratch/)。

让我们看看使用[LangChain](https://www.langchain.com/)实现ReAct模式的规划模式的实现：

### 使用LangChain实现ReAct规划模式
目标是使用LangChain和[OpenAI的GPT模型](https://platform.openai.com/docs/models)实现一个工具增强的AI智能体，该智能体可以自主进行研究并通过集成像[Tavily API](https://tavily.com/)这样的自定义工具（如网络搜索）来回答复杂问题。这个智能体旨在通过执行一个名为ReAct（推理与行动）的规划模式来模拟人类解决问题的方式。它构建了一个推理和行动步骤的循环，评估响应，并做出决策，以有效地收集和分析信息。这个设置支持实时数据查询和结构化决策制定，使得能够增强对如下问题的响应：“自设立以来，金球奖得主的名字有哪些？” 

#### 简单网络搜索工具

```python
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.tools import tool
import requests
import json
tv_search = TavilySearchResults(max_results=3, search_depth='advanced',
                               max_tokens=10000)

@tool
def search_web(query: str) -> list:
   """Search the web for a query."""
   tavily_tool = TavilySearchResults(max_results=2)
   results = tavily_tool.invoke(query)
   return results
```

#### 测试工具调用与LLM

```python
from langchain_openai import ChatOpenAI
chatgpt = ChatOpenAI(model="gpt-4o", temperature=0)
tools = [search_web]
chatgpt_with_tools = chatgpt.bind_tools(tools)
prompt = "What are the names of Ballon d'Or winners since its inception?"
response = chatgpt_with_tools.invoke(prompt)
response.tool_calls
```

**输出**

```
[{'name': 'search_web',
  'args': {'query': "list of Ballon d'Or winners"},
  'id': 'call_FW0h6OpObqVQAIJnOtGLJAXe',
  'type': 'tool_call'}]
```

#### 构建和测试AI智能体
现在我们已经定义了工具和LLM，我们可以创建智能体了。我们将使用一个工具调用智能体，通过一个提示将工具绑定到智能体上。我们还将添加存储历史对话作为记忆的能力。

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
SYS_PROMPT = """You run in a loop of Thought, Action, PAUSE, Observation.
               At the end of the loop, you output an Answer.
               Use Thought to describe your thoughts about the question you have been asked.
               Use Action to run one of the actions available to you - then return PAUSE.
               Observation will be the result of running those actions.
               wikipedia:
               e.g. wikipedia: Ballon d'Or
               Returns a summary from searching Wikipedia.
               Use the following format:
               Question: the input question you must answer
               Thought: you should always think about what to do
               Action: the action to take, should be one of [Wikipedia, duckduckgo_search, Calculator]
               Action Input: the input to the action
               Observation: the result of the action
               ... (this Thought/Action/Action Input/Observation can repeat N times)
               Thought: I now know the final answer
               Final Answer: the final answer to the original input question
             """
prompt_template = ChatPromptTemplate.from_messages(
   [
       ("system", SYS_PROMPT),
       MessagesPlaceholder(variable_name="history", optional=True),
       ("human", "{query}"),
       MessagesPlaceholder(variable_name="agent_scratchpad"),
   ]
)
prompt_template.messages
```

**Output**

![Output](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/image-1-2.webp)

现在，我们可以用LLM、提示和工具来启动智能体。智能体负责接收输入并决定采取什么行动。智能体并不执行这些行动——执行行动的是`AgentExecutor`。注意，我们传入的是模型`chatgpt`，而不是`chatgpt_with_tools`。这是因为`create_tool_calling_agent`会在幕后为我们调用`.bind_tools`。理想情况下，这应该与支持工具/函数调用的LLM一起使用。

```python
from langchain.agents import create_tool_calling_agent
agent = create_tool_calling_agent(chatgpt, tools, prompt_template)
agent
```

![Output](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/image-2-1-1.webp)

最后，我们将智能体大脑与AgentExecutor内的工具结合起来（AgentExecutor将重复调用智能体并执行工具）。

```python
from langchain.agents import AgentExecutor
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose = True)
agent_executor
```

![Output](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/image-3-1-1.webp)

```python
query = """Tell me the Ballon d'Or winners since it started?
       """
response = agent_executor.invoke({"query": query})

from IPython.display import display, Markdown

display(Markdown(response['output']))
```

![Output](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/image-4-1-1.webp)

也请阅读：[从头开始构建AI智能体的全面指南](https://www.analyticsvidhya.com/blog/2024/07/build-ai-agents-from-scratch/)

### ReWOO
**ReWOO(无观察推理)**是由Xu et al提出的一种新的智能体架构，它强调了一种在大型语言模型系统中进行多步骤规划和变量替换的高效方法。它解决了ReAct风格智能体架构中的一些局限性，特别是在执行效率和模型微调方面。以下是[ReWOO](https://arxiv.org/abs/2305.18323)相较于传统方法的改进之处：

#### ReWOO是如何工作的？

![Workflow of ReWOO (Reasoning Without Observation)](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/1-01-1-scaled.webp)

以下是[ReWOO（无需观察的推理）](https://langchain-ai.github.io/langgraph/tutorials/rewoo/rewoo/)智能体模型的工作流程。该模型旨在通过最小化冗余观察并专注于计划好的行动序列来提高多步骤推理和工具使用的效率。以下是每个组件的信息流动和步骤解释：
#### ReWOO的组件
1. Planner(规划器)
   - **规划器**负责在开始时创建整个**计划**。它确定解决任务所需的**行动序列**或步骤。
   - 对于每个行动步骤，规划器指定：
     - **Tool(工具)**：该步骤所需的特定工具或功能。
     - **Arguments(参数)**：工具所需的输入值或变量。
   - 计划使用**变量替换**来定义，其中一个工具的**输出**可以作为另一个工具的**参数**，从而在步骤之间创建依赖关系。
   - 重要的一点是，这个规划过程在单个**LLM调用**中完成，这使得它比迭代、基于观察的推理更高效，因为它减少了令牌消耗。
2. Worker(工作者)
   - **工作者**负责执行**规划器生成的计划**中的行动。
   - 工作者接收每个步骤提供的参数，调用指定的工具，并返回结果。
   - 这个执行过程可以循环进行，直到任务解决，确保每个工具行动按照计划中概述的正确顺序完成。
   - 工作者独立于LLM运行，这意味着它只是遵循规划器的指示，而在每个步骤中不进行额外的LLM调用。
3. Solver(解答器)
   - **解答器**是最后一个组件，负责解释工作者使用的工具的结果。
   - 基于工具执行中收集的**观察结果**，解答器生成用户查询或任务的**最终答案**。
   - 这部分可能涉及最终的**LLM调用**，以将信息综合成连贯的响应。
#### ReWOO的关键增强
以下是ReWOO的关键增强：
1. 高效的工具使用和减少的令牌消耗
   - **单次生成工具**：与ReAct风格的智能体不同，后者每个推理步骤都需要多个LLM调用（因此每个调用都要重复整个系统提示和之前的步骤），ReWOO在一次传递中生成所需工具的完整序列。
   - 这种方法大幅**减少了令牌消耗**并**缩短了执行时间**，使其更适合涉及多个步骤或工具的复杂任务。
2. 简化的微调过程
   - **从工具输出中解耦规划**：由于ReWOO的规划数据不依赖于工具的实际输出，它允许更直接的微调过程。
   - **无需工具执行的微调**：理论上传，可以在不调用任何工具的情况下对模型进行微调，因为它依赖于计划行动和替换，而不是实际的工具响应。
#### 工作流程过程
流程通过以下步骤进行：
1. 步骤1 – 用户输入
   - **用户**向ReWOO提交一个问题或任务。
   - 输入被传递给规划器以启动规划阶段。
2. 步骤2 – 规划器创建计划
   - 规划器制定一个多步骤计划，指定使用哪些工具以及所需的参数。
   - 计划可能涉及变量替换，其中一个工具的输出被用作另一个工具的输入。
   - 规划器然后将这个完整的计划提供给工作者。
3. 步骤3 – 工作者执行行动
   - 工作者通过使用适当的参数调用指定的工具来执行计划的每个步骤。
   - 这个循环过程确保每个工具行动按顺序完成，直到任务完成。
4. 步骤4 – 解答器生成答案
   - 一旦所有必要的行动被执行，解答器解释结果并为用户生成最终答案。
   - 然后将这个答案返回给用户，完成工作流程。

本质上，ReWOO通过分离**推理（规划器）**和**执行（工作者）**阶段，提高了智能体的效率，从而为复杂任务创建了一个更快和更资源高效的框架。

### 观察推理与ReWOO的比较
![观察推理与ReWOO的比较](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-07T175015.109.webp)来源：[ReWOO](https://arxiv.org/pdf/2305.18323)
在涉及大型语言模型的系统中，有两种不同的任务推理方法：**（a）观察推理**和**（b)ReWO(无观察推理)**。

以下是基于上图的比较：

#### 1. 观察推理（左）
- 设置和流程
  - 用户的任务首先通过**上下文**和**示例**（帮助LLM推理的示例或提示）进行增强，然后输入LLM以开始推理过程。
  - LLM生成两个关键输出：
    - **T（Thought 思考）**：代表LLM初始处理得出的内部思考或理解。
    - **A（Action 行动）**：这是LLM基于其思考决定采取的行动，通常涉及查询工具以获取信息。
  - 每次行动后，接收来自工具的**O(Observation 观察)**。这种观察作为一种反馈循环，并附加到提示历史中，形成下一次LLM调用的更新输入。
- 迭代性质
  - 这种设置是迭代的，意味着LLM重复经历思考、行动和观察，直到达到足够的推理。
  - 每个周期依赖于**在提示历史中连续堆叠观察**，随着时间的推移，随着更多信息积累，创建提示冗余。
- 限制
  - 这种方法可能导致**提示冗余**，并可能由于每个周期重复输入上下文和示例而造成效率低下，因为相同的数据（上下文和示例）被反复反馈到系统中。
#### 2. ReWOO（右）
- 增强的结构
  - 与依赖观察的推理设置不同，ReWOO通过分离角色引入了更结构化的方法：
    - **规划器(Planner)**：负责创建一系列相互依赖的**P(Plan 计划)**。
    - **工作者(Worker)**：根据规划器的指示从各种工具中获取**E(Evidence 证据)**。
  - 规划器生成计划，然后传递给工作者。工作者通过工具交互收集必要的证据来执行这些计划。
- 计划和证据的角色
  - **计划（P）**：这些是预先定义的、相互依赖的步骤，概述了系统的推理路径。
  - **证据（E）**：这是根据规划器的指示检索的具体信息或数据。
  - **计划（P）和证据（E）的组合**形成了一个更有组织的输入，最终由**解答器**LLM处理，以产生用户的输出，同时结合原始任务和上下文。
- 解答器
  - 解答器作为最终的推理模块，整合任务、上下文、计划和证据以生成连贯的答案。
  - 由于上下文和示例没有反复输入LLM，ReWOO减少了提示冗余的问题。
#### ReWOO的关键差异和优势
- 提示效率
  - **依赖观察的推理**由于重复周期的相同上下文和示例而遭受提示冗余，可能过度加载提示并增加处理时间。
  - **ReWOO**通过分离规划和证据收集阶段，避免了这种冗余，使提示更加高效。
- 结构化的任务执行
  - ReWOO的设计引入了**规划器**和**工作者**，使得任务规划和证据收集之间有了清晰的区分。这种结构化的流程确保了每个步骤的逻辑执行，使得管理复杂任务变得更加容易。
- 可扩展性
  - 借助于其模块化设置，ReWOO可以有效处理更复杂的任务。其规划和证据检索的结构化方法使其能够更好地扩展以处理复杂的推理任务，因为每个组件（规划器、工作者、解答器）都有明确的角色。
#### 总结
- **依赖观察的推理**：通过思考、行动和观察的周期循环，创建提示冗余，但保持了简单性。
- **ReWOO**：通过使用规划器、工作者和解答器来简化推理，减少提示冗余，并提高处理复杂任务的效率，采用了更有组织的结构。

### ReWOO的代码实现

为了实践ReWOO，我参考了[Vadym Barda使用LangGraph的ReWOO配方](https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/rewoo/rewoo.ipynb)。现在，我不提及其它库和需求，但我会深入到定义图状态、规划器、执行器和解答器的部分。

以下是基于参考链接中内容的概述，并非实际的代码实现：

![Hands-on on ReWoo](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-07T175134.475.webp)

来源：[Vadym Barda GitHub](https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/rewoo/rewoo.ipynb)
在LangGraph中，每个节点都会更新一个**共享的图状态(shared graph state)**，每当一个节点被激活时，这个状态就会作为输入。下面，状态字典被定义包含基本任务细节，如任务(task)、计划(plan)、步骤(steps)以及其他必要的变量(variables)。

```python
from typing import List
from typing_extensions import TypedDict
class ReWOO(TypedDict):
    task: str
    plan_string: str
    steps: List
    results: dict
    result: str
```

#### 规划器：生成任务计划

**规划器**模块使用语言模型生成一个结构化的计划，形式为任务列表。计划中的每个任务由字符串表示，这些字符串可以包括**特殊变量**（如#E{0-9}+），用于替换之前结果中的值。在这个例子中，智能体有两个工具可以使用：
1. **Google**：它充当搜索引擎，在这里由Tavily代表。
2. **LLM**：一个大语言模型工具，用于解释和分析数据，有效地提供来自先前输出的推理。
**提示**指导模型如何创建计划，指定使用哪些工具以及如何使用变量引用先前结果。


```python
from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-4o")
prompt = """For the following task, make plans that can solve the problem step by step. For each plan, indicate \
which external tool together with tool input to retrieve evidence. You can store the evidence into a \
variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...)
# Task Example
task = "what is the exact hometown of the 2024 mens australian open winner"
result = model.invoke(prompt.format(task=task))
print(result.content)
```

**输出**

```sh
Plan: Use Google to search for the 2024 Australian Open winner.

#E1 = Google[2024 Australian Open winner]

Plan: Retrieve the name of the 2024 Australian Open winner from the search results.

#E2 = LLM[What is the name of the 2024 Australian Open winner, given #E1]

...
```

##### 规划器节点
规划器节点连接到图上，创建一个get_plan节点，该节点接收ReWOO状态并使用新的步骤和plan_string更新它。

```python
import re
from langchain_core.prompts import ChatPromptTemplate
regex_pattern = r"Plan:\s*(.+)\s*(#E\d+)\s*=\s*(\w+)\s*\[([^\]]+)\]"
prompt_template = ChatPromptTemplate.from_messages([("user", prompt)])
planner = prompt_template | model
def get_plan(state: ReWOO):
    task = state["task"]
    result = planner.invoke({"task": task})
    matches = re.findall(regex_pattern, result.content)
    return {"steps": matches, "plan_string": result.content}
```

##### 执行器：执行计划任务
**执行器**遍历每个计划任务，依次执行指定的工具。它使用辅助函数来确定当前任务，并在每次工具调用之前执行变量替换。

```python
from langchain_community.tools.tavily_search import TavilySearchResults
search = TavilySearchResults()
def _get_current_task(state: ReWOO):
    if "results" not in state or state["results"] is None:
        return 1
    if len(state["results"]) == len(state["steps"]):
        return None
    else:
        return len(state["results"]) + 1
def tool_execution(state: ReWOO):
    _step = _get_current_task(state)
    _, step_name, tool, tool_input = state["steps"][_step - 1]
    _results = (state["results"] or {}) if "results" in state else {}
    for k, v in _results.items():
        tool_input = tool_input.replace(k, v)
    if tool == "Google":
        result = search.invoke(tool_input)
    elif tool == "LLM":
        result = model.invoke(tool_input)
    else:
        raise ValueError
    _results[step_name] = str(result)
    return {"results": _results}
```

##### 解决器：综合最终输出

**解决器**从每个执行的工具体集聚合结果，并根据收集到的证据生成一个结论性的答案。

```python
solve_prompt = """Solve the following task or problem. To solve the problem, we have made step-by-step Plan and \
retrieved corresponding Evidence to each Plan. Use them with caution since long evidence might \
contain irrelevant information.
{plan}
Now solve the question or task according to provided Evidence above. Respond with the answer
directly with no extra words.
Task: {task}
Response:"""
def solve(state: ReWOO):
    plan = ""
    for _plan, step_name, tool, tool_input in state["steps"]:
        _results = (state["results"] or {}) if "results" in state else {}
        for k, v in _results.items():
            tool_input = tool_input.replace(k, v)
            step_name = step_name.replace(k, v)
        plan += f"Plan: {_plan}\n{step_name} = {tool}[{tool_input}]"
    prompt = solve_prompt.format(plan=plan, task=state["task"])
    result = model.invoke(prompt)
    return {"result": result.content}
```

##### 定义图工作流程
**图**是一个有向的工作流程，它协调规划器、工具执行器和解决器节点之间的交互。条件边确保流程循环执行，直到所有任务完成。

```python
def _route(state):
    _step = _get_current_task(state)
    if _step is None:
        return "solve"
    else:
        return "tool"
from langgraph.graph import END, StateGraph, START
graph = StateGraph(ReWOO)
graph.add_node("plan", get_plan)
graph.add_node("tool", tool_execution)
graph.add_node("solve", solve)
graph.add_edge("plan", "tool")
graph.add_edge("solve", END)
graph.add_conditional_edges("tool", _route)
graph.add_edge(START, "plan")
app = graph.compile()

# Stream output to visualize final results
for s in app.stream({"task": task}):
    print(s)
    print("---")

#Input: task = "what is the exact hometown of the 2024 mens australian open winner"
```

![Output](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-07T175236.284.webp)

```python
from IPython.display import Image, display
from langchain_core.runnables.graph import MermaidDrawMethod

display(
    Image(
        app.get_graph().draw_mermaid_png(
            draw_method=MermaidDrawMethod.API,
        )
    )
)
```

![Graph](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/image-6-1-1.webp)

```css
print(s["solve"]["result"])
```

**输出**

```sh
San Candido, Italy
```

### 规划模式的优势和局限性
Agentic AI规划模式提供了显著的优势，尤其是在任务的复杂性阻止了预定的逐步分解时。规划使智能体能够动态决定他们的行动方案，允许适应性和情境感知的问题解决。它提高了处理不可预测任务的灵活性和能力，使得在需要战略远见和决策的情况下成为一个强大的工具。

然而，这种能力也带来了一些显著的局限性。规划过程的动态性质引入了**不可预测性**，使得预测智能体在任何给定场景中的行为变得更加困难。与更确定性的智能体工作流程（如反射或工具使用）相比——这些流程可靠且有效——规划则相对不太成熟，可能会导致不一致的结果。尽管当前的规划能力存在挑战，但AI研究的快速发展表明，这些局限性可能会随着时间的推移而减少，从而带来更强大和可预测的规划功能。

规划的两个重要方法包括：

- **思维链提示工程激发大型语言模型中的推理 [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)**
  Wei et al. (2022) 展示了提示大型语言模型逐步思考问题可以改善它们的问题解决能力。将任务分解为逻辑序列允许智能体遵循导致更准确结果的推理链。例如，在复杂的故障排除中，将过程分解为单独的诊断步骤可以确保不遗漏任何关键细节，最终导致更有效的解决方案。
- **HuggingGPT：在Hugging Face中使用ChatGPT生态解决AI任务 [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](https://arxiv.org/abs/2303.17580)**
  Shen et al. (2023) 展示了LLM智能体如何与Hugging Face上的模型合作来分工合作解决任务。HuggingGPT跨多个AI模型协调行动，确保每个模型都为一个协调良好的计划做出贡献，以完成复杂任务。

## 4. Multi-Agent Patern

多个LLM智能体之间的协作通常会产生更高效和更复杂的结果，例如提高问题解决速度、准确性，以及通过汇集资源和专业知识来处理更复杂的任务。例如，在医疗诊断场景中，多个LLM智能体可以协同工作，分析患者数据，交叉参考医学文献，并提出可能的诊断，从而实现更快和更准确的医疗评估。这种模式依赖于智能体作为团队进行沟通和解决问题。

在Multi-Agent Patern中，你可以将一个复杂的任务分解为子任务，不同的智能体可以执行这些任务。例如，如果你正在构建软件，那么编码、规划、产品管理、设计和质量保证的任务将由擅长各自任务的不同的智能体来完成。

![Agentic AI多智能体模式的架构](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/Artboard-1-copy-3-1.webp)
这个架构展示了一个Agentic AI多智能体系统，其中具有专门角色的不同智能体相互之间以及与总体的多智能体应用程序交互，以处理用户提示并生成响应。系统中的每个智能体都有一个独特的功能，模拟一个协作团队高效地一起完成任务。

Multi-Agent Patern组件解释

1. 用户交互：
   - **提示：**用户通过向多智能体应用程序输入提示来启动交互。
   - **响应：**系统通过智能体之间的协作交互处理提示，并向用户返回响应。
2. 智能体及其角色：
   - **智能体1：软件工程师**：专注于与软件开发相关的技术问题解决，提供编码解决方案或建议基于软件的策略。
   - **智能体2：项目经理**：监督项目管理方面，协调智能体之间的努力，确保流程与整体项目目标保持一致。
   - **智能体3：内容开发者**：生成内容，撰写草稿，或协助开发项目所需的项目文档和创意材料。
   - **智能体4：市场研究分析师**：收集数据，对市场趋势进行分析，并提供为其他智能体策略提供信息见解。
3. 交互流程：
   - 智能体之间的箭头表示通信渠道和协作路径。这意味着：
     - **双向箭头**（双头）：智能体可以来回交换信息，实现迭代协作。
     - **虚线**：表示智能体之间的次要或间接通信路径，暗示在通信流程中的支持角色而不是主要协调。
4. 通信工作流程：
   - **启动**：用户向多智能体系统提供提示。
   - 协调：
     - **智能体1（软件工程师）**可能首先确定任何初始技术要求或策略。
     - **智能体2（项目经理）**与**智能体1**和其他智能体协调，确保每个人都保持一致。
     - **智能体3（内容开发者）**创建相关的内容或草稿，这些可能是输出的一部分。
     - **智能体4（市场研究分析师）**提供研究数据，这对其他智能体的明智决策至关重要。
   - **完成**：一旦所有智能体都协作完毕，系统将编译最终响应并将其呈现给用户。
   ![软件开发](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/New-Agentic-AI-Multi-Agent-Pattern-.webp)

关键特征：

- **协作智能**：这种架构促进了协作问题解决，具有专门知识的智能体贡献了独特的见解和技能。
- **自主性**：每个智能体半独立地操作，专注于他们的特定角色，同时与其他智能体保持通信。
- **可扩展性**：可以通过添加更多专门的智能体来扩展模型，以处理更复杂的用户提示。
这种架构在需要多样化专业知识的复杂任务中特别有效，如研究项目、产品开发和全面的内容创作。强调明确的角色和协调的沟通确保了复杂任务的每个部分都得到高效和连贯的处理。我希望你已经理解了多智能体是如何工作的。现在，我们将讨论一个构建多智能体解决方案的框架。

### AutoGen
![AutoGen：通过多智能体对话启用下一代LLM应用程序](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-13T180522.746.webp)
来源：[AutoGen 通过多智能体对话启用下一代LLM应用程序](https://arxiv.org/pdf/2308.08155)

[CrewAI](https://www.crewai.com/)、[LangGraph](https://www.langchain.com/langgraph)和[AutoGen](https://microsoft.github.io/autogen/0.2/)，它们为开发者提供了构建多智能体解决方案的方法。今天，我们要谈论的是AutoGen：

AutoGen在LLM应用程序中引入了一种新范式，通过启用可定制的、可对话的智能体，这些智能体被设计在多智能体对话框架内运行。这种设计基于一个理解，即现代LLM可以无缝地适应和整合反馈，尤其是那些针对对话优化的（例如，GPT-4)。AutoGen利用这种能力，允许智能体以对话方式互动——自主地或在人监督下交换观察、批评和验证。

AutoGen智能体的灵活性源于它们能够根据开发者的需求融入各种角色和行为。例如，这些智能体可以被编程来编写或执行代码、整合人类反馈或验证结果。这种灵活性得到了一个开发者可以轻松配置的模块化结构的支持。每个智能体的后端都是可扩展的，允许进一步定制，增强其超出默认设置的功能。智能体的可对话性质使它们能够进行持续的多轮对话，并适应动态的互动模式，使它们适用于从问答、决策到复杂问题解决任务的多样化应用。

#### 对话编程
AutoGen中的一个关键创新是对话编程的概念，它通过将流程简化为多智能体对话，革命化了LLM应用程序的开发。这种编程范式将重点从传统的以代码为中心的工作流程转移到以对话为中心的计算，使开发者能够更直观地管理复杂的交互。对话编程展开为两个核心步骤：
1. **定义可对话智能体**：开发者通过配置内置功能创建具有特定能力和角色的智能体。这些智能体可以被设置为自主操作、与其他智能体协作，或在不同的点上涉及人类参与，确保自动化和用户控制之间的平衡。
2. **编程交互行为**：开发者通过对话为中心的逻辑编程这些智能体如何互动。这涉及使用自然语言和代码的混合，使对话模式的灵活脚本成为可能。AutoGen通过现成的组件促进了这些互动的无缝实现，这些组件可以被扩展或修改以用于实验性或定制的应用。
对话编程的整合支持了不同LLM能力的模块化组合，使得复杂的任务可以被分解为智能体可以协作解决的可管理子任务。这个框架支撑了跨多个领域（包括研究、编码和互动娱乐）的健壮且可扩展的LLM应用程序的开发。

### 使用AutoGen编程多智能体对话
![如何使用AutoGen编程多智能体对话？](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-13T180701.258.webp)
来源：[AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/pdf/2308.08155)

使用AutoGen编程多智能体对话主要分为三个部分：**AutoGen智能体**、**开发者代码**和**程序执行**，以下是一个详细的分解：

#### 1. AutoGen智能体(AutoGen Agents)
- **ConversableAgent**：这是不同类型智能体操作的整体框架。图表突出显示了以下几种智能体类型：
  - **AssistantAgent**：可配置选项，如human_input_mode设置为“NEVER”和code_execution_config设置为False。这意味着智能体是完全自主的，在操作过程中不依赖人类输入。
  - **UserProxyAgent**：设置为human_input_mode为“ALWAYS”，表明它是用户控制的，并且总是需要人类输入来响应。
  - **GroupChatManager**：管理群组对话中多个智能体之间的互动。
- **统一对话接口Unified Conversation Interfaces()**：所有智能体共享发送、接收和生成回复的接口。

#### 2. 开发者代码(Developer Code)
这一部分展示了设置和定制智能体之间互动的步骤。
- **定义智能体(Define Agents)**：
  - 定义了两个智能体，**用户智能体A**和**助手B**。它们可以相互通信，形成多智能体对话的基础。
- **注册自定义回复函数(Register a Custom Reply Function)**：
  - 为一个智能体（智能体B）注册了一个自定义回复函数（reply_func_A2B）。这个函数概述了智能体B在被调用时如何生成回复。
  该函数包括一个简单的逻辑结构：

```python
def reply_func_A2B(msg):
    output = input_from_human()
    if not output:
        if msg includes code:
            output = execute(msg)
    return output
```

这个功能允许智能体B要么从人类获取输入，要么如果输入消息包含可执行命令则执行代码。

- **发起对话(Initiate Conversations:)**：显示了一个示例启动行：
  `initiate_chat("Plot a chart of META and TESLA stock price change YTD.")`

#### 3. 程序执行(Program Execution)
本部分详细介绍了初始化后对话的进行方式。
- 对话驱动的控制流程(Conversation-Driven Control Flow)：
  - 交互从智能体A向智能体B发送请求开始。
  - 然后，智能体B接收请求并调用generate_reply函数，如果需要，可能会触发代码执行。
- 以对话为中心的计算(Conversation-Centric Computation)：
  - 流程展示了如何在generate_reply和智能体之间传递消息：
    - 例如，尝试执行命令后，如果缺少必要的包，将发送错误消息回去（例如，错误：未安装包yfinance）。
    - 然后回复会通知用户安装缺失的包（“抱歉！请先执行pip install yfinance，然后再执行”）。

简而言之，它展示了如何使用AutoGen编程智能体之间的对话驱动交互。这个过程包括定义智能体，通过回复函数定制它们的行为，以及处理对话控制流程，包括执行代码和响应用户请求。

AutoGen智能体、开发者代码和程序执行旨在指导开发者设置自动化的多智能体交互，从定义和定制智能体到观察对话和控制流程的执行。

### AutoGen实践

![实践型智能体AI多智能体模式](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-13T180830.313-3.webp)
来源：AutoGen智能体的基本概念 | DeepLearning.ai

在这里，我们将讨论智能体AI多智能体对话（这是受[Deeplearning.ai](https://www.deeplearning.ai/)启发的）。我正在使用AutoGen，它有一个内置的智能体类称为“可对话智能体”。

让我们从设置开始。

```python
!pip install openai
# python==3.10.13
!pip install pyautogen==0.2.25
import os
os.environ['OPENAI_API_KEY']='Your_API_Key'
llm_config = {"model": "gpt-4o"}
```
#### 定义一个AutoGen智能体

`ConversableAgent` 类用于创建一个聊天机器人智能体。`human_input_mode="NEVER"` 表示在对话过程中，智能体不会请求手动用户输入。


```python
from autogen import ConversableAgent
agent = ConversableAgent(
   name="chatbot",
   llm_config=llm_config,
   human_input_mode="NEVER",
)
reply = agent.generate_reply(
   messages=[{"content": "You are renowned AI expert. Now Tell me 2 jokes on AI .", "role": "user"}]
)
print(reply)
```

![Output](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-13T181615.501.webp)

```python
reply = agent.generate_reply(
   messages=[{"content": "Repeat the joke.", "role": "user"}]
)
print(reply)
```

**输出**

```
Certainly! Could you please tell me which joke you'd like me to repeat?
```
#### 设置对话

在Sunil和Harshit两个智能体之间设置对话，保留他们互动的记忆。

Harshit和Sunil是AI驱动的智能体，旨在进行吸引人、幽默的对话，专注于社交媒体报告。Harshit是一位社交媒体专家和办公室喜剧演员，他使用轻松、充满幽默的语言来保持对话的活跃。Sunil作为内容部门的负责人和Harshit的上级，也具有这种喜剧特质，通过以最后一个笑点开始笑话来增加结构化的幽默。两个智能体都使用预先配置的LLM设置，并且自主运行（human_input_mode="NEVER"）。这种动态模拟了工作场所的闲聊，将专业讨论与娱乐相结合，非常适合培训、团队模拟或内容生成。持续的、幽默的交流模仿真实的办公室互动，增强了参与度和共鸣。

`ConversableAgent` 通常是一种能够基于预定义的系统消息和配置进行对话的人工智能智能体。这些智能体使用[大型语言模型（LLMs）](https://www.analyticsvidhya.com/blog/2023/03/an-introduction-to-large-language-models-llms/)提供的[自然语言处理（NLP）](https://www.analyticsvidhya.com/blog/2024/08/nlp-in-modern-chatbots/)能力，根据他们的系统消息指令智能地回应。


```python
Harshit = ConversableAgent(
   name="Harshit",
   system_message=
   "Your name is Harshit and you are a social media expert and do stand-up Comedy in office."
   "Also this is a office comedy"
   "this conversation is about social media reports"
   "Keep the language light and Humour high",
   llm_config=llm_config,
   human_input_mode="NEVER",
)

Sunil = ConversableAgent(
   name="Sunil",
   system_message=
   "Your name is Sunil and you are head of content department in Analytics Vidhya, Harshit is your Junior and you also do stand-up comedy in office. "
   "Start the next joke from the punchline of the previous joke."
   "Also this is a office comedy and Harshit is Sunil's Junior"
   "This must be funny and not so lengthy"
    "this conversation is about social media reports",
   llm_config=llm_config,
   human_input_mode="NEVER",
)
```

两个智能体，Harshit和Sunil，通过他们独特的属性、个性和背景来定义。根据他们的角色，他们被指示进行幽默的互动。

```python
chat_result = Sunil.initiate_chat(
   recipient=Harshit,
   message="I'm Sunil. Harshit, let's keep the jokes rolling.",
   max_turns=3,
)
```


Sunil开始与Harshit进行对话，发起了一条初始消息，并限制了3个对话回合。


```python
import pprint
pprint.pprint(chat_result.chat_history)
```

**输出**

```sh
[{'content': "I'm Sunil. Harshit, let's keep the jokes rolling.",

  'role': 'assistant'},

 {'content': "Sure, Sunil! Let's talk about social media reports—basically "

             'where numbers and hashtags collide in a dance-off. You know, '

             'those analytics graphs are like the weather in North India; they '

             'change every five minutes, and somehow they always predict doom. '

             "But don't worry, you're not going to need an umbrella, just a "

             'strong stomach!',

  'role': 'user'},

 {'content': "That's true, Harshit! Those graphs change more often than I "

             'change my favorite Mughal Darbar biryani place. Speaking of '

             'change, did you hear why the social media influencer went broke? '

             "Because they took too many selfies and couldn't afford to pay "

             'attention! But honestly, our reports are a bit like that '

             'influencer—always needing a new filter to look good.',

  'role': 'assistant'},

 {'content': "Haha, that's spot on, Sunil! Our social media reports have more "

             'filters than my "best selfie of 2023" folder—and somehow, they '

             'still look like they woke up on the wrong side of the algorithm! '

             "It's amazing how on Instagram we strive to make our lives look "

             'perfect, while in our reports, we strive to make the numbers '

             "look believable. It's like magic, but with less prestige and "

             'more caffeine!',

  'role': 'user'},

 {'content': 'Absolutely, Harshit! Our reports are like those reality TV '

             'shows—the drama is real, but the numbers, maybe not so much. And '

             "trust me, the only time I'll ever willingly pull an all-nighter "

             'for a report is if it promises a plot twist, like turning red '

             "numbers to black! Speaking of which, why don't our reports ever "

             "go on silent mode? They're always sending alerts at odd hours "

             "like they're auditioning for a horror movie!",

  'role': 'assistant'},

 {'content': 'Haha, Sunil, I completely agree! Our reports could definitely '

             'headline a suspense thriller: "The Metrics That Never Sleep." '

             'Just when you think you can relax, bam! An alert jumps out like '

             'a cheap jump scare, reminding you that your engagement rate is '

             "working harder than you are! And let's not even get started on "

             "the notifications. They're like that one friend who keeps "

             'showing up unannounced with extra enthusiasm and zero regard for '

             'your personal space—or your night’s sleep!',

  'role': 'user'}]
```

#### 对于聊天终止
这段代码是定义聊天机器人智能体Harshit and Sunil中，他们扮演脱口秀喜剧演员的角色。目标是定制他们的行为，特别是他们如何处理对话终止。通过指定终止消息，机器人可以在遵循预定义的提示，如“I gotta go.”（我得走了）的情况下，自然地结束他们的互动。

这有助于：
- **提升用户体验：**用户获得更直观、更类人的互动，以一种清晰且易于理解的方式结束对话。
- **保持流畅和幽默：**由于这些智能体是脱口秀喜剧演员，用俏皮的短语管理他们的退场台词符合他们的角色，并增强沉浸感。


```python
Harshit = ConversableAgent(
   name="Harshit",
   system_message=
   "Your name is Harshit and you are a stand-up comedian. "
   "When you're ready to end the conversation, say 'I gotta go'.",
   llm_config=llm_config,
   human_input_mode="NEVER",
   is_termination_msg=lambda msg: "I gotta go" in msg["content"],
)
Sunil = ConversableAgent(
   name="Sunil",
   system_message=
   "Your name is Sunil and you are a stand-up comedian. "
   "When you're ready to end the conversation, say 'I gotta go'.",
   llm_config=llm_config,
   human_input_mode="NEVER",
   is_termination_msg=lambda msg: "I gotta go" in msg["content"] or "Goodbye" in msg["content"],
)
chat_result = joe.initiate_chat(
   recipient=cathy,
   message="I'm Sunil. Harshit, let's keep the jokes rolling."
)
```


### MetaGPT Agents：标准操作程序中多智能体的元编程
![MetaGPT Agents：标准操作程序中多智能体的元编程](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-13T183154.970.webp)来源：[MetaGPT与真实世界人类团队之间的软件开发SOPs。](https://arxiv.org/pdf/2308.00352)

**[MetaGPT](https://github.com/geekan/MetaGPT)** 是一个使用大型语言模型进行多智能体协作的框架，旨在通过**标准化操作程序(Standard Operating Procedures, SOPs)**复制类似人类的工作流程。这种方法通过结构化LLM交互，减少逻辑不一致性和幻觉，从而提高问题解决能力。MetaGPT分解复杂任务，分配专门角色，并通过定义的输出确保质量。在代码生成基准测试中，它优于现有的系统，如 [AutoGPT](https://autogpt.net/) 和 [LangChain](https://www.langchain.com/)，展示了一个强大且高效的软件工程元编程解决方案。

#### 结构化方法学与SOP驱动的流程
MetaGPT通过融入模仿标准操作程序（SOPs）的结构化方法学，实现了元编程的突破。这个基于GPT模型构建的创新框架要求智能体产生详细且结构化的输出，如需求文档、设计工件和技术规格。这些输出确保了沟通的清晰性，并在协作过程中最小化错误，有效提高生成代码的准确性和一致性。MetaGPT中的SOP驱动流程组织智能体协同工作，类似于软件开发公司中的精简团队，严格的标准规定了交接流程，减少了智能体之间不必要的交流。
#### 角色区分与任务管理
通过定义产品经理、架构师、工程师、项目经理和QA工程师等专门角色，MetaGPT将复杂任务编排成可管理的具体行动。这种角色区分促进了项目的有效执行，每个智能体都贡献其专业知识并保持结构化沟通。整合这些实践使得协作过程更加无缝和有效，限制了重复消息或误解等可能阻碍进展的问题。
#### 通信协议与反馈系统
MetaGPT还以其创新的通信协议而著称，该协议允许智能体通过结构化界面和发布-订阅机制交换目标信息并访问共享资源。一个独特的特点是可执行的反馈系统，它不仅在运行时检查代码，还优化并运行代码，显著提高生成输出的质量和可靠性。
#### 应用以人为中心的做法
应用以人为中心的做法，如SOPs，加强了系统的健壮性，使其成为构建基于LLM的多智能体架构的强大工具。这种在协作框架内使用元编程的先驱做法，为人工智能体之间更加规范和类似人类的互动铺平了道路，将MetaGPT定位为多智能体系统设计领域的先进方法。
### MetaGPT中的软件开发过程
![MetaGPT中的软件开发过程](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-13T183339.003.webp)来源：[MetaGPT](https://arxiv.org/pdf/2308.00352)

图中展示了MetaGPT如何通过实施标准化操作程序（SOPs)来管理软件开发过程。以下是图表的分解：

1. **人类输入**：过程从用户提供的项目需求开始，在这个案例中是创建一个2048滑动拼图游戏。
2. 产品经理（PM）
   - 产品经理进行彻底的分析并制定详细的产品需求文档（PRD）。
   - PRD包括**产品目标**、**用户故事**、**竞争分析**和**需求分析**。
   - 这种分析将用户需求分解成可管理的部分，并定义项目的主要目标、用户需求和设计考虑。
3. 架构师
   - 架构师接收PRD并将其转化为**系统设计**。
   - 设计包括**程序调用流程**、**文件列表**和软件组件结构的高层次计划。
   - 架构师确定组件如何交互，以及将使用哪些工具和框架（例如，使用[Python](http://python.org/)的Pygame进行游戏开发）。
4. 项目经理（PM）
   - 项目经理根据架构师的系统设计创建**任务列表**，并将工作分配给相应的智能体。
   - 这确保了任务被明确定义并与项目需求保持一致。
5. 工程师
   - 工程师根据详细计划实施指定的代码和功能。
   - 显示的代码片段突出了2048游戏核心逻辑的开发，包括必要的类和函数。
6. QA工程师
   - QA工程师审查并测试代码以进行质量保证。
   - 这一步确保游戏符合预定义的要求，并保持高功能性和可靠性标准。
7. 最终产品
   - 图表包括了最终输出的视觉表示，展示了用户
   与开发的游戏互动的方式。
   如图所示的工作流程强调了信息和工作任务从一位角色到另一位角色的顺序流动，展示了MetaGPT如何使用定义的SOPs来简化开发过程。这种结构化方法通过强制执行清晰的 角色、责任和标准通信实践，最小化了误解并最大化了生产力。
#### MetaGPT通过以下几项关键创新解决这些挑战
1. **元编程框架**：MetaGPT提供了一个独特的元编程方法，将结构化的人类式工作流程整合到多智能体交互中。这个结构化框架确保智能体遵循类似于人类解决复杂问题所使用的系统方法。
2. **标准化操作程序（SOPs）**：通过将SOPs编码到提示序列中，MetaGPT将多智能体系统的流程与明确定义的过程对齐。这导致了智能体之间更平滑的协作，并最小化了逻辑不一致性，因为这些SOPs指导智能体通过结构化过程。
3. **通过验证减少错误**：MetaGPT框架内的智能体被设计成模拟人类式的领域专业知识，能够验证中间结果并检查其输出的正确性。这个验证步骤对于减少典型LLM系统故障导致的错误至关重要。
4. **流水线范式**：MetaGPT引入了一种类似流水线的任务管理方法，不同的智能体被分配特定的角色。这种结构化的角色分配确保了复杂任务被分解成可管理的子任务，促进了多个智能体之间的协调努力，并提高了整体任务执行。
5. **在基准测试中提升性能**：在涉及协作软件工程基准测试的测试中，MetaGPT显示出能够产生比传统基于聊天的多智能体系统更连贯、更可靠的输出。这证明了其流水线结构和特定角色任务划分在实现更好任务结果方面的有效性。
多智能体系统需要MetaGPT来通过结构化、类似人类的工作流程管理复杂任务的复杂性，以减少错误和逻辑不一致性。通过采用SOPs、角色分配和中间结果验证，MetaGPT确保智能体协作高效，导致性能优越和任务完成的连贯性。
### 多智能体模式的优点是什么？
![多智能体模式的优点是什么？](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/11/unnamed-2024-11-13T183513.605.webp)
以下是多智能体模式的优点：
1. **通过协作提升性能**：部署多个共同工作的AI智能体通常比单个智能体产生更好的结果。智能体之间的协作努力可以导致改进的结果，正如在多智能体设置中表现更好的研究表明。
2. **提高专注度和理解力**：能够处理大量输入的大型语言模型（LLMs）可能仍难以理解复杂或冗长的信息。通过为不同的智能体分配特定角色，每个都可以专注于特定任务，从而提高整体理解力和有效性。
3. **优化子任务以提高效率**：将复杂项目分解为更小、更易于管理的子任务，允许每个智能体专门化和优化其分配的角色。这种针对性的方法确保了任务的每个部分都得到更精确和高效的处理。
4. **复杂任务的结构化框架**：多智能体模式提供了一种系统化的方法来分解复杂任务，类似于开发者在编程中使用过程或线程。这种结构简化了复杂项目的管理和执行。
5. **熟悉的管理类比**：管理AI智能体类似于组织中的管理者监督团队的方式。这个熟悉的观念帮助开发者直观地为智能体分配角色和责任，利用对团队动态的现有理解。
6. **灵活和动态的工作流程**：每个智能体都有自己的工作流程和记忆系统，允许与其他智能体进行动态互动和协作。这种灵活性使智能体能够参与规划、工具使用，并适应不断变化的要求，从而实现高效和复杂的工作流程。
7. **降低实验风险**：管理人类团队可能带来重大后果，但实验AI智能体的风险要小得多。这允许在优化智能体角色和交互时进行试错，而不会产生严重后果。
8. **有效利用资源**：将特定任务分配给专用的智能体确保计算资源得到有效使用。这种专注的分配防止了单个智能体的过载，并促进了工作负载的平衡分配。
9. **可扩展性和适应性**：多智能体方法允许通过按需添加或调整智能体来轻松扩展任务。这种适应性对于处理不同大小和复杂性的项目至关重要。
10. **提高问题解决能力**：智能体之间的协作互动可以带来创新解决方案和改进的问题解决。多个智能体的专业知识和观点可以揭示单个智能体可能错


这一领域的两个重要发展包括：

- **ChatDev – 软件开发中的通信智能体 [ChatDev – Communicative Agents for Software Development](https://arxiv.org/abs/2307.07924)** Qian et al. (2023) 展示了智能体如何在软件开发中协作，分配任务，并在流程中高效地工作。这些智能体相互沟通决策和进度，以确保工作流程顺畅。
- **AutoGen：通过多智能体对话启用下一代LLM应用 [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155)** Wu et al. (2023) 展示了一种创新的方法，利用LLM进行多智能体对话。在这种设置中，智能体通过对话共同得出解决方案，反映了AI智能体作为协调团队工作的潜力。.

## 5 参考

- [Agentic Reasoning Design Patterns in AI: Examples (vitalflux.com)](https://vitalflux.com/agentic-reasoning-design-patterns-in-ai-examples/)
- [Top 4 Agentic AI Design Patterns - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2024/10/agentic-design-patterns/)
- [What is Agentic AI Reflection Pattern? (analyticsvidhya.com)](https://www.analyticsvidhya.com/blog/2024/10/agentic-ai-reflection-pattern/)
- [What is Agentic AI Tool Use Pattern? - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2024/10/agentic-ai-tool-use-pattern/)
- [What is Agentic AI Planning Pattern? - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2024/11/agentic-ai-planning-pattern/)
- [What is Agentic AI Multi-Agent Pattern? - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2024/11/agentic-ai-multi-agent-pattern/)

