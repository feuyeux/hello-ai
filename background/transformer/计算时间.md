# 计算时间

**计算量** FLOPs(Floating point number operations) 浮点操作次数 用来衡量算法/模型时间的复杂度

全连接层的 FLOPs 计算：假设 $I$ 是输入层的维度，$O$ 是输出层的维度，对应全连接层（线性层）的权重参数矩阵维度为 $[I, O]$。

- 不考虑 bias，全连接层的 $FLOPs = (I + I -1) \times O = (2I − 1)O$
- 考虑 bias，全连接层的 $FLOPs = (I + I -1) \times O + O = (2\times I)\times O$

对于矩阵 $A\in\mathbb{R}^{1\times n}$，$B \in \mathbb{R}^{n\times 1}$，计算 $A\times B$ 需要进行 n 次乘法运算和 n 次加法运算，共计 2n 次浮点数运算，矩阵乘法操作对应的 FLOPs 为 $2n$。对于 $A \in \mathbb{R}^{m\times n}$，$B\in\mathbb{R}^{n\times p}$，执行矩阵乘法操作 $A\times B$，对应 `FLOPs` 为 $2mnp$。

**FLOPs 理论计算**

对于 transformer 模型来说，其计算量**主要**来自 `MHA` 层和 `FFN` 层中的矩阵乘法运算。先考虑 batch_size = 1 和 输入序列长度为 $s$ 的情况。

以下计算是针对每个 decoder layer 的计算量

1，通过线性变换计算 $Q$、$K$、$V$。

- 输入 token 序列长度为 $s$，则输入矩阵 $x$ 的形状为 $[s, h]$，其中 $h$ 是每个 token 的 embedding 维度。同时可知，做线性变换的权重矩阵 $W_Q$、$W_K$、$W_V$ $\in \mathbb{R}^{h\times h}$。
- qkv `FLOPs`: $3\times 2sh^2 = 6sh^2$

2，self-attention 层内部的计算，对输入矩阵进行操作。（其计算公式为: $\text{Attention}(Q, K, V) = \text{softmax} (\frac{QK^T}{\sqrt{d_k}})V$）这里只估算两个矩阵乘法的计算量，`attention_scale`（$/\sqrt(k)$）、`attn_softmax` ($\text{softmax}$) 的计算量忽略不计，因为这两个小算子都是逐元素操作。

- $QK^T$ 矩阵乘法的输入和输出形状为: [n_head, s, h//n_head] * [n_head, h//n_head, s] -> [n_head, s, s]，`Flops` 为 $2\cdot s^2h$。
- 计算在 `score` 上的加权 $score \cdot V$，矩阵乘法的输入和输出形状为: [n_head, s, s] * [n_head, s, d_model//n_head] -> [n_head, s, s, h//n_head]，`Flops` 同样也为 $2\cdot s^2h$。

3，`attention` 后的输出乘以输出投影（projection）矩阵。

- $z \in \mathbb{R}^{s\times h}$ 乘以 $W_o \in \mathbb{R}^{h\times h}$
- attn_output linear `FLOPs`: $2sh^2$

4，Feed-forward（MLP/FFN）层的计算量分析。包含两个线性层，以及一个 `relu` 激活层（逐元素操作，flops 很小$=5\cdot 4h$，可忽略）

- MLP 两个线性层的权重参数矩阵: $W_1 \in \mathbb{R}^{h\times 4h}$、$W_2 \in \mathbb{R}^{4h\times h}$，MLP 的输入矩阵: $\in \mathbb{R}^{s\times h}$。
- MLP `FLOPs`: $2\times (2 \cdot s \cdot h\cdot 4h) = 2\cdot 8 sh^2$

5，其他操作的计算量

- decoder block 后还有一个线性层，将隐藏向量映射为词表大小。以及顶部的 `softmax` 层用于输出每个 token 的概率。线性层的权重矩阵为：$W_{last} \in \mathbb{R}^{h\times V}$，矩阵乘法的输入和输出形状为: $[s, h] \times [h, V] -> [s, V]$。`FLOPs`: $2shV$。
- Layer Norm 操作是**逐元素**进行的，因此不存在通用的公式来精确 FLOPs。layer norm 层的两个权重都是一个长度为 $h$ 的向量，计算量预估为: $2h$。
- 另外，原始的 Transformer 模型使用余弦绝对位置编码方案，它是对 token embedding vector 进行加法操作。

将上述计算量相加，得到每个 decoder 层的计算量大约为: $(6sh^2 + 2sh^2 + 16sh^2) + 4hs^2 = 24sh^2 + 4s^2h$

**总结**，对于一个 $n$ 层的自回归模型，输入数据形状为 $[b, s]$ 的情况下，**一次训练/推理迭代的计算量**:

$$n\times (24bsh^2 + 4bs^2h) + 2bshV$$
> 忽略了向量-向量（甚至向量-标量）运算，这些运算的因子是 $h$ 远小于 $h^2$，因此可以忽略。

**计算量 FLOPs 的定性和定量结论**

当隐藏维度 $h$ 比较大，且远大于序列长度 $s$ 时，则可以忽略一次项，计算量可以近似为 $24bsh^2 * n$。前面提到当模型参数量为 $12nh^2$，输入的 tokens 总数为 $bs$（即上下文总长度），可得公式:$\frac{24nh^2}{12nh^2} = 2$。实际会有不到 `2%` 的误差，主要是因为我们忽略了一些小算子的计算量。

由此，我们可以近似认为：**在一次前向传播中，对于每个 `token`，每个模型参数，需要进行 $2$ 次浮点数运算，即一次乘法法运算和一次加法运算**。

一次迭代训练包含了前向传递和后向传递，后向传递的计算量是前向传递的 `2` 倍。因此，前向传递 + 后向传递的系数 $=1 + 2 = 3$ 。一次迭代训练中，对于每个 token，每个模型参数，需要进行 $6$ 次浮点数运算。

有了上述训练和推理过程中计算量与参数量关系的结论。接下来，我们就可以估计一次迭代训练 `GPT3-13B` 所需要的计算量。对于 GPT3，每个 token，每个参数进行了 $6$ 次浮点数运算，再乘以参数量和总 `tokens`数就得到了总的计算量。GPT3 的模型参数量为 12850M，训练数据量 300B tokens。

$$6 \times 12850 \times 10^6 \times 300 \times 10^9 = 2.313 \times 10^{22}$$

**推理 Latency 估算公式**

对于小 `batch` 的模型推理，单个 token 的推理 `latency` 受限于 gpu 的内存带宽；对于大 `batch`，单个 token 的推理 `latency` 受限于 gpu 的算力，同时将忽略卡与卡之间的通信延迟因素。

本章 Latency 的计算忽略了**预填充阶段**中计算和**读取 kv cache 的时间、读取 unembedding vector 并计算 logits 的时间**。预填充阶段对应的就是生成第一个 `token` 的过程，这个时候需要计算 `kv cache`，所以第一个 `token` 的 `latency` 会比后面的 `token` 大很多。

对于自回归模型的推理来说就是，**固定 seq_len**， 如果 seq_len *bs < ops:byte ratio* gpu_num，即**小 `batch_size` 范围 的 latency 不明显增加的**。

且模型推理的**解码阶段** Latency 的理论计算公式如下所示：

1，前提：**内存读取时间 > 计算时间**，一般是小 batch：

$$
\begin{align}
\text{compute} = \frac{2\cdot P}{N\cdot A_{bm}} \nonumber \\
\text{comms}  = 4\cdot n_{layers}\cdot 8us \nonumber \\
\end{align}
$$

2，前提：内存读取时间 < 计算时间，一般是大 batch：

$$
\begin{align}
\text{compute} = B\cdot \frac{2\cdot P}{N\cdot A_{f}} \nonumber \\
\text{comms}  = B\cdot \frac{2\cdot 4\cdot n_{layers}\cdot d_{model}}{A_c} \nonumber \\
\end{align}
$$

- $N$ 是 GPU 数目
- $A_c$ 是 GPU 之间通信带宽
- $A_{bm}$ 是 GPU 内存带宽
- $A_f$ 是 GPU 算力
- $P$ 表示模型(`float16`)参数量
- $B$ 是 `batch size`

注意，上述公式计算得到理论 `Latency` 只是个上限，我们永远不可能达到这个值，而且现实中 GPU 卡的性能也很少能达到其规格所宣称的数字。

**计算时间公式**
$$
计算密度(FLOPs/Byte)=\frac{计算量(FLOPs)}{访存量(Bytes)}
$$
$$
计算速度(FLOPs/s)=min(计算密度\times带宽，算力)
$$
$$
计算时间
=\frac{计算量}{计算速度}
=\frac{计算量}{min(\frac{计算量}{访存量}\times带宽,理论算力)}
$$
$$
\begin{equation}
计算时间=\left\{
\begin{aligned}
\frac{访存量}{带宽} [访存密集区] \\
\frac{计算量}{理论算力} [计算密集区]
\end{aligned}
\right.
\end{equation}
$$

----

OPS(operations per second 每秒处理次数) INT8类型的操作次数

FLOPS(Floating point number operations per second 每秒处理浮点数次数) FP32的操作次数

| INT8 | FP32   | 英文名称 | 每秒操作次数 | $10^n$    |
| ---- | ------ | -------- | :----------- | :-------- |
| MOPS | MFLOPS | mega     | 百万次       | $10^6$    |
| GOPS | GFLOPS | giga     | 十亿次       | $10^9$    |
| TOPS | TFLOPS | tera     | 兆(万亿)次   | $10^{12}$ |

- IEEE FP16(half-prcision)
- Google BF16(Brain Float 16)
- IEEE FP32(single-precision)

浮点数表示方式：

- sign 表示符号
- exponent 表示指数位（决定了动态范围）
- mantissa 表示尾数位（决定了表示精度）

FP8 的各种表示方式：E5M2、E4M3、E3M4、E2M5
