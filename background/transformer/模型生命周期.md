# 模型生命周期

## 1 基础

單精度浮點數(float point, FP) 32bit 4Byte

### 0 Transformer

注意力机制

$$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$$

多头注意力机制

$$\text{MultiHead}(Q, K, V) = \text{Concat}(head_1, head_2, ..., head_h)W^O$$

$$head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

## 2 模型生命周期

1. 预训练（Pretraining）
2. 对齐（Alignment） 对齐是指调整模型的输出，使其符合人类的指令和偏好。这通常涉及到使用人类反馈来训练一个奖励模型（Reward Model, RM），然后通过强化学习（RL）等方法来优化模型的输出。
   - [DPO](https://arxiv.org/abs/2305.18290)(Direct Preference Optimization, 直接偏好优化)不包括直接的奖励模型和强化学习过程，而是通过指令的偏好数据进行模型微调，将强化学习过程直接转换为类大模型SFT(Supervised Fine-tuning)的过程，从而使得训练更加快速和稳定。
   - [IPO](https://arxiv.org/abs/2310.12036)(身份偏好优化)
   - [KTO](https://arxiv.org/abs/2402.01306)(Kahneman-Tversky Optimization, 卡尼曼-特沃斯基优化)
   - [CPO](https://arxiv.org/abs/2401.08417)(Contrastive Preference Optimization)
   - [SimPO](https://github.com/princeton-nlp/SimPO)(Simple Preference Optimization )
3. 微调（Fine-tuning）微调是将预训练的模型进一步训练，以适应特定的任务或领域。这个过程可以在相对较小的针对性数据集上进行，目的是提高模型在特定应用场景中的性能表现。微调可以采取多种形式，包括基于少量样本的学习（Few-shot learning）、迁移学习（Transfer learning）和特定领域的微调（Domain-specific fine-tuning）。
   - 监督式微调
   - RLHF(Reinforcement Learning from Human Feedback, 人类反馈的强化学习)
   - RLAIF(AI反馈的强化学习)
   - PEFT(Parameter-Efficient Fine-Tuning, 参数高效微调技术) 如Adapter Tuning、Soft Prompts、LoRA(Low-rank adaptation, 低秩适应)等
4. 优化（Optimization）
   1. 子图融合
   2. 模型压缩（稀疏、量化、蒸馏）
      1. 模型稀疏化（Model sparsification，也叫模型剪枝 Model Pruning）
      2. 张量分解（Tensor Decomposition）
      3. 轻量化网络设计（Lightweight Network Design）
      4. 数值量化（Data Quantization，也叫模型量化）
      5. 知识蒸馏（Knowledge Distillation）
   3. 并行化（数据并行、张量并行、流水线并行）
   4. Transformer 结构优化
   5. 动态批处理
   6. KV cache 优化
   7. 解码优化
   8. 硬件升级等
5.  评估与迭代 在微调和优化的过程中，需要定期评估模型的性能，并根据反馈进行迭代改进。这包括监控模型在训练集和验证集上的表现，以及避免过拟合和数据泄露等问题。
6. 部署 经过预训练、对齐、微调和优化的模型，最终可以部署到实际应用中，为终端用户提供服务。
