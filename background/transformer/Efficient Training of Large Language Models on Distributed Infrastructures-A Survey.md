<!-- markdownlint-disable MD033 -->

# 揭秘大型语言模型训练：突破AI极限的最新技术与未来趋势

> [Efficient Training of Large Language Models on Distributed Infrastructures: A Survey](https://arxiv.org/pdf/2407.20018) Submitted on 29 Jul 2024
>
> 由段江飞、张硕、王泽瑞、江丽娟、屈文文、胡庆豪、王国腾、翁启振、严航、张兴成、邱兴鹏、林大华、文永刚、金新、张天威和孙鹏等作者共同撰写。
>
> 上海人工智能实验室·网络与分布式系统（NDS）研究小组由孙鹏博士领导，专注于为深度学习模型训练和部署开发高效的系统和架构，包括在数千个AI芯片上高效训练大型语言模型和多模态模型。
>
> 上海人工智能实验室·AI训练与计算中心（CAIF），由林大华教授和张兴成教授领导。该小组在分布式训练系统的创新方面取得了一定的成就，包括在OSDI、NSDI和ASPLOS等顶级会议上发表的研究成果。
>
> 本文基于 『未来科技潮 2024年08月06日 18:26 上海』的[翻译稿](https://mp.weixin.qq.com/s/aXtJ04wlfIZdtFnKy_LvBA)，补充缺失内容，并完善翻译。

翻译和整理：[六翁](https://github.com/feuyeux)

大型语言模型(如GPT和LLaMA)正以其高级能力革新人工智能行业。这些模型的训练需要庞大的GPU集群和大量的计算时间，这给可扩展性(scalability)、效率(efficiency)和可靠性(reliability)带来了重大挑战。本调研探讨了大型语言模型训练系统的最新进展，包括在AI加速器、网络、存储和调度方面的训练基础设施创新。此外，本调研还涵盖了并行策略，以及在分布式大型语言模型训练中的计算、通信和内存优化。同时，还包括了在长期训练期间保持系统可靠性的方法。通过考察当前创新和未来方向，本调研旨在为改进大型语言模型训练系统和解决持续挑战提供有价值的见解。此外，传统的基于数字电路的计算系统在满足大型语言模型的计算需求方面面临重大限制，这突显了对创新解决方案的需求，例如光学计算(optical computing)和光学网络(optical networks)。

## 1 介绍

大型语言模型(LLM)正在改变人工智能行业，展示了在包括个人助手[personal assistants](1)、代码辅助[code copilot](2)、芯片设计[chip design](3)和科学发现[scientific discovery](4)等广泛任务和应用中的卓越能力。这场革命的成功建立在像GPT[5]、LLaMA[6]、Gemini[7]等基于 Transformer 架构的LLM前所未有的规模之上。此外，已有证据表明，LLM的规模化尚未达到上限[plateaued](8)。这一趋势显著地改变了底层训练系统和基础设施的设计，因为LLM通常遵循相对固定的架构，并且其训练过程长期占用大规模GPU集群。例如，在Meta的生产集群上，使用16K H100-80GB GPU对LLaMA-3进行预训练大约需要54天[9]。

LLM训练对当今训练系统(training system)和基础设施(infrastructure)在“**SER**(Scalability, **E**fficiency, and **R**eliability)”方面提出了重大挑战，即可扩展性、效率和可靠性。**可扩展性**要求基础设施和系统能够无缝地适应包含数万个GPU或人工智能加速器的大规模集群，同时保持训练的正确性和模型的准确性。这需要在硬件配置(hardware configuration)、网络(networking)和训练框架(training framework)方面进行创新。**效率**关注于在整个集群中最大化资源利用率，通常以**模型浮点运算利用率**(**MFU**, Model FLOPs Utilization)来衡量。实现高MFU涉及到在史无前例的规模上优化计算、最小化通信开销以及高效管理内存。由于LLM训练的持续时间通常很长，短则数周，长则数月，因此**可靠性**至关重要。系统必须保持稳定的性能，并对各种故障(failures)保持韧性(be resilient)，包括硬件故障(hardware malfunctions)、网络问题(network issues)和软件错误(software errors)。系统应能够迅速检测( detecting)并从这些故障中恢复(recovering)，而不会显著损失进度或训练质量。这些相互关联的挑战需要对系统和基础设施设计采取全面的方法，推动大规模分布式计算的发展，并为高性能机器学习系统的研究和创新开辟新的途径。

本文旨在提供关于LLM训练系统和基础设施进展的全面概述，解决上述挑战。本调研涵盖了从分布式训练基础设施到训练系统。作者审视了基础设施设计的创新方法，包括为LLM工作负载定制的**高性能GPU集群**(advancements in GPU clusters)、**高速网络**(high-performance networking)和**分布式存储系统**(distributed storage systems)。作者也探讨了分布式训练系统的关键方面，包括**并行策略**(parallelism strategies)、**计算**(computation)、**通信**(communication)和**内存优化**(memory optimizations)，以提高**可扩展性**和**效率**。作者还深入探讨了提高训练**可靠性**的容错机制(fault tolerance mechanisms)。通过综合最近的进展并确定未来的研究方向，本调研旨在为研究行人和实践者提供关于改进LLM训练系统的最有前景的研究途径。作者的目标是提供一个有价值的资源，它不仅解决当前挑战，而且为大规模机器学习基础设施的未来创新铺平道路。

### 组织结构

图1展示了本调研的结构。第2节讨论了LLM架构的背景信息、LLM训练的特点和挑战。

- 第3节总结了**训练基础设施**的关键方面，包括人工智能加速器、网络基础设施和存储系统。
- 第4节作者研究了分布式LLM训练的**并行**方案。
- 第5节作者讨论了**计算优化**，以利用前所未有的计算能力。
- 第6节作者讨论了优化LLM训练**内存占用技术**。
- 第7节作者介绍了**通信优化**，以最小化通信开销。
- 第8节，作者首先呈现了**故障分析**，然后介绍快速故障检测和恢复的方法。
- 最后，在第9节作者总结了本调研。

<img src="Fig1.png" alt="Fig1.png" width="700" />

## 2 背景

### 2.1 基于 Transformer 的 LLM

当前最先进的大型语言模型(LLM)主要是基于 Transformer 的。其核心架构围绕注意力机制[10]构建，这使得模型能够动态地衡量句子中不同单词的重要性。图2展示了典型 Transformer 层的架构[10]，该层可以通过多层堆叠来构建LLM。输入文本首先被标记化为单个标记，然后通过嵌入层转换为标记向量。为了保留文本的顺序性质，标记向量被嵌入位置信息。得到的标记向量随后被送入 Transformer 层，该层由一个**注意力块(Attention block)**和一个**前馈神经网络(FFN, Feed-Forward Neural Network)**块组成。

假设输入的标记向量是$X = [x_1, x_2,· · ·, x_n]$。这些标记首先通过线性变换转换为 Query 、键和值张量。注意力机制计算注意力输出的方式如下：

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

其中$d$是**键向量**的**维度**。这个公式确保了LLM可以通过计算值的加权和来关注输入序列的相关部分，权重来自 Query 和键之间的相似性。经过注意力层之后，输出被传递给FFN进行进一步处理。

如今，LLM通常遵循原始的仅解码器 Transformer 架构，但对注意力和FFN机制进行了修改以提高效率和性能。原始的注意力机制，即**多头注意力(MHA, Multi-Head Attention)**[10]，由于键值缓存，存在二次计算复杂度(quadratic computational complexity)和高内存消耗(high memory consumption)的问题。为了解决这些问题，提出了几种变体，如**多 Query 注意力(MQA,  Multi-Query Attention)**[11]、**组 Query 注意力(GQA, Group Query Attention)**[12]和**多潜在注意力(MLA,Multi-Latent Attention)**[13]。FFN组件的一个显著进展是**专家混合(MoE,  Mixture-of-Experts)**[14]、[15]架构，它采用了一种稀疏激活的FFN。在MoE中，对于每个输入，只有FFN层(或专家)的一个子集被激活，这显著降低了计算负载，同时保持了高模型容量。

<img src="Fig2.png" alt="Fig2.png" width="600" />

### 2.2 LLM 训练工作负载特性

大型语言模型(LLM)的训练工作负载特性(Training Workloads Characteristics)与传统深度学习工作负载显著不同，这主要归因于其复杂性和规模。这些独特的特性影响了训练系统的设计、性能、可扩展性和资源利用率。在此，作者强调LLM的一些关键差异和要求。

**(1) 同质模型架构(Homogeneous Model Architecture)。** 与先前采用各种模型架构(如**LSTM** [16]，**CNN** [17])针对不同任务的深度学习工作负载不同，LLM主要使用Transformer架构 [10]。像**GPT** [5]，**LLaMA** [6]，**InternLM** [18]和**MOSS** [19]这样的模型都共享这一共同基础。这种架构的一致性为针对特定模型架构优化系统性能提供了巨大的潜力。

**(2) 前所未有的规模和训练时长(Unprecedented Scale and Training Duration)。** LLM训练的规模是空前的，通常更新具有数百亿参数的模型，并使用达到太字节规模的训练数据集。这种规模需要跨大型GPU集群的分布式训练，并提出了保持高效率的挑战。此外，LLM的训练可能持续数周或数月，这要求有强大的容错机制和高效的检查点策略，以防止数据丢失并便于恢复中断的训练会话。

**(3) 专门的软件优化(Specialized Software Optimization)。** 为了适应LLM巨大的模型规模，专用系统实施了先进的技术以优化执行。例如，**Megatron** [20] 和 **Alpa** [21] 通过混合并行主义加速训练。**DeepSpeed** [22] 通过整合状态分片优化器来减少内存消耗。

**(4) 训练范式的转变(Shift in Training Paradigm)。** 传统深度学习工作负载遵循一种特定任务的范式，即在特定领域的数据上训练模型以完成如翻译等特定任务。相比之下，LLM采用自我监督的训练方法，在广泛的数据集上训练以创建基础模型，然后这些模型被适配用于各种下游任务。这一范式转变标志着模型开发流程的实质性变化，包括预训练和对齐阶段，与先前的DL工作负载相比，产生了独特的工作负载特性。从数据中心的角度看，LLM开发涉及许多与预训练相关的小规模工作负载，包括对齐(即微调)和周期性评估工作负载 [23]。

### 2.3 LLM 训练的挑战

大型语言模型(LLM)训练工作负载的独特特性引发了在开发高效训练系统和基础设施方面的重大挑战。这些挑战主要体现在三个关键领域：可扩展性、效率和可靠性。这些挑战直接源自LLM的巨大规模及其训练过程的复杂性，这需要创新性的解决方案，以推动分布式计算和机器学习系统的边界。以下作者详细介绍了这些挑战及其对LLM训练的含义：

**(1) 可扩展性(Scalablity)。** LLM的成功在很大程度上归功于其规模，随着LLM变得越来越大，性能通常会有所提高[8]。然而，模型规模的扩展带来了相当大的可扩展性挑战，因为训练LLM需要越来越大的GPU集群或专门的AI加速器。首先，构建可扩展的基础设施，提供巨大的计算能力和内存容量是必要的。这包括设计并部署大型GPU集群或专门的AI加速器、高性能网络以连接这些设备，以及能够处理大量数据集和模型检查点的分布式存储系统。挑战在于确保这些组件在大规模部署中高效协同工作，管理散热、功耗和硬件故障。其次，设计可扩展的训练系统，能够有效地并行利用大规模加速器至关重要。这包括设计并行化和通信算法，以在数千个加速器上实现近线性可扩展性，同时保持LLM的准确度。

**(2) 效率(Efficiency)。** LLM训练巨大的计算需求转化为高昂的训练成本，这使得最大化硬件和软件系统的效率变得至关重要。效率可以用模型FLOPs利用率(MFU)来衡量，该指标量化了系统有效利用可用计算资源的能力。然而，在大规模下实现高效率仍然是一个重大挑战。例如，LLaMA3在16K GPU上仅实现了38%至41%的MFU[9]，这突显了系统扩展时保持高利用率所面临的困难。最大化效率需要在并行性、计算、通信和内存方面进行优化。首先，分布式LLM训练的并行性需要精心设计以最小化通信需求。其次，优化的计算操作符和低精度算术对实现高GPU FLOPS利用率至关重要。第三，需要减少通信开销以减少GPU空闲时间。最后，需要高效的内存优化来在现有硬件上容纳LLM并减少重新计算的FLOPs浪费。

**(3) 可靠性(Reliability)。** 确保LLM训练在长时间内的可靠性至关重要。由于训练任务可以在由成千上万个GPU组成的大型集群上持续数周或数月，训练失败的概率会增加，因此需要快速故障检测和恢复机制以实现弹性的LLM训练。首先，LLM训练任务可能因各种错误而崩溃，这使得在成千上万的GPU中快速准确地识别故障原因变得困难。其次，由于训练的同步性质，LLM训练任务的挂起会导致所有GPU闲置，造成重大浪费。此外，一些微妙的异常，如冗余链路故障或拖后腿的设备，可能不会立即导致崩溃，但可能导致训练速度减慢。这种不稳定可能导致训练效率降低。为应对这些挑战，需要具备检测灾难性故障和性能降级能力的健壮异常检测系统。此外，实现能够无缝处理节点故障和网络问题的容错训练框架至关重要。

### 2.4 相关调研

本调研聚焦于基于 Transformer 的大型语言模型(LLM)的高效训练系统及基础设施，包括底层分布式基础设施的设计、并行主义范式、计算与通信的优化、高效内存管理以及训练系统的韧性。作者还研究了一些新兴工作负载的高效训练系统，如MoE这一有前景的高效LLM变体，以及微调，这是将LLM能力对齐的必要阶段。然而，本研究不包括有希望的LLM架构演变[24, 25]以及训练[26]、指令调优[27]和安全性对齐[28]的算法，这些算法旨在打造强大且安全的LLM。尽管先前的研究[29, 30, 31]讨论了LLM训练系统的某些方面，但它们的主要关注点并非高效训练系统与基础设施的设计。

- Wan 等人[29]旨在提供关于模型中心及数据中心的LLM高效进展的整体视图。
- Liu  等人[30]涵盖了LLM的训练和推理部署技术。
- Xu 等人[31]针对LLM开发中算法和系统层面的资源高效策略进行了讨论。
- 本文还探讨了量化LLM训练和高效LLM微调的方法，但作者的重点在于系统性方法。压缩和微调LLM的算法方法由Zhu 等人[32]和Han 等人[33]讨论。

本研究的讨论范围不包括高级优化算法[34]和分布式深度神经网络训练系统[35]。尽管Liang 等人[36]广泛回顾了自动并行方法，但他们的关注点是通用深度神经网络，而非特定的LLM。

## 3 LLM 训练的基础设施

在本节中，作者探讨了用于训练大型语言模型(LLMs)的基础设施设计，包括**加速器(accelerators)**、**网络(networks)**、**存储(storage)**以及**调度系统(scheduling systems)**(见图3)。

<img src="Fig3.png" alt="Fig3.png" width="500" />

### 3.1 人工智能加速器

大型语言模型(LLMs)的快速进步极大地得益于GPU和**人工智能加速器**的演变，这些加速器对于提升模型训练性能至关重要。

#### 3.1.1 NVIDIA Graphics Processing Unit (GPU)

英伟达GPU已成为分布式大型语言模型(LLM)训练不可或缺的组成部分，这归功于它们在处理并行计算方面的卓越能力。这些处理器配备了众多紧凑、高效的核心，能够同时执行众多任务。GPU的设计与LLM训练中的矩阵和向量操作完美匹配。它们支持包括FP32、TF32、FP16、BF16、FP8、INT8甚至FP4在内的多种数值精度格式。这使得研究行人能够在训练速度和准确性之间取得良好平衡，提高了LLM训练的效率[110]。英伟达的GPU编程语言(即CUDA)简化了在GPU上任务的划分和处理并行管理。这帮助研究行人充分利用GPU的强大功能来训练高级LLM。

一个典型的GPU由一系列**流式多处理器**(SMs, Streaming Multiprocessors)组成，每个SM内部有多个核心，它们共享一个指令单元，但能够并行执行不同的线程。每个SM内的共享内存有效支持线程之间的数据交换和同步，这对于优化LLM计算所需内存访问模式至关重要。此外，GPU配备了**高带宽内存**(HBM,  high-bandwidth memory)，这加速了数据传输并缓解了计算密集型任务中的内存访问瓶颈。最新的GPU架构，如英伟达的Ampere[37]、Hopper[38]和Blackwell[39]，不断推动LLM计算的边界。它们提供了增强的**内存带宽和容量**(memory bandwidth and capacity)、每秒浮点运算次数(FLOPS)的增加，以及像Tensor核心这样的**专用混合精度计算单元**(specialized mixed-precision computing units)。值得注意的是，英伟达的**Hopper架构**通过引入Transformer引擎[111]这一重大进步，该功能利用混合FP8和FP16精度来加速基于Transformer的LLM训练。

#### 3.1.2 Other AI Accelerators

分布式在AMD GPU上进行LLM(大型语言模型)训练已经成为现实，特别是在世界上首个exascale Level 的超级计算机**Frontier** [112]上。每个Frontier节点配备了8个MI250X [40] AMD GPU，每个GPU拥有64 GB的HBM内存，以及理论上的FP16峰值性能为191.5 TFLOPS。这种配置为高效训练万亿参数模型提供了无与伦比的机会。解锁这一潜力的关键在于将现有的基于CUDA的工具和框架适配到**ROCm**平台 [113, 114]。值得注意的是，已经开发了支持ROCm的FlashAttention [115]和FlashAttention2 [116]版本，使得注意力机制的执行效率得到提升。

为了训练LLM，已经开发了多种具备强大计算能力和软件优化的AI加速器。**GAUDI** [41]提供了一种异构计算架构，包括两个矩阵乘法引擎和一组完全可编程的张量处理器核心，能够有效地处理LLM训练操作。这款处理器支持使用384个GAUDI2卡 [117]训练拥有1750亿参数的GPT-3模型。谷歌的**TPUv4** [42]超级计算机拥有4096个芯片，在LLM训练中平均可达到峰值FLOPS的大约60%。Graphcore的**Bow Pod64** [43]，一个机架设置包含64个Bowclass IPUs，实现了22 petaFLOPS的性能。它支持使用256个IPUs训练GPT-3模型。**Cerebras CS-2** [44]是一种晶圆级深度学习加速器，包含850,000个处理核心，每个核心提供48KB的专用SRAM内存。它被用于训练Cerebras-GPT，这是一系列开放的计算最优语言模型 [118]。

图4：针对分布式LLM训练的基础设施优化研究。

<img src="Fig4.png" alt="Fig4.png" width="800" />

### 3.2 网络基础设施

**通信开销**(Communication overhead)是扩展大型语言模型(LLM)训练的主要障碍[119, 120]。例如，在训练过程中减少模型梯度可能导致超过90%的训练时间被用于通信[121]。为了解决这个问题，研究界专注于提升大型语言模型训练的通信基础设施(communication infrastructure)。

#### 3.2.1 芯片间通信

**芯片间通信**(Chip-to-chip communication)对于节点内AI加速器之间的数据传输至关重要，对LLM训练的效率有着显著影响。

图5：五种芯片间拓扑结构：树状拓扑、立方体网格拓扑、基于交换机的全连接拓扑、基于P2P的全连接拓扑和2D环面拓扑。

<img src="Fig5.png" alt="Fig5.png" width="800" />

传统上，这种通信依赖于PCI Express PCIe [122]，它采用**树形拓扑结构(Tree Topology)**——一种多个设备连接到单一根复合体的分层结构。多年来，PCIe提高了其带宽：**PCIe 3.0**每通道提供大约**1 GB/s**，16通道的配置总带宽约为16 GB/s；**PCIe 4.0**将带宽翻倍至**2 GB/s**每通道，而**PCIe 5.0**则进一步提高到**4 GB/s**每通道。尽管有了这些增强，PCIe在带宽、延迟和可扩展性方面的固有局限使其对于LLM训练来说不是最佳选择 [123]。

为了解决这些限制，像**NVLink** [45]这样的专用芯片间互联在LLM训练中越来越受到青睐。与PCIe相比，这些先进的互联通过使用各种拓扑结构(如：**立方体网格**(*cube-mesh*)、**全连接**( *fully-connected*)和**3D环面**(*3D-torus*))提供了显著更高的带宽和更低的延迟。此外，共享内存模型、专用通信协议和同步机制也发挥着关键作用。

**立方体网格拓扑(Cube-Mesh Topology)**。**NVLink-1.0** [45] 提供 GPU 之间的直接高速连接，每个链路提供 **160 GB/s** 的双向带宽。该架构能够为四个 GPU 形成平面网格结构，为八个 GPU 形成立方体网格拓扑，可配置到 **DGX-1** 服务器中。这种立方体网格配置虽然不是全对全连接(all-to-all connection)，但显著提高了 GPU 上的数据通信效率和训练性能。

**全连接拓扑(Fully-Connected Topology)**。许多互连利用基于交换机或基于 P2P 的全连接拓扑来提高芯片到芯片的通信性能。NVIDIA 使用 **NVSwitch** [46] 实现 GPU 之间基于交换机的全对全互连。在 **DGX-2** [124] 系统中，六个 NVSwitches 将 16 个 GPU 中的每一个完全连接到所有其他 GPU，在任意两个 GPU 之间提供 **300 GB/s** 的双向带宽。使用 **NVSwitch 2.0**，此带宽增加到 **600 GB/s**，使用 **NVSwitch 3.0**，带宽进一步增加到 **900 GB/s**。英特尔、AMD 和华为 Ascend 在其加速器中使用基于 P2P 的全连接拓扑，其中每个芯片使用以太网或 Infinity Fabric 直接连接到同一节点内的每个其他芯片 [47]。与基于交换机的拓扑相比，基于 P2P 的拓扑中两个 GPU 之间的带宽受到直接连接链路带宽的限制。

**2D/3D 环面拓扑(2D/3D-Torus Topology)**。Google 的 TPU 系统利用环面网络拓扑 [59] 进行芯片间通信。它通过将每个 TPU 芯片连接到网格中的四个相邻芯片来建立连接，边缘环绕( wraparound edges,)，从而形成环形结构(toroidal structure)。由于芯片之间有多条直接路径，这种架构设计可确保低延迟和高带宽。具体而言，**TPUv2** [48] 超级计算机采用 16x16 2D 环面配置，包含 256 个芯片，通过**高速芯片间互连** (high-speed ICI, Inter-Chip Interconnect) 链路互连。**TPUv3** [49] 超级计算机采用 32x32 2D 环面，包含 1024 个芯片。从 2D 环面设计发展而来，**TPUv4** [42] 超级计算机将计算资源组织成具有 3D 环面拓扑的多机立方体。每台 TPU 机器包含四个芯片，排列成 2x2x1 网格，通过 ICI 链路互连。16 台 TPU 机器组合在一起形成一个数据中心机架，机架内的 ICI 链路互连形成 4x4x4 网格，形成 3D 圆环结构。这种先进的配置显著提高了通信效率和可扩展性，对 LLM 培训尤其有益。

#### 3.2.2 节点间通信

**远程直接内存访问(RDMA, Remote Direct Memory Access)** [54] 能够在节点间实现高速、低延迟的数据传输。RDMA允许不经过任何节点的操作系统，直接从一个计算机的内存访问另一个计算机的内存。**GPUDirect-RDMA** [50]通过使GPU在不同节点间直接通信，完全绕过CPU，从而增强了这一过程。这项技术对于大型语言模型(LLM)的训练特别有益，因为它加速了模型参数和梯度的同步。目前最流行的两种RDMA技术是**InfiniBand** [51]和**基于融合以太网的RDMA(RoCE, RDMA over Converged Ethernet)** [52]。

InfiniBand是一种在高性能计算(HPC)环境中广泛使用的高速、低延迟网络技术，例如Eagle超级计算机[125]。这种技术需要专用的网络基础设施，体现了其致力于提供卓越性能的设计理念。多年来，InfiniBand在带宽能力方面有了显著的进步，从**增强数据速率**(**EDR**, Enhanced Data Rate)的100 Gbps发展到**高动态范围**(**HDR**, High Dynamic Range)的200 Gbps，最近更是提升到每链路400 Gbps的**下一数据速率**(**NDR**, Next Data Rate) [126]。RoCE利用现有的以太网基础设施提供RDMA功能。这种方法提供了一个更具成本效益且易于部署的解决方案，尤其是在已经使用以太网的数据中心。**RoCE**有两个版本：**RoCE-v1** [52]，它作为以太网链路层协议运行；以及**RoCE-v2** [53]，它基于UDP运行。行业领导者如字节跳动和Meta已经采用这些技术来扩展LLM训练。另一种RDMA协议，即**互联网广域RDMA协议**(**iWARP**, Internet Wide Area RDMA Protocol) [54]，支持在TCP/IP网络上进行RDMA。然而，由于其相对有限的性能，iWARP并不常用于分布式LLM训练[127]。

#### 3.2.3 网络拓扑

在大型语言模型(LLM)训练集群中，网络架构被划分为前端和后端组件(图3)。前端网络处理多种流量，如作业管理、模型推理和存储活动，而后端网络专门处理训练过程中产生的大量流量。作者在优化LLM训练时的主要关注点是提高后端网络的性能和效率，以便将AI加速器扩展到数万个。

图6：大规模GPU集群中的四种典型网络拓扑：Clos拓扑，Dragonfly+拓扑，轨道优化拓扑和仅轨道拓扑。

<img src="Fig6.png" alt="Fig6.png" width="800" />

**高性能计算(HPC)网络拓扑(HPC Network Topology)。** 高性能计算环境中的传统拓扑也可以用于AI集群进行分布式训练，例如**Clos** [55]，**BCube** [56]，**DCell** [57]，**Jellyfish** [58]，**Torus** [59]，**Dragonfly** [60] 和 **Dragonfly+** [61]。**Clos网络架构**，通常被称为Fat-Tree拓扑，在LLM训练集群中被广泛使用。在基于Clos的集群中，每台服务器配备一个或多个网络接口卡(NIC)，按机架组织，连接到叶交换机。这些叶交换机与脊柱交换机相连，提供机架间的连接并形成一个单元。单元通过核心交换机进一步互连，使得集群内服务器之间可以进行任意对任意的通信。例如，Meta公司上一代的GPU集群架构，支持高达24000个GPU，由八个单元组成，它们之间具有全带宽，并在核心层使用7:1的超额订阅比例[9]。Meta公司使用24000个GPU在此集群上训练**LLama 3** 405B。

**训练优化拓扑(Training-Optimized Topology)。** 许多网络拓扑是与分布式训练算法共同设计的。**轨道优化拓扑**(rail-optimized topology) [62]增强了GPU到叶交换机的连接。在每个轨道内，不同服务器上共享相同索引的GPU通过同一个叶交换机相互连接。这种配置通过减少数据流之间的网络干扰来提高集体通信性能。**SuperPod** 架构采用轨道优化网络，能够连接超过16,000个GPU[128]。字节跳动在其**MegaScale**系统设计中采用三层轨道优化网络(three-layer rail optimized network)来连接超过10,000个GPU[71]。然而，轨道优化网络设计可能效率较低，因为它们需要将GPU连接到远端的交换机，这需要昂贵且耗电的光学收发器。这些光学部件增加了功耗和热量，导致网络故障率更高，这对于分布式LLM训练来说是非常重要的。阿里巴巴进一步优化了轨道优化拓扑，提出了一个名为**HPN**[63]的两层双平面架构。该架构采用最新的51.2Tbps单芯片交换机，支持一级网络中的1,000个GPU，以及一个Pod内的最多15,000个GPU。

对GPT/OPT-175B模型训练的网络流量分析显示，99%的GPU对不承载任何流量，不到0.25%的GPU对处理流水线/张量并行和数据并行流量[64]。基于这些发现，仅轨道拓扑[64]消除了轨道优化网络中不同轨道之间的连接。每个轨道通过一个专用的但分离的Clos网络连接。不同轨道上GPU之间的通信通过内部芯片间互连转发数据来管理。这种方法可以在保持性能的同时有效降低成本。**HammingMesh**[65]将GPU组织成具有2D环面拓扑的组，并通过稀疏连接的交换机连接这些2D环面组。这种设计旨在节省成本而不损害训练性能。考虑到仅通过PCIe连接的GPU，**BiGraph**[129]提出了一种新的网络架构，将节点内GPU通信导出到节点外部，绕过PCIe带宽瓶颈。它具有通过Clos架构互连的两层网络，具有独特的最短通信路径，并支持应用程序控制的流量路由。

**可重构拓扑(Reconfiguable Topology)。** 可重构网络可以动态调整以优化通信模式，从而提高训练性能。它们通常利用**光开关**(optical switching)和定制配置来提高网络基础设施的带宽利用率、灵活性和可扩展性。由**硅光子**(**SiP**, Silicon Photonic)接口驱动，**SiP-ML** [66] 推进了两种主要架构：**SiP-OCS**和**SiP-Ring**。**SiP-OCS**采用完全连接的配置，通过使用商业上可获取的光学电路开关最大化带宽，并通过Tbps的SiP接口将GPU连接到所有开关。相比之下，**SiP-Ring**使用无开关的环形配置，通过在SiP接口内集成微环谐振器来降低重构延迟。Wang等人提出了**TopoOpt** [67]，用于协同优化分布式训练中的网络拓扑和并行化策略。这种方法不仅优化了计算和通信需求，还解决了网络拓扑的物理层问题。TPUv4 [42] 特性光学电路开关(OCS, Optical Circuit Switches)，允许动态重构基于3D-Torus的互连拓扑，优化了适应LLM训练中多变且密集通信模式的数据流。例如，使用512个芯片，TPUv4在3D-Torus拓扑上提供了如4x4x32或8x8x8的灵活性。

#### 3.2.4 负载均衡和拥塞控制

**负载均衡(Load Balancing)。**LLM训练的网络流量特点是少量的**大象流**(elephant flows)。具体来说，由于梯度同步，LLM训练显示出周期性的网络流量爆发[63]。每次爆发都需要大量的网络带宽。此外，参与LLM训练的计算节点生成的连接数量非常少[63]。传统的负载均衡技术，如**等成本多路径路由**(**ECMP**, Equal-Cost Multi-Path routing) [68]，使用哈希算法在等效路径上均匀分配流量，例如在Clos拓扑中从叶子交换机到脊柱交换机。然而，这种基于哈希的方案对于处理由少量大象流组成的LLM训练流量效率低下。当多个大象流被路由到同一链路时，可能导致拥塞和高延迟。

为了解决大规模GPU集群中的负载均衡挑战，已经开发出各种策略。在**Llama 3** 405B训练期间，集体库(collective library)在两个GPU之间建立16个网络流，而不是一个单独的流，从而减少了每个流的流量并增强了负载均衡的机会[9]。此外，增强型ECMP(E-ECMP)协议通过在数据包的RoCE Head 额外字段进行哈希，有效地将这16个流分配到不同的网络路径。数据包喷洒[69]将流的包分配到所有可用的并行链路，这可能导致包乱序。网络接口卡(NIC)需要处理乱序的RDMA数据包。基于LLM训练的流量模式，**Difreal**[70]通过贪婪地为每个流分配路径，可以证明能够将负载均匀地分布在所有网络路径上，并解决ECMP哈希冲突问题。在一个大规模GPU集群中，HPN[63]通过识别精确的不相交等路径，在集体通信库内实现负载均衡。**MegaScale**[71]显示，轨道优化拓扑也可以减少ECMP哈希冲突。

**拥塞控制(Congestion Control)。**在RDMA集群中，无损传输至关重要。基于优先级的流量控制[**PFC**, Priority-based Flow Control](72)是一种防止数据包丢失的流量控制机制。当下游设备上的PFC启用的队列出现拥塞时，设备会指示上游设备停止队列中的流量，从而确保数据包零丢失。由于PFC是一种粗粒度机制，它可能导致队头阻塞(head-of-line blocking) [130]，这会显著降低网络吞吐量。为了解决这些挑战，已经开发出各种通用拥塞控制方案。这些技术包括**TIMELY**[73]，数据中心量化拥塞通知[**DCQCN**, Data Center Quantized Congestion Notification](75, 76)，Swift[74]，高精度拥塞控制(**HPCC**, High Precision Congestion Control) [77]，边缘排队数据报服务(**EQDS**, Edge-Queued Datagram Service) [78]]，以及鲁棒拥塞控制(**RoCC**, Robust Congestion Control) [79]。这些方案监控网络拥塞，调整数据速率以减轻拥塞，并在最小化吞吐量降低的情况下恢复速率。

当有并发训练任务时，许多拥塞控制方案利用流量爆发和周期性模式有效地交错网络流量。**MLTCP**[80]基于一个关键洞察：训练流应基于每次训练迭代的发送字节数调整其拥塞窗口大小，来交错竞争带宽的任务的通信阶段。**CASSINI**[81]通过考虑不同任务的通信模式，优化了网络链接上的任务放置。**MLT**[82]利用了LLM训练的特点，即早期层的梯度不如后期层重要，较大的梯度比较小的梯度更重要。因此，在通信拥塞事件中，MLT在交换机层面基于梯度内部的重要性优先排队或丢弃数据包，以减轻通信拥塞问题。

### 3.3 存储

存储系统在分布式大型语言模型(LLM)训练中扮演着至关重要的角色，需要满足几项关键要求。首先，它应与GPU的计算能力相匹配，以最大化其利用率，避免因存储瓶颈造成的资源浪费。其次，它应支持大规模结构化和非结构化训练数据集的存储，并在分布式处理环境中具备可扩展性。此外，在LLM训练中，模型检查点的存储和检索也带来了挑战，要求系统能够满足由模型大小和训练时长决定的读写带宽需求。最后，存储系统还应满足传统企业级需求，如数据保护、高可用性和安全性。

#### 3.3.1 Storage Systems for Checkpoint

在大型语言模型(LLM)训练中，模型检查点的大小极为庞大。随着参数数量的增加，需要写入的数据量也随之增多，这对存储系统的写入带宽提出了更高要求。例如，具有700亿个参数的LLM的检查点大小为980GB。在大型GPU数据中心部署了众多存储系统来管理模型检查点。Meta的分布式文件系统**Tectonic** [83]，能够让数千个GPU同时保存和加载模型检查点，为大规模训练操作提供了高效且可扩展的存储解决方案 [131]。在字节跳动，使用HDFS [84] 进行集中式模型检查点的维护，以确保大规模的一致性和可靠性 [71]。为了减轻检查点恢复期间的带宽瓶颈，一种常见的方法是指定一个单独的工作者从HDFS 读取检查点分区，然后广播给共享相同数据的其他工作者。分布式目标存储，如**Ceph**目标存储 [85]，提供了更易于扩展的优势。这一优势源于它们没有分层目录树或命名空间，简化了一致性的维护。由于这些优势，目标存储已广泛应用于模型检查点的存储。

#### 3.3.2 Storage Systems for Training Data

原始的用于大型语言模型(LLM)训练的数据集非常庞大。LLaMA 3模型是在超过15万亿个标记上进行训练的，这比**LLaMA 2**的数据集[6]大了七倍多。每个标记大约需要2个字节，相当于大约300TB的数据。为训练准备数据集涉及到广泛的预处理步骤，包括数据爬取和清洗，这需要大量的实验。通常，在处理这些步骤中所处理的数据量会超过最终训练数据集大小的100倍[132]。例如，**WanJuan-CC数据集**[132]有选择性地提取了大约680亿个文档，生成了大约1000亿个高质量标记，在摒弃了99%的原始数据后，相当于大约2TB的数据量。因此，用于LLM训练的总数据量预计将超过数十PB。

像**Lustre** [86]，**GPFS** [87]，和**BeeGFS** [88]这样的并行文件系统经常被部署在领先的高性能计算系统上，以确保高效的I/O、持久存储和可扩展性能。这些系统也广泛应用于训练集群中的数据加载，提供了必要的基础设施，以高效处理大规模训练数据。此外，文件系统还需要让工程师能够对使用数千个GPU的作业进行交互式调试，因为代码更改需要立即对所有节点[131]可见。

在大多数LLM的训练过程中，每个标记通常只遇到一次。然而，采用数据缓存仍然至关重要，以减轻在数据加载过程中的I/O瓶颈。这一策略包括从较慢的后端存储预取训练数据到较快的缓存存储。**Alluxio** [89]和**JuiceFS** [90]通过从底层存储系统(如HDFS或目标存储)高效缓存训练数据，提升了LLM的训练效率。**Quiver** [91]支持跨多个作业和用户对同一数据集上缓存的透明重用。**Fluid** [92]利用Alluxio进行数据缓存，并融入了一种根据I/O条件动态自动扩展缓存大小的机制。

### 3.4 调度

LLM训练任务通常在大型多租户基础设施上进行(例如，GPU集群、公共云)，在这些设施中，用户共享集群资源。有效的调度机制对于管理这些工作负载至关重要，它确保了资源的有效利用和任务的顺利执行[133]。与关注单个作业细粒度优化的任务级调度(例如，流水线调度[134, 135, 136])不同，集群级调度旨在优化整个集群的资源分配和任务调度。根据它们主要的优化方面，作者将现有的集群级调度系统分为两类：工作负载调度和资源调度。

#### 3.4.1 工作负载调度

近年来，针对深度学习训练工作负载的调度器已被积极研究[137, 138, 139, 93, 94, 95, 141, 96, 97, 98, 99, 140, 141]。为了提高资源利用率，通常实施了以下三个高级特性：

(1) *异构感知(heterogeneous-aware)* 调度器(例如，Gavel [96]，Gandviafair [97])专注于优化不同GPU代之间的作业分配；

(2) *作业打包(job-packing)* 调度器(例如，FGD [98]，Lucid [99])通过细粒度的GPU共享来充分利用硬件能力；

(3) *自适应缩放(adaptivescaling)* 调度器(例如，Pollux [100]，Sia [101])动态调整GPU的数量及训练超参数，以加速训练进度。然而，这些调度器是为通用深度学习工作负载设计的，由于LLM工作负载的独特特性[23]，它们可能并不直接适用于LLM。

为了更好地管理LLM工作负载，一些近期的研究提出了针对LLM定制的系统。**Crius** [102] 同时考虑了异构集群中的混合并行性(hybrid parallelism)(SS4.1)和硬件亲和性(hardware affinity)。它研究了在集群调度 Level 集成自适应并行性配置的工作流效率，为同时训练多个LLM提供了显著提高效率的机会。为了实现LLM高效的超参数调整，**Hydro** [103] 将模型缩减为较小的代理模型进行超参数搜索，然后将多个模型融合为一个实体以提高硬件利用率。此外，**Hydro** 通过将调优工作负载与启用流水线的LLM预训练任务交织，有效地扩展了调优工作负载的资源，充分利用了流水线气泡(pipeline bubbles)。**Acme** [23] 进一步表征了LLM开发流程中的工作负载混合，并提出了一个系统，以高效地调度与LLM训练相关的作业，包括解耦评估调度以获得及时的模型质量反馈，以及涉及LLM的故障诊断和自动恢复。

#### 3.4.2 资源调度

除了工作负载调度之外，关联资源调度(例如CPU、内存和网络)也是集群 Level 管理的另一个关键方面。在网络上，**Cassini** [81] 通过使用亲和图(affinity graph)确定调整通信阶段的时间偏移值，使得不同作业在上传和下载阶段能够交错带宽需求。**HIRE** [104] 引入了一种创新的数据中心交换机内的网络计算调度系统，显著减少了网络绕道和尾部放置延迟。对于存储，**SiloD** [105] 将数据缓存和远程I/O视为一等资源进行联合分配，显著提高了吞吐量。对于CPU和内存，**Synergy** [106] 通过优化CPU核心分配而不是依赖与GPU成比例的分配来提高训练效率。此外，一些研究工作专注于节能。**EnvPipe** [107] 利用流水线并行性中的气泡时间(bubble time)，通过降低SM频率来延长流水线单元的执行时间以节省能源。**Zeus** [108] 自动配置批处理大小和GPU功率限制，以提高训练期间的能源效率。**Perseus** [109] 引入了一种高效的基于图切割的迭代算法，用于获取大型模型训练作业的迭代时间-能量Pareto前沿。

## 4 LLM 训练的并行模式

随着LLM(大型语言模型)规模的不断增长，其对计算资源和内存容量的需求日益增加。利用大规模高性能计算(HPC)集群的分布式训练已成为高效训练这些模型的关键方法。在本节中，作者研究了一系列提高HPC集群在LLM训练中利用率的并行方案。作者将这些方法分为三个主要类别：混合并行(Hybrid Parallelism)、自动并行(Auto Parallelism)和异构并行(Heterogeneous Parallelism)。**混合并行**结合了多种手工设计的并行化策略，如数据并行( data parallelism)、张量并行(tensor parallelism)、流水线并行(pipeline parallelism)、序列并行(sequence parallelism)和专家并行(expert parallelism)。**自动并行**根据模型和硬件特性自动确定最优的并行化策略。**异构并行**利用硬件或模型中的异质性进行高效训练。这包括利用不同类型的加速器或利用单一模型内的异质性(例如，RLHF训练)来提高HPC集群上的整体训练效率。

图7：分布式LLM训练并行方案研究。

<img src="Fig7.png" alt="Fig7.png" width="800" />

当今大多数最先进的并行化策略采用了单程序多数据(**SPMD**, Single Program Multiple Data)编程模型，类似于消息传递接口(MPI)范式[239]，即相同的程序在多个处理器上运行，每个处理器处理不同的数据[225]。例如，数据并行、模型并行和序列并行采用SPMD编程模型。这种方法确保了操作的统一性和一致性，非常适合大规模分布式训练环境。一些策略尝试打破SPMD的限制，并采用多程序多数据(**MPMD**, Multiple Program Multiple Data)模型进一步提高资源利用率，在该模型中，不同的程序(或程序的不同部分)在不同的处理器上运行，处理数据或模型的不同部分[225]。例如，流水线并行在不同设备上运行LLM的不同部分。此外，自动并行和异构并行可以利用SPMD和MPMD模型来提高资源利用率。因此，作者根据并行发生的维度以及所使用的计算设备是否为同构或异构来讨论这些方法，而不是关注底层的编程模型。

### 4.1 混合并行

混合并行通常结合多种手工并行化策略来划分 LLM 的不同可并行化维度。这些策略包括数据并行、张量并行、流水线并行和序列并行，如图 8 所示。数据并行、张量并行和流水线并行的组合也称为 3D 并行。

<img src="Fig8.png" alt="Fig8.png" width="800" />

#### 4.1.1 数据并行

数据并行性是分布式训练最常用的并行化策略，因为它具有高可扩展性和易于实现性。它遵循单程序多数据 (SPMD) 编程模型。数据并行性沿批处理维度对输入训练数据进行分区，其中每个 GPU 处理其分配的数据段，如图 8(a) 所示。在整个训练过程中，数据首先逐层使用完整模型权重进行前向计算，然后按相反顺序执行后向计算。每一层都会产生梯度，这些梯度将使用集体通信操作在所有 GPU 上聚合以进行优化器更新。
数据并行性结合了各种分片策略，这些策略会显著影响内存占用和通信开销。假设全局世界大小为 W（即设备数量），则引入分片因子 F 来控制所使用的分片策略 [146]，定义为参数分区的设备数量（1 ≤ F ≤ W）。我们有以下场景。

**完全复制(Full replication)**（F = 1）：此分片策略简化为原始数据并行性。**Pytorch-DDP** [240] 和 **Horovod** [143] 在所有设备上完全复制模型，并使用 All-Reduce 进行梯度聚合。它们还将梯度分成小桶，以将梯度通信与后向计算重叠。

**完全分片(Full sharding)**（F = W）。这种分片策略的内存消耗最低，但通信开销最大（是原始数据并行的 1.5 倍）。完全分片策略将模型完全分片，其中每个设备仅保存 1 W 个模型参数。在计算之前，会按需传递和恢复完整的权重和梯度，并在计算之后立即丢弃。**ZeRO-3** [145] 采用按参数分片对整个模型进行分片，并分别使用 All-Gather 和 ReduceScatter 进行非分片和分片通信。分片权重更新 [144] 也采用按参数分片，但更侧重于在所有设备上分片冗余参数更新计算。**FSDP**（完全分片数据并行）[146] 通过在模块单元粒度中分片模型参数来实现相同的功能，并提供更用户友好的 API。

**混合分片(Hybrid sharding)**（1 < F < W）。在此策略 [146] 中，所有设备被划分为 N × M 设备网格。模型参数沿网格的 N 维进行分片，并沿 M 维进行复制。**MiCS** [147] 调用 All-Gather 集合来收集分片参数，并调用 All-Reduce 来聚合梯度。**FSDP** [146] 用 Reduce-Scatter 替换 All-Reduce，以减少内存和通信开销。与完全复制和完全分片相比，混合分片更灵活，可以通过根据模型架构和硬件约束调整 F 来在内存消耗和通信开销之间进行权衡。

#### 4.1.2 张量并行

张量并行（图 8(b)），也称为**层内模型并行(intra-layer model parallelism)**，是一种旨在跨多个 GPU 训练 LLM 的技术。它沿多个维度对每层的参数张量进行分区，从而有效地将模型参数分布在可用的 GPU 上。张量并行传递中间激活张量，其大小远小于数据并行传递的参数和梯度，长上下文 LLM 训练场景除外。
然而，在张量并行中，将通信与计算重叠是一项挑战，需要使用高带宽连接。因此，张量并行更常用于单个 GPU 节点。

根据分区的维数，张量并行可分为 1-D [20]、2-D [148]、2.5-D [149] 和 3-D [150] 并行。基于 Transformer 的 LLM 的 **MLP** 和自注意力模块中都有两个参数矩阵。 **Megatron-LM** [20] 首先采用一维张量并行，沿列划分第一个参数矩阵，沿行划分第二个参数矩阵。它复制每个分区模块的输入和输出张量，并在所有 GPU 上引入两个 All-Reduce 集体通信，以将 LLM 装入多个 GPU。受可扩展通用矩阵乘法算法 (**SUMMA**, Scalable Universal Matrix Multiplication Algorithm) [241] 和 **Cannon** 的二维并行矩阵乘法算法 [242] 的启发，**Optimus** [148] 进一步在二维上划分输入和参数张量，以提高一维张量并行的通信和内存效率。 **Tesseract** [149] 将为提高 Cannon 算法效率而提出的 2.5 维矩阵乘法方法 [243] 扩展到 LLM 训练，并提出 2.5 维张量并行性以克服模型尺寸增加导致的大量不必要通信。3 维张量并行性 [150] 采用并改进了线性运算的 3 维并行矩阵乘法算法 [244]，实现了 LLM 训练在多个设备之间的完美负载平衡。

#### 4.1.3 流水线并行

流水线并行（图 8(c)）[151]，也称为**层间模型并行(inter-layer model parallelism)**，旨在适应跨多个 GPU 的大型模型，尤其是跨不同节点的大型模型。流水线并行将模型的各层划分为多个阶段，每个阶段由模型中一组连续的层组成，并映射到一组 GPU。与通常需要 NVLink 等高带宽连接进行通信的张量并行不同，流水线并行只需要在指定的切割点交换中间张量，从而减少通信需求。因此，流水线并行适用于在连接小带宽的多个 GPU 节点上扩展 LLM 训练。例如，Strati 等人 [245] 采用流水线并行充分利用地理分布式资源来克服 GPU 短缺的问题。由于不同阶段的数据依赖性，流水线并行通常将输入数据拆分为多个微批次进行流水线处理，以实现对巨型模型的高效训练。然而，它带来了两个重大问题。首先，由于等待前一阶段的输出所花费的时间，流水线泡沫问题降低了 GPU 的利用率。其次，不同阶段之间存在内存消耗不平衡，因为前一阶段需要比后一阶段保存更多活跃的微批次，以实现更好的流水线处理和更高的利用率。我们在下面详细介绍每个问题。

**流水线气泡(Pipeline Bubble)**。提出了有效的微批次调度算法来减少流水线气泡。**GPipe** [151] 引入了一种填充-排出调度，该调度一次注入所有微批次以进行前向传递执行，然后进行后向传递。由于前向和后向传递的预热和冷却，Gpipe 会产生严重的流水线气泡。**PipeDream** [134]、[135] 引入了 **1F1B**（1 前向 1 后向）调度，该调度在相应的前向传递完成后立即执行微批次的后向传递，以减少异步场景中的流水线气泡。**DAPPLE** [152] 采用早期后向调度，首先在每个阶段开始时注入固定数量的微批次，然后以循环方式交错前向和后向传递。**交错式 1F1B** [153] 采用了 1F1B 调度，但为每个 GPU 分配了多个阶段（即循环流水线布局）。流水线气泡的减少是以更高的通信和峰值内存消耗为代价的。**Chimera** [136] 引入了双向流水线来减少具有权重重复的气泡。**Hanayo** [157] 进一步提出了一种波浪式流水线，将多个对称阶段分配给一个 GPU，以提高流水线利用率。**零气泡** [156] 将后向计算分为两部分：激活和参数梯度计算。它使用 1F1B 调度前向和激活梯度计算，然后用参数梯度计算填充气泡，从而减少具有更高峰值内存消耗的气泡。广度优先 [159] 在循环流水线布局中一次运行所有微批次，以减少与分片数据并行结合时的通信开销。 **TeraPipe** [154] 沿序列维度分割微批，并利用更细粒度的令牌并行性来减少管道气泡。但是，由于 TeraPipe 基于 GPipe 调度，因此其内存开销很大。Seq1F1B [158] 将序列分割成块，并利用 1F1B 调度来减少峰值内存消耗，同时实现较低的管道气泡率。**DynaPipe** [160] 使用动态微批处理方法对具有可变长度输入的 LLM 进行多任务训练。
它引入了一种内存感知的自适应调度算法和提前通信规划，以进一步降低管道气泡率。Tessel [155] 是一种两阶段方法，包括重复模式构建和调度完成，用于自动搜索指定分区策略的有效管道调度。
**DISTMM** [161] 启动双倍微批次以绕过多模态训练的大批次要求所导致的依赖性障碍，从而减少空闲周期。**GraphPipe** [162] 保留 DNN 图拓扑并将其划分为可并发执行的阶段，以提高管道利用率并减少内存消耗。

**内存不平衡(Memory Imbalance)**。流水线并行性通常会在开始阶段注入更多微批次以提高流水线利用率，从而导致这些阶段的激活内存消耗更高。为了解决这个问题，**BPipe** [163] 和 **MPress** [164] 采用 D2D（设备到设备）传输，在运行时将中间激活张量从高负载 GPU 交换到轻负载 GPU。MPress 还结合了激活重新计算以减少内存占用。**Chimera** [136] 引入了一种双向流水线，将两个不同方向的流水线组合在一起，以实现更平衡的内存消耗。每个 GPU 都有两个对称阶段，导致权重重复。**Hanayo** [157] 将双向流水线变成两个数据并行流水线，以消除权重重复，并通过将多个阶段对称地分配给一个 GPU 来实现平衡的内存消耗。 **V-Shape** [166] 将模型划分为两倍于设备数量的阶段，其中两个阶段的一半按相反的顺序放置。通过改变阶段之间的偏移量，V-Shape 在峰值内存消耗和气泡利用率之间进行了权衡。**mCAP** [165] 采用增量分析方法，根据峰值内存使用情况在 GPU 上均匀划分模型。

峰值内存消耗限制了流水线并行中的活动微批次数量，从而限制了其效率。可以使用激活重新计算来有效降低峰值内存消耗。**Varuna** [167] 结合了流水线并行和激活重新计算来实现这一目标。它设计了一个基于规则的静态调度，该调度针对给定的流水线进行枚举，并采用机会策略来隐藏抖动并减少气泡。静态调度是基于激活重新计算时间、激活内存管理和后向计算优先级等约束生成的。为了以较低的重新计算开销解决内存不平衡问题，**AdaPipe** [168] 采用自适应重新计算来支持不同阶段的不同重新计算策略，并基于 1F1B 调度进行自适应分区以平衡每个阶段的计算。

#### 4.1.4 序列并行

当今的 LLM 的上下文窗口增长迅速，最强大的 LLM 可以支持数百万个 token [7]。这种超长序列导致 LLM 训练对内存和计算的要求很高：激活的内存占用线性增加，注意力机制的复杂度呈二次方增长。在后向重新计算激活可以减少峰值内存消耗，但也会带来大量开销（完全重新计算时为 30%）。大张量并行度会带来大量通信开销。序列并行性（图 8(d)）[169]、[170] 被提出来适应长序列训练，并在内存容量内有效地将计算分配到多个 GPU 中。它将输入数据沿序列维度分成多个块，每个块被输入到一个 GPU 进行计算。由于序列并行性复制了模型参数，因此它通常与张量和流水线并行性相结合，以扩大 LLM 训练。当与张量并行一起使用时，序列并行将注意力的内存和计算分布在多个 GPU 上，但在 Transformer 层的非张量并行区域会产生冗余的内存消耗和计算。Megatron-SP [170] 沿序列维度拆分这些计算，以减少冗余的激活计算和内存消耗，而不会增加通信。

尽管序列并行将内存、计算和通信划分到多个 GPU，但二次因果注意力在训练效率方面仍然面临显著挑战，包括键值张量通信开销、IO 感知注意力计算开销以及因果注意力掩码导致的 GPU 间负载不平衡。大多数用于注意力的序列并行方法都是基于环的 [169]、[173]、[175]、[176]、[178]、[179]。 Ring Self-Attention [169] 利用序列并行性，通过环形通信计算自注意力，以扩大 LLM 训练的上下文窗口。它首先在 GPU 之间传输关键张量，以循环方式计算注意力分数，然后根据以类似方式传输的注意力分数和值张量计算自注意力输出。DistFlashAttn [176] 同时传输键值张量块，以利用 IO 感知 FlashAttention [115] 内核，并通过用较晚的 token 填充较早 token 的空闲周期来平衡不同 GPU 的计算。Megatron Context Parallel [173] 也利用 FlashAttention 内核，并消除了低三角因果掩蔽导致的不必要计算。它通过与对称 GPU 交换一半块来进一步平衡 GPU 之间的计算。 DistFlashAttn 和 Context Parallel 还将键值张量通信和注意力计算与单独的 CUDA 流重叠。Striped Attention [178] 通过为每个 GPU 分配一个在整个序列中均匀分布的 token 子集（而不是连续的块）来解决不平衡问题。BurstAttention [179] 使用每个 GPU 上的 FlashAttention 计算注意力，并利用双缓冲区重叠通信和计算。Blockwise Ring Attention [175] 将 Ring Self-Attention [169] 扩展为 blockwise 注意力，以小块计算注意力以减少内存占用。受 Nbody 模拟的启发，WallFacer [180] 首先将 GPU 划分为子组，并通过异步 AllGather 在每个子组中复制查询和键值张量。注意力计算利用多个环式 P2P 通信来提高效率。最终需要一个异步 ReduceScatter 来分配注意力输出。

**DeepSpeedUlysses** [172] 与以前的基于环的方法不同，它拆分了头部维度而不是序列维度，并利用 All-to-All 将分区维度从序列转移到头部。DeepSpeedUlysses 可以与现有的注意力实现（例如 FlashAttention）无缝结合，并且 GPU 之间的工作负载自然平衡。然而，DeepSpeed-Ulysses 的并行度受到头部数量的限制，尤其是对于使用 MQA [11] 和 GQA [12] 的 LLM。LoongTrain [174] 和 USP [171] 是并发工作，整合了 DeepSpeedUlysses 和 Ring Attention 的优势。他们将 GPU 组织成二维网格，形成混合的 ulysses 和 ring 式进程组。在训练期间，他们首先在 ulysses 组之间执行 All-to-All 以将分区从序列切换到头部维度，然后在 ring 组之间使用 Ring-Attention 执行注意力计算。 LoongTrain 进一步提出了 Double-Ring-Attention，以充分利用可用带宽进行节点间通信，并将通信与计算重叠。DSP [177] 根据多维 Transformer 中的计算阶段动态切换并行维度，如 DiT [246]。

#### 4.1.5 专家并行

<img src="Fig9.png" alt="Fig9.png" width="500" />

混合专家 (MoE) 是目前 LLM 中最流行的稀疏模型。虽然显著增加了 LLM 中的参数数量，但 MoE 并没有通过条件计算大大增加计算成本 [247]。MoE 的基本框架如图 9 所示，由处理不同训练数据子集的多个专家网络和一个应用路由算法将输入标记分配给不同专家网络的门网络组成。MoE 能够训练具有万亿级以上参数的大型模型，据称已用于流行的 LLM 模型，如 Mixtral 8x7B [248] 和 DeepSeek2 [13]。

**稀疏激活(Sparse Activation)****。随着模型规模的增加，所有专家无法在单个设备上容纳和训练。因此，GShard** [15] 将 MoE 的思想扩展到分布式环境中的 Transformers，其中专家分布在不同的工作者中并通过 All-to-All 通信进行协作，如图 9 所示。后续对专家并行性的研究通常遵循相同的范式。例如，**Switch Transformer** [181] 在 T5 模型上采用了分布式 MoE 训练的设计。但与 GShard 中使用的 top-2 路由算法不同，Switch Transformer 将每个 token 仅路由到 top-1 专家，以最大限度地提高计算效率。此外，**DeepSpeed-MoE** [183] 提出了一种新的分布式 MoE 架构，该架构在每个工作者中应用共享专家，并在更深的层中放置更多专家，以平衡通信成本和训练准确性。

专家并行性可以有效地与传统的 3D 并行性相结合。例如，GShard、Switch Transformer 和 DeepSpeed-MoE 都将专家并行性视为混合并行性的正交维度。为了实现高效的混合训练，DeepSpeed-TED [249] 提出了一种混合并行算法，该算法结合了数据、张量和专家并行性，以实现 MoE 模型的训练。作者将 MoE 参数划分为预定义大小的“图块”，以避免优化器内存峰值过高，并提出了通信优化，如重复令牌丢弃 (DTD) 和激活检查点，以消除 All-to-All 通信中的重复数据。然而，由于 MoE 的动态特性，选择最佳混合并行计划具有挑战性，并且在运行时在不同并行策略之间切换也会产生大量开销。因此，一些研究（如 Tutel [182]）设计了一种自适应并行切换算法，该算法对所有可能的最佳策略应用相同的分布模型布局，并且可以在每次迭代时动态切换并行策略而无需任何额外开销。

由于通用矩阵乘法 (GeMMs,  General Matrix Multiplications) 要求所有专家输入的大小保持一致，现有的 MoE 训练框架通常会执行标记删除和填充以匹配相同的专家容量，这会浪费计算资源。**Megablocks** [185] 通过实现块稀疏矩阵乘法来优化分组 GeMM，并在单个内核中支持不同批量大小的专家计算，以避免 MoE 训练中不必要的标记删除。另一个支持分组 GeMM 的框架是 **ScatterMoE** [184]，它实现了 ParallelLinear 内核，该内核融合了分组 GeMM 和分散的读写操作，以减少 top-k（k ≥ 2）门控的内存占用。

**通信优化(Communication Optimization)**。专家并行中的全对全通信会严重影响 MoE 的训练效率，尤其是在网络环境较差的情况下。现有的分布式训练系统试图通过将通信任务与计算任务重叠来优化 MoE 的性能，从而隐藏一些通信成本。例如，**Tutel** [182] 沿专家容量维度将输入张量分成几组，并重叠不同组之间的计算和通信以隐藏全对全开销。FasterMoE [191]、[192] 使用与 Tutel 类似的策略，但沿专家维度分割张量。此外，Tutel [182] 还通过将小消息聚合到节点内的单个大块中，然后在不同节点之间交换数据来优化全对全内核实现。FasterMoE 和 ScheMoe [187] 也使用了这种优化。 **PipeMoE** [186] 基于 Tutel 中的重叠策略，根据工作负载对通信和计算任务的执行时间进行建模，并设计一种自适应算法来找到最佳分区数以最小化训练时间。ScheMoE [187] 在 All-to-All 通信之前考虑数据压缩方法，并将耗时操作模块化，包括数据压缩、集体通信和专家计算。然后，ScheMoE 提出了一种自适应最优调度算法来流水线通信和计算操作，以提高训练效率。

专家并行性通常与 MoE 训练中的其他并行策略相互作用。通过细粒度的任务调度可以减少通信开销。例如，Lina [188] 系统地分析了 MoE 在分布式训练和推理过程中的 All-to-All 开销，并发现当 All-to-All 与 AllReduce 操作重叠时，延迟会延长。Lina 建议优先考虑 Allto-All 而不是 AllReduce，以提高其带宽并减少其在分布式训练中的阻塞时间。此外，Lina 结合张量分区和流水线来执行类似于 Tutel 的微操作调度。Lina 还在推理过程中根据专家受欢迎程度动态调度资源以最大限度地减少开销。Janus [189] 设计了一个以数据为中心的范式，该范式将数据保持在原位并基于参数服务器在 GPU 之间移动专家。以数据为中心的范式使用细粒度的异步通信，并允许专家使用非阻塞通信原语（如拉动）在 GPU 之间移动。 Janus 实现了一种拓扑感知策略，可以有效地在节点之间拉取专家，并支持专家预取，将所有外部专家拉到本地 CPU 内存中。

有一些研究从模型系统协同设计的角度优化了 MoE 训练。例如，**TA-MoE** [190] 提出了一种用于大规模 MoE 训练的拓扑感知路由策略。TA-MoE 将调度问题抽象为优化目标，以获得不同拓扑下的目标调度模式，并根据调度模式设计拓扑感知辅助损失。这种方法自适应地路由数据以适应底层拓扑，而不会牺牲模型准确性

**负载平衡(Load Balance)**。由于 MoE 的稀疏和条件计算特性，在专家并行性中，一个受欢迎的专家可能会比其他专家获得更多的令牌（通常是由于路由算法不佳造成的），从而导致严重的负载不平衡，影响 MoE 的训练效率。FasterMoE [192] 提出了影子专家方法，该方法根据前几次迭代的工作负载，将受欢迎专家的参数动态广播到所有其他 GPU。通过将受欢迎专家的工作负载分散到不同的设备，影子专家方法减少了专家受欢迎程度不均的影响。SmartMoE [193] 采用两阶段方法来搜索负载平衡的最佳并行计划。首先，SmartMoE 设计了一个数据敏感的性能模型，将并行计划划分为池，其中在池内切换并行模式的成本相对较低。然后，SmartMoE 可以切换到适当的并行性（在 SmartMoE 中称为专家放置）以在在线阶段保持负载平衡。 FlexMoE [194] 发现专家到设备映射的分布在短时间内不会发生显著变化，因此它引入了细粒度的复制专家并行性，可在多个设备上复制重度专家。FlexMoE 监控数据工作负载，并使用三个放置调整原语（即扩展、收缩、迁移）在超过平衡比率时生成最佳放置解决方案。Prophet [195] 提出了一种系统、细粒度、高效的大规模 MoE 模型负载平衡训练方法。以 MoE 模型、设备池和令牌分布作为输入，Prophet 的规划器迭代搜索和评估专家放置，最终输出均衡的专家放置。此外，Prophet 使用分层调度策略隐藏了这些资源分配操作的开销。

### 4.2 自动并行

给定一个任意的 DNN 模型和一个 GPU 集群，存在大量的并行选项，包括各个层的分区及其分区程度。设计可以最大化训练效率的手工混合并行方法是一个耗时且知识密集的过程，需要专家了解模型架构、硬件特性以及并行化策略中涉及的复杂权衡。此外，高效实施最佳并行化策略通常需要大量的人力。为了应对这些挑战，自动并行化作为一种有前途的解决方案应运而生，它旨在自动确定特定 GPU 集群上给定 DNN 模型的最有效并行化策略。通过利用复杂的算法和启发式方法，自动并行化系统可以分析模型架构、硬件规格和性能特征，以确定并行技术的最佳组合，例如数据、张量和流水线并行化。这种方法简化了跨各种模型和基础设施优化分布式训练的过程，提高了整体效率并减少了人工工作量。此外，自动并行可以适应不断变化的硬件配置和模型架构，自动调整并行化策略以保持最佳性能。接下来，我们根据目标模型架构将现有的自动并行系统分为通用(general)和特定于 Transformer 的(transformer-specific)框架。

#### 4.2.1 General Framework

**通用自动并行框架**专注于在特定的计算集群上自动并行化通用深度神经网络(DNN)。这些框架通常遵循一个三步流程：(1)定义并行化策略的搜索空间；(2)开发性能模型以衡量不同策略的训练效率；(3)设计算法以高效地确定最优的并行化策略。下面作者将根据它们覆盖的搜索空间来研究不同的方法。

一些研究探讨了混合数据并行和 Pipeline 并行策略的搜索空间，以优化DNN训练。这些方法专注于自动分割DNN，并设计 Pipeline 调度以提高 Pipeline 利用率。**PipeDream** [134] 通过最慢阶段的执行时间来衡量 Pipeline 分区的效率，并开发了一种动态规划算法(dynamic programming algorithm)，通过最小化最慢阶段来均匀分割DNN。**DAPPLE** [152] 建立了一个分析模型来估计一种分区策略的执行时间，并使用动态规划来确定最优的 Pipeline 分区。**AutoPipe** [205] 构建了一个模拟器来模拟 Pipeline 执行，并提出了一种启发式算法以获得平衡的分区。AutoPipe 还自动分割微批次以减少 Warm up 阶段的延迟。一些设备放置方法 [206, 207, 208] 使用强化学习来预测 Pipeline 并行性下的最优操作符放置。

研究行人还探索了通过在不同维度上划分算子(operators)来自动实现数据和模型并行。**OptCNN** [198] 在其输出张量的所有可分维度(divisible dimensions)上划分算子，并利用一种分析性能模型来选择最佳的并行策略，包括可并行维度(parallelizable dimensions)和并行度(degree of parallelism)，这定义了如何跨不同设备对一个单独的层进行并行化。FlexFlow [199] 进一步将搜索空间扩展到**样本-算子-属性-参数**(SOAP, Sample-Operator-Attribute-Parameter)，这几乎包括了输入和输出张量的所有可分维度，并引入了一种新颖的执行模拟器进行精确的性能建模。FlexFlow 使用马尔可夫链蒙特卡洛(MCMC)采样高效地找到最优的并行策略。Tofu [200] 和 HyPar [201] 开发了动态规划算法，这些算法最小化的是总通信成本而非端到端性能，以在混合数据和模型并行空间中为每个算子确定最优分区。TensorOpt [204] 使用前沿跟踪算法在给定内存预算下优化并行策略。AutoMap [202] 采用蒙特卡洛树搜索(MCTS)来选择一系列由 PartIR [212] 定义的划分规则，这些规则用于通过学习打分器选择一组重要算子。整个并行策略是通过所选算子传播的策略。

最近的工作还设计了针对自动数据、模型和流水线并行的方法。**Piper** [209] 设计了一个两级的动态规划方法，以找到最佳的混合数据、张量和流水线并行策略，并结合激活重计算。它首先将模型划分为小分区以便流水线操作，然后在每个分区内拆分算子。**Alpa** [21] 将并行性视为两个层次结构：算子间并行和算子内并行，从而形成一个综合空间。然后它自动在各个并行 Level 推导出有效的并行执行计划。**Unity** [210] 通过将它们表示为统一并行计算图上的替换，共同优化并行化和代数变换。**Aceso** [211] 提出了一种迭代的瓶颈缓解方法，以显著减少搜索时间。它每步识别一个性能瓶颈，并调整策略以减轻瓶颈直至收敛。**nnScaler** [213] 引入了三种原语，以支持具有任意划分和划分模型的空间-时间调度的搜索空间的组合。领域专家可以应用约束到这些原语以构建有效且小的搜索空间，这些空间可以以低开销自动探索。**AutoDDL** [215] 定制了一个坐标下降算法，通过迭代更新每层的SBP [214] 分布，快速发现近最优通信成本的最优策略。

通用自动并行框架需要高效的系统支持各种并行化策略，除了快速优化算法以发现最优并行化策略之外。这是因为并行性通常涉及复杂的计算和通信操作符，特别是对于分割操作符的模型并行性。先前的研究通过在现代深度学习框架[21, 213]的基础上构建，或者从头开始实现[199]，开发了能够支持广泛并行化策略的高效系统。Mesh-TensorFlow [196]观察到实现并行化策略的内在复杂性，并首先提出将设备集群抽象为多维网格，将并行性抽象为划分迭代空间(即张量维度)。通过映射张量和网格维度，可以轻松实现具有高性能的混合数据与模型并行策略。例如，数据并行和模型并行分别分割批处理和隐藏维度。GSPMD [197] 进一步提供了一种统一的方法，通过基于JAX [250] 和 XLA [251] 的简单张量分片标注来实现各种通用并行方案。OneFlow [214] 提出了SBP(分割、广播、部分值)抽象概念以实现分割，并允许用户为张量指定放置和SBP签名以实现不同的并行化策略。PartIR [212] 将模型与其分割解耦，并设计了一个编译器堆栈，让用户通过调度逐步组合SPMD分片策略。类似于TVM [252]，Slapo [203] 定义了一组全面的调度原语，用于并行化和子图优化，如算子融合和激活检查点。这些调度与执行解耦，并保留了原始模型结构以进行逐步优化。

#### 4.2.2 Transformer-Specific Framework

由于大型语言模型(LLMs)基于Transformer架构，近期的研究工作针对Transformer定制了自动化系统。**DeepSpeed-Autotuning** [216] 能够自动调整系统旋钮，在用户定义的调优空间中找出与性能相关的良好配置，包括并行度。**Galvatron** [217] 设计了一个动态规划算法，生成最有效的混合数据、张量和流水线并行策略。**Merak** [218] 引入了一个自动模型分割器，用于非侵入式自动并行，以及一个高性能的3D并行运行时引擎，以提高可用资源的利用率。Colossal-AI [219, 253] 为混合数据、张量、序列和流水线并行的模块化使用提供了一个统一接口。**Galvatron-BMW** [220] 扩展了Galvatron的研究空间，包括分片数据并行和激活重计算，同时在最大化硬件利用率的前提下，考虑内存消耗和计算，寻找最优策略。

### 4.3 异构并行

不断增加的LLM训练计算需求推动了异构硬件的发展，这种硬件利用了各种计算资源和全球分布的设备。这种异质性也体现在模型架构中，特别是在利用人类反馈的强化学习(RLHF)方面。采用异构硬件和多样化的模型架构对于高效训练LLM变得至关重要。

#### 4.3.1 异构硬件

大规模的LLM训练计算需求推动了加速器的发展，导致了拥有不同设备类型和不等互联带宽的混合集群。此外，由于电力短缺等因素，现代数据和计算集群通常分布在全球各地。这些现象促使采用异构并行，利用多样化的计算资源和地理分布的设备来加速LLM训练。

一些研究利用异构计算资源，如CPU、GPU和专业加速器，以提高LLM的性能。这些设备在计算、内存容量和互联带宽方面的差异为高效LLM预训练带来了挑战。HetPipe [221] 将异构集群划分为多个虚拟工作单元。每个虚拟工作单元通过流水线并行处理小批量数据，不同的虚拟工作单元采用异步数据并行以提高吞吐量。AccPar [222] 提出灵活的张量划分以平衡不同加速器的计算，并使用动态规划自动为DNN在不同异构设备间决定张量划分。Whale [223] 提出了一个统一的抽象概念，以简化在异构集群上对大型模型并行训练的努力。它通过自动图优化无缝适应异构GPU，并利用硬件信息平衡工作负载。AMP [224] 利用一个对异构环境敏感的性能模型，寻找最佳的混合数据、张量和流水线并行策略。HPH [226] 根据计算通信比降序排列不同GPU为各个阶段，并将模型划分作为一个整数规划问题来最小化迭代时间。Pathways [225] 采用分片数据流模型和异步组调度，在异构集群上高效执行机器学习模型。SDPIPE [227] 引入了一种半去中心化方案，该方案将通信模型同步去中心化，将流水线并行的组调度过程集中化，以利用异构设备。HAP [228] 使用基于A*的搜索算法生成最优张量分片策略，在异构设备间分片比例以及分布式训练的通信方法。PipePar [229] 提出一个动态规划算法，考虑GPU和网络带宽的异质性，将模型划分为流水线阶段。

其他一些研究探索了具有低网络带宽特征的地理分布式设备，以提高训练效率。Yuan等人 [230] 将LLM划分为计算任务单元，并提出一种新的调度算法，以高效利用通过慢速异构网络连接的一组异构设备进行混合数据和流水线并行。SWARM并行 [231] 将模型划分为等大小的阶段，并优先将输入路由到延迟较低的稳定对等体以实现工作负载平衡。它还自适应地在阶段间移动设备以最大化训练吞吐量。FusionAI [232] 将训练计算图(DAG)拆分为子图(子-DAG)，并生成一个负载平衡的任务调度，以利用通过低带宽连接的异构消费者GPU进行流水线训练。像CocktailSGD [254] 这样的通信压缩方法也可以在低带宽集群中高效地训练LLM。

#### 4.3.2  异构模型

在LLM训练过程中，异质性(heterogeneity)不仅体现在硬件上，也体现在模型上。训练可能涉及几种不同模型的交互。一个具体的例子是**来自人类反馈的强化学习**(RLHF, Reinforcement Learning from Human Feedback)。RLHF是一种旨在使AI系统与人类偏好更紧密对齐的训练方法[255]，利用人类在判断适当行为方面的优势，而不是演示。这种方法受到了广泛关注，特别是用于微调大型语言模型。然而，由于**近端策略优化(PPO, Proximal Policy Optimization)**[256]算法的特殊性，模型异质性被引入RLHF训练中，使得RLHF的训练过程与预训练和有监督微调大不相同。

原则上，RLHF由三个不同的阶段组成：第一阶段是有监督的微调，第二阶段是奖励模型的训练，第三阶段是PPO训练。

模型异质性体现在第三阶段，如图10所示。**PPO训练阶段**包括两个不同的过程，即生成数据的过程和更新演员模型和评论家模型权重的训练过程。PPO训练是通过这两个过程的协作完成的。此外，训练阶段引入了更高的内存成本，因为作者需要同时服务几个自动回归生成模型和奖励模型的副本，并且需要更多的时间成本，因为作者必须等待经验生成完成后才能更新权重。

<img src="Fig10.png" alt="Fig10.png" width="700" />

图10：RLHF的一个示例。**推理过程**：**1** 演员模型根据给定 Query 生成回应。**2** 评论家模型、奖励模型和参考模型通过推理，使用 Query 和回应对生成训练所需的值、得分和KL散度。**训练过程**：**3** 演员模型和评论家模型使用推理过程中收集的数据通过梯度下降更新它们的权重。

已经提出了许多框架用于RLHF训练。例如，DeepSpeed-Chat[233]使用混合引擎，在训练和推理之间无缝切换模型分割，如推理时使用张量并行以提高吞吐量，训练时使用ZeRO[145]或LoRA[257]以提高内存利用率，为RLHF训练提供了卓越的系统效率。HuggingFace TRL[234]可以充分利用各种参数高效微调(PEFT, parameter-efficient fine-tuning)方法，如LoRA或QLoRA[258]，以节省内存成本，并使用由unsloth[259]设计的专用 Kernel 来提高RLHF的训练速度。Colossa端到端RLHF训练框架也支持LoRA，并支持使用ZeRO[145]减少内存冗余。

然而，上述工作采用了模型放置的平坦化策略，即将RLHF中的四个模型放置在相同的设备上，然后使用ZeRO或LoRA等方法最小化内存成本。但仅使用ZeRO在训练更大模型时会导致内存瓶颈，而使用如LoRA这样的高效参数微调策略会损害模型性能。为了解决这个问题，OpenRLHF[235]使用Ray[260]和vLLM[261]将奖励模型分布到不同的设备上，避免将PPO中的所有四个模型放置在同一个设备上。同样，自适应放置与并行(APP)框架[236]提出了另外两种模型放置策略，即**交错策略**(Interleaving Strategy)和**分离策略**(Separation Strategy)。它捕捉到了在PPO训练期间生成部分和训练部分可以独立运行的事实，通过将它们放置在不同的设备上可以消除一些序列化，这引入了额外的通信，但可以很好地与计算重叠。

同时，有些工作在细粒度调度方式下，将前两个阶段的并行策略应用于RLHF的第三阶段。例如，ReaLHF[237]通过重新分配参数，为第三阶段的不同子阶段切换最适合的并行模式，这大大增加了优化空间。PUZZLE[238]根据不同阶段的亲和性重新安排任务执行顺序，使得亲和性更好的阶段可以有效覆盖执行，提高训练效率。

## 5 计算优化

当今的AI加速器在浮点运算能力(FLOPs)方面提供了前所未有的计算能力。然而，为了充分利用这些FLOPs，需要采用复杂的优化技术。本节介绍了计算优化系统和技巧，以有效利用GPU的FLOPs。作者首先详述**操作优化**(operator optimizations)，包括核心注意力操作符优化和通过编译器自动优化。基于利用底层硬件特性的大规模并行性和高效的多层次内存访问，操作符和计算图获得了卓越的性能。其次，作者详细介绍了**混合精度训练**，计算加速得益于降低的精度。16位浮点混合训练已成为大多数训练系统中的事实标准方法。低比特固定点，甚至低至1位，已经被研究并用于提高训练效率。

图11：针对分布式LLM训练的计算优化研究。

<img src="Fig11.png" alt="Fig11.png" width="700" />

### 5.1 操作优化

操作优化可以分为手动优化和自动优化。手动优化主要关注注意力操作符，而自动优化则被广泛应用于更广泛的领域。

#### 5.1.1 Manually Optimized Attention Operator

注意力作为 Transformer 架构的核心，在LLM(大型语言模型)的训练效率中扮演着关键角色。给定一个 Query $q$，以及键$k_1, k_2, \ldots, k_n$和值$v_1, v_2, \ldots, v_n $的列表，其中$q, k_i, v_i \in \mathbb{R}^d$，注意力的计算方法如下：

$$S_i = \text{dot}(q, k_i),S'_i = \text{softmax}(s_i)= \frac{e^{S_i}}{\sum_{j}e^{S_j}},O_i = {\sum_{i} v_i s'_i}$$

自注意力相对于序列长度表现出二次时间复杂度和内存复杂度。由自注意力引起的巨大内存消耗以及对高带宽内存(HBM)的频繁访问限制了 Transformer 模型的性能和上下文长度。已经提出了大量工作来优化自注意力。作者关注精确的注意力优化，而像线性注意力这样的有损优化不在作者的研究范围内。

内存高效注意力主要被提出以减轻大内存成本。Rabe等人[287]证明了自注意力需要的内存复杂度是$O(logn)$而不是$O(n^2)$。通过采用懒惰的softmax，可以在注意力操作的末尾延迟除以的操作。因此，求和可以递增地处理，这只需要一个标量(即$O(1)$)来保持中间结果，但不会改变输出。自注意力需要额外的内存复杂度来保持对 Query 列表的额外索引，以顺序计算所有 Query 的结果。

FlashAttention系列进一步展示了具有IO感知、高并行性和在GPU上平衡工作负载的快速和内存高效的精确注意力。在FlashAttention [115]中，提出了一种IO感知的瓷砖算法，该算法通过在线softmax减少在慢速HBM和快速片上SRAM之间的内存读写次数。更具体地说，通过跟踪规范化统计数据(包括最大分数和指数化分数的总和)可以一次计算一个块的softmax。因此，瓷砖算法将自注意力中的所有计算操作链(包括矩阵乘法、softmax、矩阵乘法等)融合到一个cuda Kernel 中，以减少对HBM的访问。FlashAttention-2 [116]通过在序列长度维度上增加额外的并行性以及改进线程块内的数据共享调度，进一步改善了FlashAttention中的低占用率和不必要的共享内存读写。此外，流行的训练系统[174]通常采用FlashAttention-2以达到高性能。FlashAttention-3 [262]通过挖掘新推出的硬件功能，加速了H100 GPU上的注意力，因为之前的FlashAttention实现基于A100 GPU。基于FlashAttention-2，重新设计了交错块GEMM和softmax算法，通过异步WGMMA指令隐藏softmax中的非GEMM操作。此外，通过利用张量核心和张量内存加速器(TMA)的异步性，通过一种面向线程块的软件流水线方案，将计算与数据移动进行整体重叠。Blockwise Parallel Transformer (BPT) [263]通过将FlashAttention中的瓷砖算法扩展到融合前馈网络，进一步减少了大量的内存需求。

注意力操作还通过利用硬件特定特性在各种架构上进行优化。例如，SWattention [264]设计了一种双级阻塞注意力算法，以利用新型Sunway架构的底层硬件，建立在FlashAttention之上。同样，Bikshand等人[265]使用Cutlass库在H100 GPU上实现了FlashAttention-2。他们利用TMA和WarpGroup Matrix-Multiply-Accumulate (WG-MMA)指令分别优化数据复制和GEMM操作。此外，基于Cutlass库，精心设计了张量布局转换和数据复制以及两次GEMM之间计算的软件流水线。

针对可变长度序列对注意力机制进行了优化，这在分布式LLM训练中很常见。如果将这些可变长度序列填充到最大长度，可能会产生显著的内存和计算成本。FlashAttention-2通过不可分割地并行化序列长度维度，有效地处理可变长度输入。ByteTransformer [266]关注于针对可变长度输入的无填充 Transformer ，在计算过程中维护一个位置数组。该数组记录了原始张量和中间打包张量之间有效标记的映射关系。针对长序列的融合多头注意力算法对未填充张量使用优化的分组GEMM。这种优化减少了与填充相关的内存和计算开销，从而提高了性能。

#### 5.1.2 Automatic Optimizations via Compilers

深度神经网络编译器在优化大型模型(LLM)训练中的关键计算中扮演着重要角色。它自动生成了操作符的高效 Kernel ，极大地减轻了基于库的 Kernel 优化在不同硬件供应商之间的负担。通过自动分析训练过程中的计算图来执行操作符融合。

**高效操作核生成(Efficient Operator Kernel Generation)**。Halide [267] 和 TVM [252] 自动生成高性能的操作实现，依赖于多个有效的调度原语，这些原语在各种后端上利用并行性和数据局部性。此外，Roller [268] 优化了在 Kernel 实现的大型搜索空间中寻找最优替代方案的成本。它主要生成包含加载、存储和计算接口的瓦片核，然后通过先扩大后缩放的方法构建完整的操作核。Triton [269] 提供了一种基于 C 的语言和编译器，便于表达和优化瓦片张量程序，以获得有竞争力的性能。特别是，通过机器相关的编译通道支持分层瓦片和共享内存分配等有效优化。ALCOP [270] 对 GPU 上的操作执行自动加载-计算流水线，以重叠高延迟内存访问与计算。

**操作融合的图级优化(Graph-level Optimizations for Operator Fusion)**。 随着计算核心速度与内存带宽之间的差距扩大，现代深度神经网络受到内存访问的限制。通过编译器进行操作融合，挖掘操作符之间的数据重用。许多编译器工作 [288, 289, 290, 291] 通过设置专家规则执行操作融合。特别是，Chimera [271] 优化了计算密集型操作链。首先将操作链分解为一系列计算块，然后根据分析模型选择最优的块执行顺序，以最大化数据重用。此外，设计了可替换的微 Kernel ，以利用特定硬件的块内优化。Welder [272] 将计算图降低到瓦片 Level 的数据流图，其节点是操作瓦片，边标记由连接节点重用的张量数据的内存 Level 。在瓦片 Level 搜索跨不同内存层次结构最大化数据重用的操作融合组合。

Pytorch2 [273] 提出了两种扩展，即 Python  Level 的 JIT 编译器 TorchDynamo 和相应的编译器后端 TorchInductor，以实现在不牺牲 Python 灵活性的情况下，在各种后端上进行更健壮的图编译，从而显著提高性能。Slapo [203] 提出了一种调度语言，将模型执行与定义分离。通过声明一组调度原语，用户可以转换模型以生成高性能 Kernel 。JIT-Q [274] 提出了权重的即时量化，它允许在训练期间只存储权重的高精度副本，并根据商用 PIM(内存内处理)解决方案的内存 ALU 增强创建低精度权重副本。

### 5.2 混合精度训练

低精度训练是一种有效的方法，可以减少在训练大规模模型时的计算、存储和通信成本。目前，大型语言模型(LLM)的训练通常采用FP16和BF16数据类型。特别是，BF16可以表示与FP32相同的数值范围。由于在FP16中当损失标量变得过低时，损失会缓慢发散，因此BF16训练被用于诸如BLOOM [292]等模型中。然而，快速支持bfloat16仅在TPU上可用，或在NVIDIA安培系列及之后开发的GPU上可用。此外，由于减少的精度表示的动态范围有限，混合精度训练以及诸如损失缩放技术等被利用来确保数值稳定性。8位甚至更低位的训练也正在成为量化研究关注的焦点。

#### 5.2.1 16-Bit Floating Point

流行的训练系统通常采用FP16/BF16混合精度策略来在训练过程中降低精度，正如Megatron-LM [20] 和 Colossal-AI [253] 等工作所强调的。FP16混合精度训练方案 [275] 使用IEEE半精度格式来存储权重、激活值和梯度，以用于前向和反向算术操作。为了在降低精度的同时保持模型准确度，在每个优化器步骤中保持权重的一个单精度副本用于累加。同时，还应用损失缩放来保持小梯度值的大小。Campo [276] 通过自动图重写优化了FP32与FP16之间转换所产生的造型成本。这是关键的一步，因为造型成本有时会抵消低精度所带来的性能优势。Campo还采用离线训练的线性回归模型来预测FP32和FP16操作的造型成本和执行时间。BF16 [277] 也广泛应用于各个领域的混合精度训练中 [294, 295]。它具有与FP32相同的表示范围，并且不需要调整超参数以实现收敛。此外，THC [278] 通过消除解压缩和压缩的需求，解决了参数服务器架构中的计算开销问题。THC通过统一的同态压缩属性直接聚合压缩梯度值，从而提高了效率。

#### 5.2.2 Sub-8-Bit Floating Point

随着新型芯片的发布，这些芯片支持如FP8之类的低精度数据类型，混合精度训练旨在使用较低精度进行训练。结合确保数值稳定性的技术，主要利用新设计的数据格式来支持深度学习神经网络的FP8训练。王等人[279]使用新的FP8浮点格式对数据进行数值表示及计算。在浮点累加和权重更新过程中，分别采用分块计算和随机舍入来保持模型精度。孙等人[280]提出了一种混合8位浮点训练方法，该方法在整个深度学习模型范围内不会降低精度。新颖的混合FP8格式针对前向传播和反向传播分别使用不同的指数位和尾数位，因为前向和反向传播对范围和精度的最优平衡是不同的。此外，还使用了诸如损失缩放等技术来避免精度下降。随着更多支持FP8数据类型的加速器的成熟，提出了一种基于NVIDIA H100 GPU [296]的FP8自动混合精度框架(FPR-LM) [281]用于训练大型语言模型(LLMs)，其中逐步融入了8位梯度、优化器状态和分布式并行训练，并指定了包括张量、流水线及序列并行在内的FP8低比特并行性。此外，还设计了精度解耦和自动缩放，以解决由于动态范围较窄和精度降低导致的数据下溢或上溢问题。FlashAttention-3还利用了块GEMM量化和非连贯处理，利用H100 GPU上对FP8低精度的硬件支持。此外，Rouhani等人[282]通过利用与张量细粒度子块相关联的微缩数据格式，以最小的精度损失训练了权重、激活和梯度均低于8位的LLMs。

#### 5.2.3 Low-Bit Fixed Point

**低比特固定点**训练也已被研究用于大型语言模型(LLM)的训练。Jetfire [283] 保持了一个INT8数据流，输入和输出以INT8数据格式进行加载和存储，以加速计算受限的线性运算符和内存受限的非线性运算符。此外，还采用了分块算法来挖掘每个块量化方法中的共享内存数据访问，在此方法中，线性运算符使用更高精度的计算(即INT32用于WMMA张量核心操作)，非线形运算使用FP32以保持预训练 Transformer 的准确性。Xi等人 [284] 提出了一种用于转换模型的创新INT4训练算法。在前向传播中，首先将激活矩阵转换为块对角哈达玛矩阵，以减轻激活中异常值引起的精度下降，然后对转换后的矩阵进行量化。在后向传播中，利用位分裂和杠杆得分采样来基于激活梯度的结构稀疏性选择信息丰富的梯度进行量化。

最近，针对LLM的低精度训练已经发展到了使用1位精度。BitNet [285] 在 Transformer 块内部采用了一种新颖的低比特精度矩阵乘法，权重为1位，激活为8位。模型权重围绕零中心化，以在有限的数值范围内最大化容量，然后使用符号函数将其二值化为+1或-1。为保证训练的稳定性和准确性，梯度、优化器状态以及一个高精度的潜在权重副本被保持用于参数更新。在BitNet的基础上，BitNet b1.58 [286] 进一步通过将模型权重降低到三元值{-1, 0, 1}来提升建模能力。权重矩阵通过其平均绝对值进行缩放，并将每个值四舍五入到-1、0和+1中的最近整数。

## 6 内存优化

在LLM训练过程中的内存消耗可以归纳为四个关键组成部分：模型状态(model states)、激活(activations)、临时缓冲区(temporary buffers)和内存碎片(memory fragmentation)。

- **模型状态：**模型状态包括优化器状态(optimizer states)、梯度(gradients)以及模型参数(model parameters)所占用的内存。在混合精度训练[275]中，模型参数和激活存储为16位精度。训练一个具有个参数的模型时，需要字节来存储参数和梯度。参数、动量和方差的32位副本各自需要字节，总计字节。因此，存储模型状态的总内存需求为16字节。
- **激活：**激活是指在正向传递中生成的张量。这些张量对于反向阶段计算梯度至关重要。
- **临时缓冲区：**临时缓冲区用于存储中间结果。例如，梯度AllReduce等操作常常会将桶中的梯度融合到一个单一的扁平缓冲区中，然后执行操作以提高吞吐量。
- **内存碎片：**内存碎片可能导致内存请求失败，尽管有大量可用内存。这是因为可用内存可能变得碎片化，且没有足够的连续内存来满足内存请求[145]。

为了解决LLM训练的内存限制问题，已经提出了各种内存高效技术。这些技术包括**激活重计算**(activation recomputation)策略，它通过增加计算来减少内存使用；**减少冗余**(redundancy reduction)方法，最小化训练过程中的数据复制；**去碎片化**(defragmentation)技术，优化内存分配和释放，减少碎片并提高内存利用率；以及**交换和卸载**(swap and offload)方法，利用CPU内存和NVMe SSD来补充GPU内存。图12概述了这些针对内存高效LLM训练的优化分类。

### 6.1 激活重计算

在模型训练的反向传播阶段，激活值对于计算梯度至关重要。随着模型尺寸的增加，训练过程中存储这些激活值所需的内存可能会超出GPU内存的容量，从而限制了可训练模型的规模。激活值重新计算方法[325]通过在前向传播中策略性地丢弃某些激活值，并在反向传播中按需重新计算，提供了一种解决方案。这种方法已成为减少大规模语言模型(LLM)训练中内存消耗的默认方法。有效激活值重新计算的关键是平衡内存节省与额外的计算开销。

图12：针对分布式LLM训练的内存优化研究。

<img src="Fig12.png" alt="Fig12.png" width="700" />

作者将这些方法分为两种主要途径：**静态驱逐**(static evicting)和**动态驱逐**(dynamic evicting)。静态驱逐方法通常涉及为特定模型架构或模块定制驱逐策略。相比之下，动态驱逐方法则在实时决策，无需事先了解模型。尽管静态方法对新模型需要进行修改，但大多数LLM共享相似的架构，这使得在LLM训练期间可以普遍应用这些策略。尽管动态驱逐方法具有固有的灵活性，但在LLM的训练中并未被广泛采用。尽管如此，作者仍在本节中探讨了一些相关研究，以供进一步参考。

#### 6.1.1 静态驱逐

静态驱逐涉及在正向传递过程中建立固定的计划来丢弃激活，并在反向传递过程中重新计算它们。Checkmate [300] 将这种激活重新计算问题公式化为混合整数线性规划问题，以确定静态深度学习模型的最佳重新物质化计划。然而，由于巨大的搜索空间，Checkmate 在扩展到大型模型(如 LLMs)时遇到困难。

近期，有几项工作提出了专门针对LLM训练定制的激活重新计算策略。选择性检查点 [170] 选择性地丢弃了内存密集型注意力模块的激活。FlashAttention [115] 将注意力模块融合为单个 Kernel ，并采用选择性检查点来减少内存消耗。DistFlashAttn [176] 针对由注意力模块重新计算引起的长序列中的高计算开销，采用了一种意识到的重新物质化梯度检查点策略。具体来说，DistFlashAttn 在 FlashAttention  Kernel 输出而不是 Transformer 层边界处放置检查点，从而在反向传递中移除了注意力模块的重新计算，只要求存储其输出。LoongTrain [174] 引入了选择性检查点++，它通过将注意力模块添加到 *白名单* 来进一步优化检查点过程，特别是对于长序列的训练。这种方法保存了注意力输出和 softmax 统计数据(softmax_lse)。在正向传递中，它保存白名单中模块的输出，在反向传递中，它检索这些存储的输出而不是重新计算它们，继续计算图，从而减少了重新计算注意力的需要。

与近期主要关注于针对LLM训练的手工制作注意力模块检查点策略的工作不同，Yuan 等 [301] 仔细测量了在模型训练期间重构每个激活张量所需的最小计算成本。他们通过枚举所有可能的检查点方法，导出了内存和计算成本的帕累托前沿。从这个帕累托前沿，他们选择了一个在计算和内存成本之间最优平衡的解决方案。

#### 6.1.2 动态驱逐

动态驱逐根据训练过程的当前状态实时决定哪些激活需要丢弃和重新计算。DTR [297] 提出了一种贪心的在线算法，以启发式的方式在运行时为静态和动态模型驱逐和重新生成张量。MegTaiChi [298] 引入了一种动态张量驱逐方法，它利用了在运行时跟踪的张量的访问模式。Coop [299] 提出了一种方法，以减轻由于不考虑张量连续性而驱逐张量导致的激活重新计算方法引起的内存碎片问题。Coop 采用了一种高效的滑动窗口算法，确保只驱逐连续的内存块，从而最小化内存碎片。

### 6.2 减少冗余

传统数据并行方法需要在所有GPU上复制整个模型状态，这导致了大量的内存冗余使用。为了优化内存使用，提出了减少冗余技术，通过消除或减少每个设备上的内存冗余。这些技术通常旨在平衡内存效率与引入的通信开销，从而以可接受的成本促进更大规模或批量大小的训练。

#### 6.2.1 Fully Sharding

**零冗余优化器**(ZeRO, Zero Redundancy Optimizer) [145]通过三个阶段：ZeRO-1、ZeRO-2和ZeRO-3，在所有GPU上完全分片模型状态来优化内存冗余。ZeRO-1将优化器状态全局分布在所有GPU上。在训练期间，每个GPU独立进行前向和反向传播以计算梯度，随后使用ReduceScatter操作在数据并行组内的所有GPU上同步这些梯度。每个GPU负责更新模型参数的特定分片。接下来，通过AllGather操作从其他GPU收集更新后的模型参数分片，确保所有GPU都拥有最新的模型参数。ZeRO-1将优化器状态的内存消耗从降低到，其中是数据并行的大小。在ZeRO-1的基础上，ZeRO-2进一步在所有GPU上分片梯度，并且每个GPU只更新其参数分片，从而将持有梯度的内存需求从降低到。ZeRO-3除了分片优化器状态和梯度外，还分割参数。每个GPU只保存部分参数。当需要来自远程GPU的参数进行即将进行的计算时，它们通过AllGather操作被收集，并在之后丢弃。在ZeRO-3中，每个GPU只保存与其特定参数分区相对应的权重、梯度和优化器状态，从而将整体内存消耗从降低到。ZeRO被广泛采用于众多框架中，例如**DeepSpeed** [183]、**PyTorch-FSDP** [146]和**ColossalAI** [253]。

#### 6.2.2 Partially Sharding

ZeRO面临通信挑战，因为集体通信操作的延迟随着通信规模的增长而增加。在分布式LLM训练中，内存利用(memory utilization)与通信成本(communication cost)之间存在权衡。通过将模型状态分片到较小的GPU组(大型GPU集群内的较小GPU集合)，可以优化通信开销。这种方法减少了节点间通信和通信规模，但由于模型状态的冗余增加，可能导致内存使用量更高。关键是平衡通信规模与内存利用[305]。

基于ZeRO框架，已经提出了一些方法来解决通信效率低的问题，同时提高内存利用率。ZeRO++ [302] 在遵循ZeRO-3的基础上，全局地跨所有设备划分所有模型状态，并在GPU子组中引入了第二级参数分片。在前向阶段，它利用所有GPU上的主分片收集参数，并在子组内维护第二级参数分片，通常在同一个节点内。在反向阶段，它从这第二级分片中收集参数，从而减少通信规模和节点间通信。此外，ZeRO++ 使用量化压缩参数和梯度，以牺牲准确性的代价有效减少通信量。MiCS [147] 和 FSDP [146] 在子组内分片所有模型状态组件，并在子组间复制它们，从而减少通信规模，进而降低通信延迟，提高训练性能。AMSP [305] 和 PaRO [303] 结合了三种灵活的分片策略，包括全副本、全分片和部分分片，允许模型状态内的每个组件独立选择分片策略。AMSP 将其表述为一个优化问题，以在内存约束下找到最小化通信成本的最佳分片策略。此外，AMSP 提出了一个自定义的通信与计算重叠策略，将这些灵活的分片策略结合起来以实现优化的训练效率。RTP(旋转张量并行) [304] 通过策略性地分片激活和旋转权重/梯度，力求最小化内存重复。

### 6.3 去碎片化

GPU内存碎片化指的是在相邻张量之间产生的分散且无法使用的GPU内存块。在训练大型语言模型(LLM)期间，这一问题尤为突出，这是由于不同张量的生命周期各异，以及像PyTorch [240] 和 TensorFlow [326] 这样的通用深度学习框架在内存分配和回收方面的低效。此外，诸如重计算和卸载之类的内存优化技术通过引入更频繁且不规则的内存分配和回收请求，进一步加剧了这一问题 [299, 306, 308]。内存碎片化问题可能导致高峰内存占用和内存不足(OOM)错误，限制了批处理大小和整体训练效率。提出了去碎片化的努力，通过内存管理技术来减轻这些问题。

#### 6.3.1 Tensor-based Defragmentation

深度学习框架通常使用带有内存池的缓存分配器来实现快速内存分配和回收，而无需进行设备同步。针对缓存分配器中的张量分配和回收方案，已经提出了几种减少内存碎片的方法。ROAM [306] 方法通过考虑张量的生命周期和大小，优化操作符的执行顺序和张量分配。它引入了一种高效的基于树的算法来搜索执行计划，以最大化张量重用并减少数据碎片。ROAM已在单GPU场景中进行了评估，最大的模型是1.5B的GPT-2 XL [5]，但尚未在分布式训练场景以及更大的模型中进行测试，在这些场景中，计算图可能会变得非常大。Imanishi等人 [307] 提出了一种将张量分配建模为2D装箱问题的离线优化方法。在这个模型中，每个张量分配被表示为一个可垂直移动的矩形，反映了模型训练期间的周期性分配模式。他们提出了一种使用模拟退火算法来优化分配的拓扑排序的启发式算法，旨在最小化碎片。尽管这种方法是有效的，但当应用于大型语言模型(LLMs)时，由于涉及的分配数量众多和模式复杂，可能会面临可扩展性问题。MegTaiChi [298] 和 Coop [299] 在减少内存消耗时考虑了驱逐激活张量时的内存碎片问题。

#### 6.3.2 VMM-based Defragmentation

GMLake [308] 和 PyTorch 可扩展段 [309] 提出通过利用低 Level  CUDA 驱动程序应用程序接口的虚拟内存管理(VMM)功能来减轻碎片化问题。这个低 Level  API 为开发者提供了对 GPU 虚拟内存操作的直接控制，例如保留、映射和管理虚拟内存地址。在此基础上，GM-Lake [308] 引入了一种虚拟内存拼接机制，通过虚拟内存地址映射将非连续内存块整合成更大的内存块，从而最小化数据移动和复制。同样，PyTorch 的可扩展段 [309] 能够将已分配的内存段扩展到更大的尺寸以供重用。这两种方法对于不同的模型和内存高效训练技术都是透明的，并且可以无缝集成到现有的深度学习框架中。此外，GMLake 在多 GPU 上展示了极好的可扩展性，并且开销极小，无需修改用户代码。PyTorch-v2.1 也已经集成了可扩展段。

### 6.4 卸载

为了在较少的GPU上高效训练大型语言模型(LLMs)，已经提出了各种利用交换和卸载方法的研究。这些技术将部分计算和数据从GPU内存转移到外部资源，这些外部资源成本较低、速度较慢，但具有巨大的容量。

#### 6.4.1 CPU 卸载

许多研究提出了方法来有效地利用CPU内存以增强分布式大语言模型(LLM)的训练。这些技术可以大致分为两种主要方法：静态卸载和动态卸载。

**静态卸载。** 静态卸载方法涉及在GPU和CPU内存之间预定的模型组件分配。L2L [310] 管理并逐层移动张量。L2L在GPU内存中同步获取即将进行计算的下一层所需的张量，同时将剩余层的张量存储在CPU内存中。L2L允许将模型扩展到任意深度，但未能跨多个GPU进行扩展。相比之下，ZeRO-Offload [311] 集中于多GPU训练。它在GPU上保存模型参数，并在CPU内存中存储优化器状态和梯度。此外，它还将优化器更新计算卸载到CPU上。这种方法使训练可以使用多达16个V100 GPU来训练高达70B的模型。然而，ZeRO-Offload可能会留下一些GPU内存未使用，并且存在CPU优化器更新缓慢的问题[312]。

为了解决这个问题，**Elixir** [312] 利用预运行时模型分析，使用搜索引擎找到最佳的内存分区和卸载组合。与ZeRO-Offload不同，Elixir通过在GPU和CPU之间分割模型状态和优化器块，有效地利用所有可用的GPU内存。**Mobius** [315] 针对具有有限GPU间带宽和高通信争用问题的商用服务器上的多GPU训练，引入了一种流水线并行方案。此方案为每个GPU分配多个阶段，并在GPU和CPU内存之间动态交换它们。此外，Mobius通过预取和跨映射优化通信，以减少开销和争用。Yuan等人[301]提出通过在流水线阶段的粒度上卸载和重新加载激活来缓解激活瓶颈，同时最大化激活传输与计算的重叠，从而避免减缓训练过程。与其他卸载工作相比，这项工作更多地关注于改善计算与内存利用率之间的平衡，而不是在极其紧张的内存预算下进行训练。

#### 6.4.2 动态卸载

**动态卸载**方法根据内存利用率和数据传输的实时优化，在GPU和CPU内存之间自适应地分配模型或张量的分区。STRONGHOLD [318] 提出了一种在GPU和CPU内存之间动态卸载模型状态的方法，并维持一个合适的工作窗口大小，以最小化卸载过程中的GPU停滞。**Harmony** [316] 采用基于启发式的调度器将计算和模型状态映射到物理设备上，通过减少交换次数和快速的点对点交换，降低卸载的额外开销。**TMOF** [317] 引入了不连续交换和双向重叠协调机制，以防止在交换和卸载过程中出现PCIe通道争用。对于MoE模型，**MPipeMoE** [327] 设计了一种自适应且内存高效的流水线并行算法。特别是，MPipeMoE 通过消除内存冗余和自适应选择组件来决定是否卸载或重新计算所需的张量，从而实现高效的内存复用策略，以减少内存需求。

为了促进更好的内存管理，一些研究提出了将张量分解为更细粒度单元的系统。TSPLIT [313] 和 PatrickStar [314] 是两个动态内存管理系统，它们优化了峰值GPU内存使用。**TSPLIT** 将张量分割成微张量，并在微张量 Level 执行操作，从而实现精确和动态的内存操作。**PatrickStar** 将模型数据组织成内存块，这些内存块在CPU和GPU内存之间动态分配，并优化CPU-GPU数据传输以及带宽利用率。此外，TSPLIT 使用模型引导的规划算法为每个张量找到最优的内存配置，而PatrickStar 则采用运行时内存跟踪、块驱逐策略和设备感知的操作放置进一步减少CPU和GPU之间的数据移动。

#### 6.4.3 SSD 卸载

为了促进万亿规模大型语言模型(LLM)的训练，针对仅依赖CPU卸载方法不足的问题，已有几项工作提出了在训练过程中将数据卸载至CPU内存和NVMe SSD的研究。ZeRO-Infinity [319] 方法将所有分区的模型状态卸载到CPU或NVMe内存，只将激活数据卸载到CPU内存。这种方法支持在32个节点上(总计512个V100)训练多达32T参数的模型。然而，对于激活数据的CPU卸载仍需大量CPU内存。例如，训练一个10T模型大约需要0.76 TB的CPU内存来存储激活检查点，而100T模型则需要大约4 TB。Fuyou [322] 针对在CPU内存容量有限且仅有一个GPU的商用服务器上训练LLM进行了研究。与ZeRO-Infinity相比，Fuyou进一步将激活数据卸载到SSD，并将SSD-CPU通信作为一个额外的优化维度。它还提出了一种同步的外核CPU优化器，该优化器与反向传播阶段重叠，并引入了自动激活交换机制，从而最大化GPU利用率。Smart-Infinity [321] 提出通过使用近存储处理设备进行参数更新来减少对二级存储带宽的需求。MoSys [323, 324] 结合了不同的存储设备(GPU、CPU内存和SSD)来保存稀疏参数状态和密集参数状态，并提出了一种2D预取调度策略以应用于MoE训练，使得参数计算可以与调度过程重叠。

## 7 通信优化

不同的并行机制引入了不同的网络通信流量模式。例如，张量并行需要在张量并行等级上进行AllReduce操作。另一方面，数据并行需要在每次迭代结束时，在数据并行等级上进行梯度同步的AllReduce操作。流水线并行则是在每个阶段结束时将激活值传递给下一个阶段。通常，训练框架会将需要高带宽的张量或序列并行通信组放置在高带宽域内(例如，同一节点内)，而将带宽需求较低的数据并行或流水线并行通信组放置在高低带宽域之间。图13展示了实际大语言模型(LLM)训练的通信 Heatmap ，很好地反映了不同并行策略带来的数据流量。从这张 Heatmap 可以看出，LLM训练的通信流量呈现出明显的模式和层次结构，大多数通信在较小的范围内进行，只有少量流量跨越整个集群。这一洞察启发了如轨道优化拓扑(rail-optimized topology) [62]等方法的提出，该方法减少了不必要的核心交换以降低成本。

<img src="Fig13.png" alt="Fig13.png" width="400" />

图13：在单个迭代过程中，使用128个GPU对**InternLM-2 102B**进行预训练时的通信流量热图(Heatmap)，其中

在单次迭代中使用128个GPU进行预训练，采用张量并行（TP）大小为8，流水线并行（PP）大小为4，数据并行（DP）大小为4，以及ZeRO阶段1（ZeRO-1）大小为4。拓扑结构的优先级是TP > DP/ZeRO-1 > PP。

存在四种不同的数据通信负载：1. TP的AllReduce；2. 3. DP/ZeRO-1的ReduceScatter/AllGather；4. PP的Send/Recv。

TP的通信利用NVSwitch的全连接拓扑结构，导致图中沿着对角线有16个密集的正方形流量模式，每个模式代表一个节点。DP和ZeRO-1的跨节点通信流量在图中显示为四个32x32矩形拓扑内的六条对称对角线。需要注意的是，DP/ZeRO-1还涉及节点内的通信流量，这些流量累积到与TP相同的热图网格中。由于其相对较小的通信量，PP在热图上形成了两条黄色线条，坐标为((32, 0), (128, 96))和((0, 32), (96, 128))。（在这个图中，所有通信都使用基于环的集体算法）

图14：针对分布式LLM训练的通信优化研究。

<img src="Fig14.png" alt="Fig14.png" width="400" />

本节介绍了优化分布式LLM训练集体通信性能的系统和技术。如图14所示，作者首先讨论了**集体通信库**，这些库同时使用预定义和合成的算法。接下来，作者探索通信调度技术，这些技术旨在重新组织通信操作以与计算重叠，从而减少延迟并加速训练过程。最后，作者深入探讨了网络内聚合(INA)，该方法利用网络设备的计算能力执行聚合操作，例如对深度学习模型的梯度进行求和。

有效地压缩模型参数和梯度可以减少分布式LLM训练期间的通信开销。各种研究探索了稀疏通信和量化方法。例如，ZeRO++ [302]采用权重量化方法，在通信前将每个模型参数从FP16缩小到INT8数据类型。然而，这些工作通常涉及有损失的稀疏化或量化技术。由于这些技术超出了本工作的范围，作者在此部分不讨论有损数据压缩技术。

### 7.1 集体通信

消息传递接口(MPI, Message Passing Interface)是一种在并行计算架构上广泛应用于大规模科学计算的编程模型。MPI有几个实现版本，包括OpenMPI [328]、MPICH2 [329]和MVAPICH [330]。这些库提供了各种CUDA感知原语，如AllReduce、AllGather和ReduceScatter，这些对于分布式大型语言模型(LLM)训练至关重要。实际上，当前的训练框架更倾向于使用为特定人工智能加速器量身定制的集体通信(Collective Communication)，这些通信具有预定义或合成的算法。

#### 7.1.1 预定义的集体通信算法

英伟达的**NCCL** [331] 和AMD的**RCCL** [332] 是高度优化的库，它们通常在各自的AI加速器上超越基于MPI的集体通信库。这些库通常会根据网络拓扑和输入张量大小等条件选择预定义的算法来执行集体操作。

**环算法**(Ring Algorithm)。 环算法用于像AllReduce这样的集体通信，以跨所有GPU移动数据。使用此算法，输入张量被分割成多个块，并在操作期间逐一传输。这个流水线减少了每个设备在等待数据时的空闲时间。百度使用了带宽最优的环AllReduce算法 [333] 进行分布式深度学习模型训练。Horovod [143] 用NCCL替换了百度的环AllReduce实现，并为分布式训练设计了一个用户友好的接口。

**树算法**(Tree Algorithm)。 随着GPU设备数量的增加，环算法的延迟也会增加 [346]。为此提出了双重二叉树算法 [334]。双重二叉树依赖于这样一个事实：在二叉树中有一半或更少的等级是节点，一半或更多的等级是叶子。因此，可以构建第二个树，使用叶子作为节点，反之亦然，对于每个二叉树。这个算法在基于MPI的库、NCCL和RCCL中实现。

**混合算法**(Hybrid Algorithm)。 几种方法提出了使用混合算法来处理在具有异构的节点内和节点间通信带宽的训练集群上的集体通信任务。双层AllReduce [335] 将单个AllReduce操作划分为三个步骤：节点内使用PCIe/NVLINK的Reduce，节点间使用网络的AllReduce，以及节点内的Broadcast。2D-Torus AllReduce [336] 和ACCL [337] 将单个AllReduce操作分解为三个阶段：节点内的基于环的ReduceScatter，节点间的基于树的AllReduce，以及节点内的基于环的AllGather。BlueConnect [338] 将单个AllReduce操作分解为许多可并行化的ReduceScatter和AllGather操作。每个操作可以映射到不同的网络结构，利用每种特定结构性能最佳的预定义实现。Plink [339] 可以检测网络拓扑并有效地生成两层混合通信计划，利用数据中心网络中的局部性。

#### 7.1.2 综合集体通信算法

已经出现了几种方法，它们综合了专门针对硬件拓扑定制的集体通信算法和内核，旨在许多情况下超越通用的预定义算法。**GC3** [340] 引入了一种面向数据的领域特定语言 (DSL)，用于设计自定义集体通信算法。它包括一个优化编译器，可将这些算法转换为针对特定硬件配置优化的可执行形式。**SCCL** [341] 将集体通信综合问题编码为 SMT (可满足性模理论) 公式。该方法旨在为帕累托最优算法推导出精确的调度，优化延迟和带宽利用率。**TACCL** [342] 将寻找最佳通信算法的问题表述为混合整数线性规划 (MILP)。它利用通信草图抽象来有效地收集基本信息并减少搜索空间，目标是最小化总体执行时间。 **Blink** [343] 通过在运行时探测每个作业的可用链接集，动态构建具有合适链接容量的拓扑。使用此拓扑，它通过创建数据包生成树和生成 CUDA 代码来优化通信速率。**$P^2$**[344] 利用并行矩阵在系统级别划分并行轴，从而生成拓扑感知的并行布局和缩减策略。通过模拟和预测通信成本，该方法减少了所需的实际评估次数。

### 7.2  通信调度

分布式训练中的通信调度将通信操作重新组织为与计算重叠，从而减少延迟并加速训练过程。通信调度的关键概念涉及根据并行训练的数据依赖性重新排序通信操作。混合并行LLM训练需要多维通信调度方案来管理由数据、管道、张量和序列并行及其组合产生的通信。

#### 7.2.1 基于先进先出的调度

在反向传播阶段，不是等到所有梯度计算完成后再开始通信，而是可以在每个梯度准备好后立即开始通信。这种无等待的反向传播方法利用了一个依赖导向的无环图来高效地管理任务。**Poseidon** [345] 使用先进先出(FIFO)队列来调度AllReduce操作符，确保每个层在其梯度生成后立即开始通信。受到在大张量上集体通信效率的启发，**GradientFlow** [346] 和 **Pytorch DDP** [142] 将多个连续的AllReduce通信操作融合为单一操作。这种方法通过在反向传播阶段等待短时间，然后将多个梯度合并为一个AllReduce操作，避免了通过网络传输大量小张量。

#### 7.2.2 基于优先级的调度

FIFO方案通常不是最优的，因为反向传播阶段生成的通信序列与正向传播阶段的计算序列不匹配。这种不匹配可能导致通信阻塞计算，即使启用了重叠功能。因此，许多方法采用优先队列来有效地调度通信操作符。**P3** [347] 以更细的粒度安排AllReduce操作，将当前层的梯度通信与下一层的正向计算重叠。与基于FIFO队列的调度不同，此方法将层划分为固定大小的切片，并根据它们在正向传播中的处理顺序优先同步切片。因此，第一层获得最高优先级，越往后优先级越低。在分布式模型训练中使用参数服务器架构时，**TicTac** [348] 优先处理那些加速底层计算图中关键路径的传输。

**ByteScheduler** [349] 和**PACE** [350] 被提出用于跨训练框架推广基于优先级的通信调度。特别是，ByteScheduler [349] 引入了一个统一抽象概念，以促进通信调度，同时不干扰框架引擎中原有的依赖关系。ByteScheduler 通过使用贝叶斯优化来自动调整两个关键参数：分区大小和信用大小，从而实现良好的性能。PACE [350] 通过将原始的AllReduce操作分割成更小的片段来实现抢占式通信。被抢占的AllReduce操作可以在稍后的时间点恢复。这种抢占策略防止了大通信张量的队头阻塞。此外，PACE 使用动态规划方法融合小通信张量，以减少处理大量小张量引起的开销，从而实现更有效的带宽利用。

为了提高MoE系统中的带宽效率，**Lina** [188] 将All-to-All操作优先于AllReduce。通常，专家并行(All-to-All)和数据并行(AllReduce)过程使用分开的CUDA流，可能导致潜在的重叠和带宽共享但没有协调。Lina 将张量分割成更小的块，确保All-to-All操作获得全部带宽，同时允许AllReduce微操作在空闲时间运行。此外，微操作还使All-to-All操作与专家计算重叠成为可能。

#### 7.2.3 基于分解的调度

几个进展集中在将**通信**和**计算操作**分解(decomposing)为细粒度任务，并以更大的灵活性重新排序这些操作，以最大化重叠并优化执行效率。

**流水线阶段分解(Pipeline Stage Decomposition)。**在使用传统的流水线并行时，每个GPU存储连续的层段。Breadth-First [159] 进一步将这些连续阶段分割成更细粒度的阶段，分布在不同GPU上，通过连接第一个和最后一个GPU形成一个环，因此每个GPU分配有多个阶段。这使得给定的微批次可以更早地到达流水线末端，减少流水线气泡。Breadth-First使用广度优先调度策略来实现更大的计算-通信重叠。Fold3D [351] 采用全入全出调度策略，以重叠流水线的梯度同步过程与计算。这涉及在流水线内进一步折叠模型片段，其中每个设备包含两个模型片段，允许一个片段的梯度同步与另一个片段的前向或后向计算重叠。

异步流水线并行放宽了梯度与参数更新之间的数据依赖。利用这一特性，TriRace [352] 推迟参数更新以最大化与梯度通信的计算重叠。此外，TriRace将流水线阶段之间的双向P2P通信分解为两个单独的单向操作，并根据关键路径分析优先处理它们。

**通信分解(Communication Decomposition)。**通信原语可以被分解为具有高调度灵活性的细粒度操作。Wang等人 [353] 将通信操作(例如，AllGather和ReduceScatter)分解为一系列细粒度的点对点集合。此外，计算操作(例如，爱因斯坦求和)也被划分为细粒度任务，每个任务执行部分计算。这种分解为重叠通信与计算创造了更多机会。SYNDICATE [354] 将通信操作分割成更小的子操作，称为主题(Motifs)，并采用中心优化器使用马尔可夫链蒙特卡洛搜索来实现最优重叠执行计划。Centauri [355] 采用了不同的方法，通过使用原始分割、组分割和工作负载分割将通信操作分解为细粒度的原子操作。然后使用工作负载感知调度、后向调度和弹性调度来最大化重叠效率。DeAR [356] 也分解了通信原语，特别是将AllReduce分解为AllGather和ReduceScatter。这种分解允许后续操作与模型的前向传播过程重叠，从而无需等待通信步骤的完成。

**计算分解(Computation Decomposition.)。**当使用张量并行时，前向阶段需要使用AllReduce通信来同步矩阵乘法的输出。**CoCoNet** [357] 通过将输出分割为较小的块并在矩阵乘法 Kernel 中计算每个结果块后立即启动AllReduce Kernel ，来促进矩阵乘法与AllReduce的重叠。为了最小化等待AllReduce Kernel 的时间，数据块被按照精心安排的顺序输入到矩阵乘法 Kernel 中。T3 [358] 应用了硬件-软件协同设计方法，透明地重叠矩阵乘法与通信，同时最小化资源争用。在硬件层面，T3引入了跟踪-触发机制来协调生产者的计算和通信活动。此外，它还使用计算增强型存储器来处理通信过程所需的附带计算操作。

反向传播生成两种类型的梯度：**输出梯度**，用于计算前一层梯度；**权重梯度**，用于更新层的权重参数。这些权重梯度需要使用AllReduce与其他等级同步。传统框架同时为权重和输出计算梯度。**乱序反向传播**(**ooo-backprop**,Out-of-order backpropagation) [361] 解耦了权重和输出的梯度计算，灵活地按非原始顺序调度权重梯度计算。这使得可以优先和相应地安排更关键的计算。因此，ooo-backprop通过基于这种乱序计算策略调度通信来优化整体性能。Zero Bubble [156] 也使用此方案来减少流水线并行的气泡率。

启用激活检查点后，训练框架需要在反向阶段重新计算激活。当使用张量并行时，这种重新计算也涉及AllReduce通信。**Oases** [359] 通过总是将AllReduce通信作为重新计算单元最后一个前向通信操作来减少重新计算中的冗余通信，并进一步将批次分割为较小的子批次，允许两个批次的通信和计算重叠。**Lynx** [360] 也利用了重新计算和通信重叠的潜力，使用两种重新计算调度算法OPT和HEU来搜索最优或近优的重新计算调度策略，实现最佳重叠和训练性能。

### 7.3 网络内聚合

网络内聚合(INA, In-Network Aggregation)利用网络设备的计算能力执行诸如深度学习模型梯度求和等聚合操作。这种技术之前已被提出用于加速大数据处理。值得注意的是，像**NetAgg** [369]、**SwitchAgg** [370] 和 **CamDoop** [371] 这样的框架，通过在直接连接拓扑中的交换机附接高性能中间盒或服务器上执行数据聚合，已经显示出显著的性能优势。许多方法已经被提出，将网络内聚合应用于深度学习模型的训练中，目的是在反向传播阶段对梯度进行AllReduce操作时减少节点间交换的数据量[372]。

#### 7.3.1 基于以太网的内聚合

许多基于以太网的网络内聚合系统依赖于可编程交换机，可以被用于分布式大型语言模型(LLM)训练。**SwitchML** [362] 支持在分布式训练的反向传播阶段将集体通信操作卸载到可编程网络交换机上。由于完整的模型更新可能超出交换机的存储容量，SwitchML 通过交换机流式传输聚合操作，一次只处理有限数量的向量元素。SwitchML 有两个局限性。首先，在处理浮点操作时，SwitchML 不能直接为浮点张量执行集体通信(如 AllReduce)。相反，它使用类似于块浮点的方法将浮点值转换为32位整数。其次，SwitchML 主要在DPDK上实现，尽管有一个支持RDMA的实现，但它很难与训练框架集成。

为了更好地促进分布式模型训练，**FPISA** [363] 将浮点计算实现在一个直接在可编程交换机上运行的P4 [373] 程序中。因此，训练框架可以将FP16张量的集体通信操作卸载到交换机上，而无需将它们转换为32位整数。**NetReduce** [364] 支持与RoCE兼容的网络内聚合，完全利用了RoCE的拥塞控制和可靠性设计，无需在交换机中处理昂贵的网络协议处理堆栈。NetReduce 使用一个连接到以太网交换机的FPGA板进行原型设计。AllReduce-Switch [365] 与NetReduce密切相关，且与其网络协议兼容。它引入了一种新颖的交换机架构，专为网络内聚合任务而设计，并使用FPGA硬件实现了原型。**PANAMA** [366] 和**ATP** [367] 也通过为共享环境设计的网络内聚合框架为该领域做出了贡献。PANAMA 专注于通过管理多个同时活跃训练作业之间的带宽分配来优化网络负载。它解决了传统拥塞控制可能不足以支持同时训练操作的问题。另一方面，ATP 允许多个并发租户同时运行几个作业，强调在共享环境中支持不同工作负载。

某些工作针对特定的训练工作负载而定制，使其不适用于分布式LLM训练。例如，**Libra** [374] 针对使用参数服务器架构的稀疏模型训练而设计。它将频繁更新的参数的聚合卸载到可编程交换机上，而将不常更新的参数留给服务器处理。这种方法有效地减轻了服务器负载。另一方面，**iSwitch** [375] 针对强化学习训练任务中的参数聚合而设计。尽管其基于FPGA的实现支持本地浮点操作，但其带宽显著较低。此外，iSwitch 在聚合过程中存储整个梯度向量，这对于强化学习工作负载是可行的，但并不适用于大规模模型，尤其是LLM的扩展。

#### 7.3.2 基于Infiniband的内聚合

NVIDIA Mellanox的**可扩展分层聚合协议**(SHARP,  Scalable Hierarchical Aggregation Protocol ) [368]是一种专有的网络内聚合方案，可在特定的InfiniBand交换机和NVIDIA GPU中使用。建立在InfiniBand之上，SHARP利用链路层流控制和无损保证，并使用专用的片上浮点处理单元(FPU)进行集体卸载。SHARPv1在InfiniBand EDR交换机上推出，而SHARPv2在InfiniBand HDR交换机上得到增强，具备了支持集体通信(例如：屏障、归约、全归约和广播)、整数和浮点操作(16/32/64位)以及GPUDirect RDMA等功能。SHARPv2还使用流式聚合以线路速率处理大型矢量归约，与NCCL集成，并且易于被现有训练框架使用。在最新的InfiniBand NDR交换机上启用，SHARP已准备好用于分布式大型语言模型(LLM)训练，并已部署在许多训练集群中。除了Infiniband，NVIDIA的NVSwitch-v3[46]也集成了SHARP，以加快基于GPU的集群中的集体操作速度。

## 8 容错

大规模语言模型(LLM)的训练通常需要长达数周到数月的延长训练期，并使用成千上万的GPU集群。涉及的众多组件，从底层基础设施到训练系统的优化，都需要强大的容错机制(fault tolerance mechanisms)来确保训练过程的可靠性。这是因为系统中任何部分的单一故障点都可能导致训练过程因训练的同步性质而暂停。在本节中，作者首先介绍LLM训练中的故障分析，然后研究快速故障检测和恢复的方法。

### 8.1 LLM 故障分析

来自不同来源的实证研究表明，在大型语言模型(LLM)训练中故障(failures)频发。例如，Bloom模型的训练在平均每周会有1-2次GPU故障，该训练是在一个拥有384个GPU的集群上进行的[292]。Meta公司对992个A100 GPU上1750亿参数OPT模型的全面训练记录[398]显示，在两周内就有超过40次中断，这些中断归因于硬件、基础设施以及其他外部因素。最新的研究进一步突显了这个问题。Acme[23]在其使用超过1000个A100 GPU的训练过程中平均每1-2天报告一次故障发生。字节跳动的MegaScale项目[71]，使用了12288个Ampere GPU，在几周内就经历了超过100次故障。Meta的LLaMA3在为期54天的预训练期间，在一个由16384个H100 GPU组成的集群上经历了466次作业中断[9]。这种频繁的故障主要归因于系统的巨大复杂性和规模，以及训练周期的延长。正如作者在前几节所调研的，整个训练系统包含了大量的组件。此外，同步训练进一步加剧了这个问题，因为任何一个节点上的错误都可能导致整个作业失败，使得系统特别容易受到孤立硬件或软件故障的影响。即使像阿里巴巴集群[383]观察到的单个节点看似较低的每日1.5%的故障率，在扩展到拥有1000个GPU的系统时，也会飙升至惊人的84.8%的每日故障率。然而，训练系统的规模化趋势仍在增长，这强调了容错机制在保持系统可靠性方面所面临的伴随挑战。

这些故障背后的原因是多方面的，源自LLM训练系统的各个组成部分。根据Acme[23]的研究，影响最严重的是硬件故障，例如GPU问题(例如CUDA错误、ECC错误)、NVLink和网络系统问题(例如NCCL超时错误、连接错误)。阿里巴巴C4[377]也提供了类似的观察结果。C4进一步观察到，绝大多数错误(约82.5%)局限于特定节点甚至单个设备，尽管用户观察到的多数错误是NCCL错误。LLaMA3预训练[9]也报告称，78%的故障是硬件问题。此外，最新一代的GPU(A100和H100)往往具有较高的错误率，可能是由于快速开发、急于交付和功耗增加[377, 399]所致。除了硬件问题之外，分布式训练框架、数据预处理流程或库依赖中的软件相关问题可能导致崩溃或意外行为[378, 399, 23]。模型本身的复杂性也可能引入不稳定因素，如损失尖峰、数值溢出或下溢、梯度爆炸或优化困难[398, 400]。数据中心停电或冷却系统故障等外部因素进一步加剧了系统的稳定性问题。例如，集群服务器机房的高温也倾向于导致GPU过热，这可能导致NVLink错误或ECC错误[23]或不稳定的训练速度[9]。

图15：针对分布式LLM训练的容错技术研究。

<img src="Fig15.png" alt="Fig15.png" width="700" />

这些高频率和多方面的LLM故障导致GPU资源的巨大浪费。这种低效体现在两个方面：故障恢复和性能下降。首先，LLM训练作业在运行时定期保存检查点以保持进度。一旦发生故障，系统维护行人必须首先定位并诊断问题，然后通过回滚到以前的检查点来重新启动训练。然而，某些硬件故障很难主动检测到，并且常常需要相当长的时间来诊断和恢复，导致LLM训练长时间停滞。其次，集群中的拖后腿现象，由网络链路故障[377]或异常的计算减速[71]导致，可以显著降低最大频率利用率(MFU)，进一步加剧了整体训练低效。Meta公司1750亿参数OPT模型的训练[398]就是这些低效的例证。尽管基于MFU的理想训练时间估计约为25天，但实际的训练持续了大约57天。这意味着总时间的56%都浪费在处理各种故障上，突显了系统不稳定对资源利用和LLM训练效率的严重影响。

### 8.2 异常检测

快速检测和诊断大语言模型(LLM)的失败对于保持训练的稳定性和效率至关重要。这个过程被称为异常检测，主要采用两种方法：统计监控和主动验证。

#### 8.2.1 统计监控

统计分析是一种系统性的方法，用于观察和分析大型语言模型(LLM)训练过程中的各种指标和指示器。该方法包括收集、处理和解释数据，以识别与预期行为不符的异常或偏差。在典型的设置中，每个GPU分配有一个专用的监控进程，负责收集基本信息和运行时统计信息[378, 379, 71]。这些统计数据随后作为心跳消息传输到中央监控节点进行进一步分析。未能发送心跳消息的节点被视为已失败。此监控系统的主要目标是及时检测异常，以便快速恢复，尽量减少训练中断，并保持整体效率。

在LLM训练中监控的大多数运行时统计信息与硬件相关，包括GPU和网络指标。近期的研究[378, 379, 71]使用NVIDIA DCGM[401]收集与GPU相关的统计信息，包括SM块利用率、SM占用率、SM Pipeline 利用率、PCIe通信速率、NVLink通信速率等。一个常见的问题是GPU内存行重映射，这在硬件中无缝地将已知的退化内存单元替换为稀疏单元。Vela[378]通过利用DCGM中的DCGM_FI_DEV_ROW_REMAP_PENDING统计信息来检测这一问题。Megascale[71]和Transom[380]也通过分析训练日志中发生的错误来检测错误。

除了GPU指标之外，网络统计信息对于监控分布式训练性能至关重要。MegaScale[71]追踪RDMA通信指标以检测潜在的异常，并开发可视化系统以手动识别效率低下的GPU。Unicron[379]检测训练过程中的NCCL超时、TCP超时以及任务挂起延迟通知等错误。C4[377]收集连接的具体信息，如RDMA IP和QP号码，以及传输层上的消息统计信息，包括传输的次数、大小和持续时间，以检测训练减速和挂起。集体通信活动也可以使用PyTorch内置的NCCL飞行记录器[273]进行监控，后者将集体元数据和堆栈跟踪捕获到一个环形缓冲区，以便稍后诊断。Meta进一步与PyTorch共同设计NCCLX[9]，使PyTorch能够访问其内部状态以实现快速准确的故障检测。NCCLX追踪每个NCCLX通信的 Kernel 和网络活动，这有助于诊断通信问题。Vela[378]实现了一个增强的多NIC健康检查器，收集所有端口上每对2节点之间的节点网络带宽数据。这些信息可以用来检测RoCE/GDR性能退化的节点。利用LLM训练的关键特性作为先验知识，Transom[380]开发了机器学习算法进行异常检测。

统计监控还使谷歌的TPUv4超级计算机[376]具有弹性。每个TPUv4机器都配备了一个健康守护进程，该进程对ICI(芯片间互连)链路、PCIe链路和TPU ASIC进行实时监控。检测到的严重症状将通知集群调度器采取适当的行动，例如驱逐受影响的工作或重新安排它们。

#### 8.2.2 主动验证

主动验证(Proactive Validation)提供了一种基于在线统计监控的替代反应性故障排除方法，旨在在故障发生前验证训练系统。然而，在验证测试时间和准确性之间存在一种权衡，因为全面的验证可能会显著影响有效的训练时间。MegaScale [71] 提出了一套轻量级测试，包括网络内主机和NCCL测试，以诊断各种潜在的故障。Vela [378] 采用了双层策略，轻量级测试定期在每个节点上运行，而更具侵入性的测试仅在节点空闲时执行。谷歌的TPUv4超级计算机在用户作业之前实施了一个预检 [376]，包括一个端到端的检查和一个针对硬件健康状态的意图驱动检查器。SuperBench [381] 提出了一个综合的基准测试套件，用于评估单个硬件组件，并包含一个选择器，以平衡验证时间与可能的问题相关惩罚。

### 8.3 基于检查点的恢复

定期保存**模型状态**，即“检查点(checkpointing)”，在失败发生后从最新的检查点恢复计算，是容错大语言模型训练的常规做法。然而，这存在一个两难选择：频繁的检查点会带来高的I/O开销，而不频繁的检查点在失败发生时会导致大量的进度损失。为了解决这一两难问题，已经设计了快速持久化内存中检查点的方法。

#### 8.3.1 Persistent Checkpointing

**持久性检查点(Persistent Checkpointing)。** 持久性检查点涉及将模型状态保存到非易失性存储，例如固态硬盘(SSD)和远程云存储，以确保在系统故障时数据的持久性。这个过程通常包括两个阶段：首先，在快照阶段，将模型状态从GPU复制到CPU内存；其次，在持久阶段，将快照写入持久存储设备。尽管由于存储设备带宽较低导致I/O开销相当大，但持久性检查点由于其易用性和可靠性，仍然是容错的广泛采用方法。为减少训练中断，已经提出了高级持久性检查点方法，从而可以在没有显著性能损失的情况下更频繁地进行检查点。

**同步检查点(Synchronous Checkpointing)。** 为了保持模型参数的一致性，DeepSpeed的默认同步检查点[22]和Varuna[167]会定期暂停训练过程，在数据并行等级0上同步执行到持久存储的检查点。这种方法在快照和持久阶段都会导致GPU空闲，从而造成资源利用率不足。认识到大多数故障都是由单个GPU或网络设备引起的，JIT-检查点[382]提出了另一种策略。它在上次失败后立即采取即时检查点，允许从这些JIT检查点恢复训练。这种方法显著降低了浪费的GPU时间成本，将其限制在最多一个迷你批处理迭代的工作量内。DLRover Flash-检查点[383]利用分布式缓存服务提高了迁移效率。"通用检查点"[384]引入了一种通用检查点表示，以将分布式检查点存储与并行技术解耦。通用检查点可以在需要时将检查点从一种并行化策略无缝转换为另一种。

**快照-暂停检查点(Snapshot-Stall Checkpointing)。** 为了在保持检查点一致性的同时减少LLM训练中断，"检查并运行"[385]将快照和持久阶段解耦。它通过在快照阶段暂停训练并使用专用的后台CPU进程异步持久化快照来实现原子性检查点。TorchSnapshot[386]通过张量分块和多线程磁盘写入进一步优化了此过程。通过创建分块快照，TorchSnapshot允许持久阶段通过并行写入更早开始，从而减少了整体训练中断时间。"MegaScale"[71]和"InternEvo"[18]也采用了快照-暂停方法以实现快速检查点和恢复。快照阶段暂停训练几秒钟以捕获模型状态，而持久阶段则将检查点从CPU内存异步传输到分布式文件系统。MegaScale通过指定数据并行组中的单个工作进程从分布式文件系统读取，从而优化恢复过程，从而减轻了低带宽瓶颈。然后这个工作进程将检查点数据广播到其他GPU，使得整个系统可以更快、更有效地恢复。"InternEvo"还异步将检查点从昂贵的热存储移动到更便宜的热存储，以节省存储空间。

**异步检查点(Asynchronous Checkpointing)。** 异步检查点旨在通过在与训练同时执行快照和持久阶段来最小化训练中断。"DeepFeeze"[387]在后台应用轻量级(快照)和重量级(持久)持久化策略，跨数据并行GPU分片检查点以分配I/O工作负载。"CheckFreq"[388]小心翼翼地将快照和持久阶段与后续迭代的正向和反向传播 Pipeline 化，确保在下次参数更新之前完成快照。它还动态调整检查点频率以平衡恢复成本和运行时开销。"LightCheck"[389]利用迭代间数据依赖性，引入逐层检查点 Pipeline 以减少暂停。"DataStates-LLM"[390]通过为快照预分配固定主机内存并使用有效的计算、快照和持久逐层 Pipeline 化来解决主机内存分配慢的问题。"FastPersist"[391]识别了完全异步持久阶段的风险，并将其与下一个迭代的参数更新同步。它通过双缓冲固定内存提高SSD带宽利用率，并通过使用数据并行等级的子集进行检查点写入减少硬件争用。

#### 8.3.2 内存检查点

远程持久存储的低带宽严重限制了检查点(checkpointing)的频率。内存检查点(In-Memory Checkpointing)技术通过将检查点存储在其他计算节点的内存或专用的内存存储系统中，显著减少了I/O开销，并实现了更高的检查点频率。**Gemini** [392] 提出了一种将检查点存储到CPU内存以加快故障恢复的方法，并配合检查点放置策略以最小化检查点丢失，以及一个交通调度算法来减少对训练的干扰。**REFT** [393] 异步将模型状态缓存到宿主内存和内存存储系统(如Redis)，绕过检查点的I/O操作，实现了高频率的检查点。它还利用擦除编码实现了RAMS(受RAID5的启发，将“Disk”替换为“Memory”)，以保护数据免受节点故障的影响。尽管这些方法通过在不影响性能的情况下实现更频繁的检查点显著提高了LLM训练的容错能力，但它们可能无法提供与传统基于存储的方法相同的长期数据持久性。因此，结合内存检查点和持久检查点的混合方法对于全面的容错策略是必要的。

### 8.4 无检查点恢复

无检查点恢复(Checkpoint-Free Recovery)方法旨在通过消除在发生故障时重新启动并回滚到先前检查点的需求，来最小化训练中断。这些技术依赖于自动故障检测机制以迅速识别问题。当检测到故障时，无检查点方法会自动解决问题，并允许训练过程无需中断地继续进行。通过避免从检查点加载和重复计算的时间消耗过程，这些方法可以显著减少停机时间并提高整体训练效率。无检查点恢复策略可以广泛分为两种主要方法：热迁移和模块冗余。

#### 8.4.1 实时迁移

实时迁移(Live Migration)利用分布式LLM训练设置中固有的冗余，特别是不同数据并行 Pipeline 中的模型副本，以在出现故障时恢复模型状态。当检测到故障时，实时迁移方法会动态地使用剩余的健康实例或通过将新实例整合到训练集群中来重新配置并行策略。然后将当前模型状态转移到这些重新配置的节点上，使得训练过程能够以最小的中断继续进行。**Parcae** [394] 提出了三种不同的迁移机制，每种机制都有不同的通信开销，以有效地在不同并行策略之间转移模型状态。**Oobleck** [395] 采用了基于 Pipeline 模板的实时迁移方法。它维护一组预定义的 Pipeline 模板，并在检测到故障时，迅速根据这些模板实例化新的异构 Pipeline 。

#### 8.4.2 模块冗余

模块冗余(Module Redundancy)，如同动态迁移一样，也利用了模型状态的重冗性。然而，这种方法不是在不同的GPU之间恢复最新的模型状态，而是通过将计算路由到冗余模块来继续训练。**Bamboo** [396] 在同一个 Pipeline 中，在持有相邻 Pipeline 阶段的GPU中放置了一个冗余的流水线阶段。这个冗余阶段在训练期间利用流水线气泡进行冗余计算，并在出现故障时作为一个正常阶段激活。**SlipStream** [397] 利用模型副本 Pipeline 之间的冗余，将失败节点的计算路由到不同数据并行 Pipeline 的节点上。**SWARM** [231] 提出了一个类似的解决方案，但更关注连接性差、异构和不稳定的设备。除了冗余计算之外，SWARM 还结合了实例迁移来重新平衡 Pipeline ，将冗余计算和动态迁移方法的特点结合起来。

## 9 结论与展望

大型语言模型(LLM)的兴起改变了人工智能领域，使得诸如个人助理、代码生成和科学研究等应用成为可能。GPT、LLaMA和Gemini等模型树立了新标准，但是像LLaMA-3模型在16384个GPU上耗时54天的训练过程，体现了在可扩展性、效率和可靠性方面所面临的挑战。管理庞大的GPU集群需要创新的硬件和网络解决方案。有效的训练需求优化计算、通信和内存使用。可靠性则涉及到在漫长的训练期间检测和恢复故障的健壮机制。本调研回顾了近期在LLM训练系统和基础设施方面的进展，强调了提升可扩展性、效率和可靠性的方法。

传统的基于数字电路的计算系统，在摩尔定律和 Dennard 缩放律的指导下，在满足LLM训练和部署的计算需求方面面临着重大的物理和经济限制。因此，人工智能行业需要创新的解决方案。一种有前景的方法是大规模光电集成技术，它用集成硅光子学替代传统的数字电路，以增强计算和通信能力[402]。这种光电混合数据中心技术结合了光计算和光网络，提高了单节点计算能力以及大规模分布式计算的效率。有几项工作提出了利用光网络进行LLM训练。例如，TopoOpt [67] 优化了分布式训练中的光网络拓扑和并行化策略，提高了计算和通信效率。TIPUv4 [42] 使用光电路开关动态地重新配置其3D-Torus互连拓扑，为LLM训练中的密集通信模式改善数据流。此外，Taichi [403] 探索了一种分布式衍射干涉混合光子计算架构，以将光学神经网络有效扩展到百万神经元 Level ，同时达到每瓦160万亿次操作(TOPS/W)的能量效率。未来可能需要在LLM训练和推理方面向硅光子学转变的范式。然而，这种转变将需要在系统设计和实施方面进行大量创新。
