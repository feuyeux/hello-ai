## 6 附录

### 1 Reflection Pattern实战

Here’s the implementation of the agentic AI reflection pattern:

```python
!pip install groq
import os
from groq import Groq
from IPython.display import display_markdown
os.environ["GROQ_API_KEY"] = "your_groq_api_key_here"
client = Groq()
```

```python
generation_chat_history = [
   {
       "role": "system",
       "content": "You are an experienced Python programmer who generate high quality Python code for users with there explanations"
       "Here's your task: You will Generate the best content for the user's request and give explanation of code line by line. If the user provides critique,"
       "respond with a revised version of your previous attempt."
       "also in the end always ask - Do you have any feedback or would you like me to revise anything?"
       "In each output you will tell me whats new you have added for the user in comparison to earlier output"
   }
]
```

```python
generation_chat_history.append(
   {
       "role": "user",
       "content": "Generate a Python implementation of the Fibonacci series for beginner students"
   }
)
fibonacci_code = client.chat.completions.create(
   messages=generation_chat_history,
   model="llama3-70b-8192"
).choices[0].message.content
```

```go
generation_chat_history.append(
   {
       "role": "assistant",
       "content": fibonacci_code
   }
)
display_markdown(fibonacci_code, raw=True)
```

The code generated by the model is added to the chat history with the “role”: “assistant”, indicating the model’s response.

display_markdown displays the generated code in Markdown format.

**Output**

![Reflection Pattern - Output](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/unnamed-82.webp)Source: Author

#### Reflection Step

```python
reflection_chat_history = [
   {
   "role": "system",
   "content": "You are Nitika Sharma, an experienced Python coder. With this experience in Python generate critique and recommendations for user output on the given prompt",
   }
]
reflection_chat_history.append(
   {
       "role": "user",
       "content": fibonacci_code
   }
)
critique = client.chat.completions.create(
   messages=reflection_chat_history,
   model="llama3-70b-8192"
).choices[0].message.content
display_markdown(critique, raw=True)
```

- The reflection_chat_history list is initialized with a system prompt telling the model to act as a Python expert named Nitika Sharma and provide critique and recommendations.
- The generated code (fibonacci_code) is added to the reflection_chat_history with the “role”: “user”, indicating that this is the input to be critiqued.
- The model generates a critique of the code using client.chat.completions.create. The critique is then displayed using display_markdown.

**Output**

![Reflection Pattern - Output](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/unnamed-83.webp)

#### Generation Step (2nd Iteration)

```python
Generation_2 = client.chat.completions.create(
   messages=generation_chat_history,
   model="llama3-70b-8192"
).choices[0].message.content
display_markdown(Generation_2, raw=True)
```

The same generation_chat_history is used to generate an improved version of the code based on the original prompt.

The output is displayed as Generation_2.

**Output**

![Reflection Pattern - Output](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/unnamed-84.webp)

#### Reflection (2nd Iteration)

```go
reflection_chat_history.append(
   {
       "role": "user",
       "content": Generation_2
   }
)
critique_1 = client.chat.completions.create(
   messages=reflection_chat_history,
   model="llama3-70b-8192"
).choices[0].message.content
display_markdown(critique_1, raw=True)
```

The second iteration of generated code (Generation_2) is appended to the reflection_chat_history for another round of critique.

The model generates new feedback (critique_1), which is then displayed.

**Output**

![Reflection Pattern - Output](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/unnamed-85.webp)

#### Generation Step (3rd Iteration)

```python
generation_chat_history.append(
   {
       "role": "user",
       "content": critique_1
   }
)
Generation_3 = client.chat.completions.create(
   messages=generation_chat_history,
   model="llama3-70b-8192"
).choices[0].message.content
display_markdown(Generation_3, raw=True)
```

The model generates a third version of the code (Generation_3), aiming to improve upon the previous iterations based on the critique provided.

**Output**

![Reflection Pattern - Output](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/unnamed-86.webp)

**Here’s the consolidated output**

```python
for i in range(length):
 if i % 2 == 0:
   print("Generation")
 else:
   print("Reflection")
 display_markdown(results[i], raw=True)
 print()
```

![Reflection Pattern - Output](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/unnamed-87.webp)Source: Author

You will find the improved code version for each step above, including generation, reflection, and iteration. Currently, we perform reflection manually, observing that the process often extends beyond 3-4 iterations. During each iteration, the critique agent provides recommendations for improvement. Once the critique is satisfied and no further recommendations are necessary, it returns a “<OK>” signal, indicating that the generation process should stop.

However, there is a risk that the critique agent may continue to find new recommendations indefinitely, leading to an infinite loop of reflections. To prevent this, it is a good practice to set a limit on the number of iterations.

#### Stopping Conditions

The Reflection Pattern relies on well-defined stopping conditions to prevent endless iterations. Common stopping criteria include:

1. **Fixed Number of Steps**: The process can be set to run for a specific number of iterations, after which the refinement process stops. For example, the content can be refined over 10 iterations, and then the loop can be ended.
2. **Quality Threshold**: A stopping criterion can be based on the quality of the output. If the AI reaches a level of refinement where further changes are minimal or the model generates a predefined stop keyword (e.g., “satisfactory”), the iteration stops.
3. **Custom Criteria**: Users can define custom stopping rules, such as a time limit or detecting a specific phrase that indicates completion.
