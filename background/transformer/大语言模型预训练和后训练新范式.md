# 大语言模型预训练和后训练新范式

> <https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training>
>
> 翻译 <feuyeux@gmail.com>

大型语言模型 (LLM) 的发展已经走过了漫长的道路，从早期的 GPT 模型到今天复杂的开放权重 LLM。最初，LLM 训练过程仅专注于预训练(pre-training)，但后来扩展到了预训练和后训练(post-training)。ChatGPT 中普及了后训练，后训练通常包括监督指令微调(supervised instruction fine-tuning)和对齐(alignment)。

自 ChatGPT 首次发布以来，训练方法不断发展。本文回顾了预训练和后训练方法的最新进展，特别是最近几个月取得的进展。

![img](LLM流水线.png)

LLM开发和训练流水线概述，重点关注本文讨论的预训练和后训练新方法

每个月都有数百篇的LLM论文在提出新的技术和方法，本文关注其中最近几个月的四篇关于预训练和后训练流水线的最新技术报告。

1. Alibaba Qwen 2
2. Apple Intelligence Foundation(AFM)
3. Google Gemma 2
4. Meta AI Llama 3.1

这些模型根据各自技术论文在 arXiv.org 上的发布日期按顺序呈现，恰好是字母序。

## 1 Alibaba Qwen 2

我们从[Qwen2](https://arxiv.org/abs/2407.10671)开始，Qwen2是一个非常强大、具有竞争力的 LLM 模型系列，但由于某种原因，它不如 Meta AI、微软和 Google 的开放权重模型那么流行。

### 1.1 Qwen2概述

在进入[Qwen2技术报告](https://arxiv.org/abs/2407.10671)的预训练和后训练方法之前，我们先简单总结其中的一些核心规范。

Qwen2模型有 5 种类型，4 种常规LLM(dense)，参数大小分别为 5 亿(0.5 billion)、15 亿(1.5 billion)、70 亿和 720 亿。此外，还有一个570 亿个参数、140 亿个激活参数(14 billion parameters are activated)的混合专家模型。

Qwen2大语言模型的一个突出特点是良好的多语言能力(30 种语言)，拥有惊人的 151k(151,642) token 词汇。作为参考：Llama2使用 32k 词汇、Llama 3.1 使用 128k 词汇；根据经验，词汇大小增加2倍将减少2倍的输入token，这特别有助于处理多语言数据和编码，以覆盖标准英语词汇表之外的单词。

下面是 MMLU 基准，Qwen2与稍后介绍的其他 LLM 的简要比较（请注意，MMLU 是一个多项选择基准，因此有其局限性；但是，它仍然是报告 LLM 表现的最流行的方法之一。）

![img](MMLU基准.png)

最新开放权重模型的 MMLU 基准分数（值越高越好）。我从每个模型的官方论文中收集了该图的分数。

### 1.2 Qwen2预训练

Qwen2团队在7万亿个训练令牌(training tokens)上训练了 15 亿、70 亿和 720 亿个参数模型，这是一个合理的规模。相比之下，Llama2模型在2万亿个令牌上进行了训练，Llama 3.1 模型在 15 个令牌上进行了训练。

值得注意的是，5 亿参数模型是在 12 万亿个令牌上进行训练的，但是研究人员没有在比 12 万亿更大的令牌数据集上训练其他模型，因为他们在训练过程中没有观察到任何改进，而且额外的计算成本也是不合理的。

其中的一个重点是改进了数据过滤流水线以删除低质量数据，增强了数据混合以增加数据多样性——稍后在其他模型中将重新讨论这一主题。

他们还使用了上一代 Qwen 模型来合成额外的预训练数据，并且预训练涉及“多任务指令数据……以增强上下文学习和遵循指令的能力。”

此外，他们分两个阶段进行训练：常规预训练，然后是长上下文训练，后者在预训练的最后阶段使用“高质量、长数据”将上下文长度从 4,096 个token增加到 32,768 个。

![img](Qwen2预训练.png)

Qwen2预训练技术总结

 “持续预训练”是指两阶段预训练，研究人员从常规预训练开始，然后进行长上下文持续预训练。

### 1.3 Qwen2后训练

Qwen2团队采用了流行的两阶段后训练方法，从监督指令微调 (SFT) 开始，通过两个epoch的 500,000 个示例实现该方法，该阶段旨在提高模型在预定场景(predetermined scenarios)中的响应准确性。

![img](LLM开发流程.png)

典型的LLM开发流程

在 SFT 之后，他们使用直接偏好优化(direct preference optimization, DPO)对LLM进行人类偏好对齐，他们的术语称其为人类反馈的强化学习(reinforcement learning from human feedback,RLHF)。正如我在几周前的这篇《LLM 预训练和评估奖励模型技巧》中所讨论的那样，SFT+DPO 方法似乎是目前最流行的偏好调整策略，因为与其他方法（例如[PPO](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb)）相比，它更易于使用。

对齐阶段又分为两个阶段。首先，在现有数据集上使用 DPO（离线阶段）；其次，使用奖励模型形成偏好对（在线）。这里，模型在训练过程中生成多个响应，奖励模型在“实时”优化步骤选择首选的响应，这通常也称为“拒绝采样(rejection sampling)”。

为了构建数据集，他们使用现有的语料库并辅以人工打标来确定 SFT 的目标响应，并确定 DPO 所必需的首选(preferred)和拒绝(rejected)的响应。研究人员还合成了人工标注的数据。

此外，该团队还利用LLM生成专门为“高质量文学数据”量身定制的指令-响应对，从而为训练创建了高质量的问答对。

![img](Qwen2后训练.png)

Qwen2后训练技术总结

### 1.4 结论

与前几代 Qwen 类似，Qwen2是一个相对强大的模型。我记得在2023 年 12 月NeurIPS LLM 效率挑战上，大多数获胜方法都涉及 Qwen 模型。

关于 Qwen2的训练流程，值得注意的是，预训练和后训练都使用了合成数据。此外，对数据集过滤（而不是收集尽可能多的数据）的关注也是值得注意的地方之一。在这里，我想说，越多越好，但前提是它符合一定的质量标准。

[从头开始使用DPO进行LLM对齐](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb)

![img](DPO.png)

[LLM 对齐 DPO概述](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb)

## 2 AFM

> **Apple Intelligence Foundation Language Models**

我真的很高兴在 arXiv.org 上看到另一篇技术论文，由 Apple 出品，概述了他们的模型训练。这是一个意想不到但绝对积极的惊喜！

### 2.1 AFM 概述

在[Apple Intelligence Foundation 语言模型](https://arxiv.org/abs/2407.21075)论文中，研究团队概述了设计用于 Apple 设备上“Apple Intelligence”环境中的两个主要模型的开发。

该论文描述了 AFM 的两个版本：一个包含 30 亿参数的设备模型(on-device model)，旨在部署在手机、平板电脑或笔记本电脑上，特别是一个功能更强大、尺寸未指定的服务器模型。

这些模型是为聊天、数学和编码任务而开发的，尽管本文没有讨论任何特定于编码的训练和功能。

与 Qwen2一样，AFM 是密集(dense) LLM，没有采用专家混合方法。

### 2.2 AFM 预训练

我要给研究人员点两个赞。首先，他们使用公开数据和版权数据，避免抓取网站上的 robots.txt 文件。其次，他们还提到他们使用基准数据进行了净化工作。

研究人员提到质量比数量重要得多。设备模型的词汇量为 49k 个 token，服务器模型的词汇量为 100k 个 token，比使用 150k token词汇的 Qwen2模型，词汇量明显较小。这意味着，预训练不是分两个阶段进行，而是分三个阶段进行！

1. 核心（常规）预训练
2. 持续预训练，降低网络爬取数据(低质量)的权重，提高数学和代码的权重；
3. 使用较长序列数据和合成数据进行上下文延长

![img](AFM预训练过程.png)

AFM 模型经历的 3 步预训练过程概述

#### 2.2.1 预训练 I: 预训练

核心预训练描述了Apple 预训练流水线的第一个预训练阶段。 这类似于常规预训练，其中 AFM 服务器模型使用 6.3 万亿(6.3 trillion)个token、批量大小为 4096、4096 token 序列长度进行训练。这与 Qwen2模型非常相似，后者是用7万亿个令牌进行训练的。

然而，AFM-on-device 模型更有趣的地方是，它从一个更大的 64 亿参数模型中蒸馏(distilled)和修剪(pruned)的。

关于蒸馏过程的细节，只提到了“蒸馏损失使用真实标签和教师模型的 top-1 预测值（教师标签的权重为 0.9）的凸组合(convex combination)，来替换目标标签”。

知识蒸馏对于 LLM 预训练变得越来越普遍和有用（Gemma-2 也使用它），这里简要概述下这一过程的运作方式。

![img](知识蒸馏.png)

知识蒸馏概述：使用原始训练token加上较大教师模型（此处为 6.4B 模型）的输出，训练小型模型（此处为 AFM 设备 3B 模型）。请注意，a交叉熵损失是用于预训练 LLM 的常规训练损失。

如上所述，知识蒸馏仍然涉及在原始数据集上训练。但是，除了数据集中的训练token之外，要训练的模型（称为学生）还从较大的（教师）模型接收信息。与没有知识蒸馏的训练相比，它提供了更丰富的信号。缺点是：1）首先训练更大的教师模型，2）使用更大的教师模型计算所有训练token的预测值来预测。这些预测可以提前计算，这需要大量的存储空间；或在训练期间，这可能会减慢训练过程。

#### 2.2.2 预训练 II: 持续预训练

持续预训练阶段在 1 万亿个token的数据集上，将上下文从 4,096 个token延长到 8,192 个。但是，重点是进行高质量的数据混合——数学和代码。

研究人员发现，在这种情况下，蒸馏损失并没有什么收益。

#### 2.2.3 预训练 III: 上下文延长

第三个预训练阶段只用到 1000 亿个token（第二阶段使用的token的 10%），将上下文延长至 32,768 个token。为了实现这一目标，研究人员使用合成的长上下文问答数据增强了数据集。

![img](AFM预训练.png)

AFM 预训练技术总结

### 2.3 AFM 后训练

Apple 似乎在后训练过程中采取了与预训练类似的综合方法。他们利用了人工注释和合成数据，强调数据质量优先于数量，他们不依赖于预先确定的数据。相反，他们通过多次实验对数据混合进行微调，以达到最佳平衡。

后训练阶段包含两个阶段：监督指令微调，然后是几轮人类反馈的强化学习(RLHF)。特别值得注意的是 Apple 为 RLHF 阶段引入了两种新算法：

1. 使用教师委员会(Teacher Committee,iTeC)进行拒绝采样微调(Rejection Sampling Fine-tuning)
2. 带有镜像下降策略优化(Mirror Descent Policy Optimization)的 RLHF

简述如下：

iTeC 算法将拒绝采样与多种偏好调整技术(SFT、DPO、IPO 和在线 RL)相结合。Apple 训练模型不是依赖单一算法，而是每种方法独立训练。这些模型生成响应，人类对其进行评估打标(偏好标签)。偏好数据用于在 RLHF 框架中迭代训练奖励模型。在拒绝抽样阶段，模型委员会生成多个响应，奖励模型选择最佳响应。

这种基于委员会的方法相当复杂，但应该相对可行，特别是考虑到所涉及的模型规模相对较小(大约 30 亿个参数)。使用更大的模型(例如 Llama 3.1 中的 70B 或 405B 参数模型)来实现这样的委员会，肯定会更有挑战。

至于第二种算法，带有镜像下降策略优化(Mirror Descent Policy Optimization)的 RLHF，之所以选择它是因为它被证明比常用的近端策略优化(Proximal Policy Optimization,PPO)更有效。

![img](AFM后训练.png)

AFM 后训练技术总结

### 2.4 结论

Apple的预训练和后训练方法相对全面，可能是因为风险非常高(该模型部署在数百万甚至数十亿台设备上)。

然而，考虑到这些模型的规模较小，许多技术也变得可行，因为3B（即30亿参数）模型的规模还不到最小的Llama 3.1模型的一半。

亮点之一是，这不是 RLHF 和 DPO 之间的简单选择；相反，他们以委员会的形式使用了多种偏好调整算法。

同样有趣的是，他们明确使用问答数据作为预训练的一部分。总而言之，这是一份令人耳目一新的技术报告。

## 3 Google Gemma 2

Google 的 Gemma 模型的最新描述在 [Gemma 2: Improving Open Language Models at a Practical Size](https://arxiv.org/abs/2408.00118)中。在预训练和后训练过程之前，我们首先简述一些关键事实。

### 3.1 Gemma2概述

Gemma2模型有三种大小：20 亿、90 亿和 270 亿参数，主要关注的是探索的技术是不一定需要增加训练数据集规模，而是开发相对较小且高效的 LLM。

值得注意的是，Gemma2的词汇量很大，256k 个token，相比之下，Llama2使用 32k ，Llama 3 使用 128k。

此外，Gemma2采用滑动窗口注意力机制，类似于 Mistral 的早期模型，可能会降低内存成本。

### 3.2 Gemma2预训练

Gemma 的研究人员认为，即使是小型模型也常常训练不足。然而，他们不是简单地增加训练数据集的大小，而是专注于通过替代方法(例如知识蒸馏)来保持质量并实现改进，这与苹果的方法类似。

虽然 27B Gemma2模型是从头开始训练的，但较小的模型是使用类似于 Apple 之前解释的方法的知识蒸馏来训练的。

27B 模型使用 13 万亿个 token 进行训练，9B 模型使用 8 万亿个 token 进行训练，2B 模型使用2万亿个 token 进行训练。此外，与 Apple 的方法类似，Gemma 团队优化了数据混合以提高性能。

![img](Gemma2预训练.png)

Gemma2预训练技术总结

### 3.3 Gemma2后训练

Gemma 模型的后训练过程，包括典型的监督微调 (SFT) 和人类反馈强化学习 (RLHF)两个阶段。

指令数据(instruction data)包括使用纯英语提示(prompt)对，这些提示对是人类生成和合成生成内容的混合，具体而言，响应主要由教师模型生成，并且在 SFT 阶段也应用了知识蒸馏。

他们的 RLHF 方法的一个有趣的方面是，SFT 之后，用于 RLHF 的奖励模型比策略（目标）模型大十倍。

Gemma 使用的 RLHF 算法相当标准，但有一个独特的特点(twist)：他们通过一种称为 WARP 的方法对策略模型进行平均，该方法是权重平均奖励模型(weight-averaged reward models, WARM)的后继者 。

![img](Gemma2后训练.png)

Gemma2后训练技术总结

### 3.4 结论

Gemma团队似乎真的在知识蒸馏上加倍下注，他们在预训练和后训练阶段都使用这种方法，类似于 Apple。有趣的是，他们并没有采用多阶段预训练方法，或者至少，他们没有在他们的论文中详细说明这一点。

## 4 Meta AI Llama 3.1

Llama LLM 的新发布总是大事件，这次发布附带了一份 92 页的[技术报告](https://arxiv.org/abs/2407.21783)。

### .1 Llama 3.1 概述

在发布庞大的 4050 亿参数模型的同时，Meta 还更新了之前的 80 亿和 700 亿参数模型，使 MMLU 性能略有提升。

![img](Llama3MMLU基准.png)

不同模型的MMLU基准性能

虽然 Llama 3 像其他最近的 LLM 一样使用组查询(group query)注意力，但令人惊讶的是，Meta AI 对滑动窗口注意力和专家混合方法说不。换句话说，Llama 3.1 看起来非常传统，重点显然是预训练和后训练，而不是架构创新。

与之前的 Llama 版本类似，权重是公开可用的。此外，Meta 表示他们更新了 Llama 3 许可证，因此现在终于可以（允许）使用 Llama 3 进行合成数据生成或知识蒸馏来改进其他模型。

### 4.2 Llama 3.1 预训练

Llama 3 在 15.6 万亿个token数据集上进行了训练，这比 Llama2的 1.8 万亿个token数据集有大幅增长。研究人员表示，它支持至少 8 种语言。

Llama 3 的词汇量为 128,000，它是使用 OpenAI 的 tiktoken tokenizer 开发的。

在预训练数据质量控制方面，Llama 3采用了基于启发式的过滤和基于模型的质量过滤，使用Meta AI的fastText和基于RoBERTa的分类器这样的快速分类器。这些分类器还有助于确定训练期间使用的数据混合的上下文类别。

Llama 3 的预训练分为三个阶段：第一阶段使用 15.6 万亿个token和 8k 上下文窗口进行标准初始预训练，第二阶段持续预训练，将上下文长度扩展至 128k。最后阶段涉及退火(annealing)，这进一步增强了模型的性能，让我们在下面更详细地研究这些阶段。

#### 4.2.1 预训练 I: 标准（初始）预训练

在他们的训练中，从包含 400 万个token的批次开始，每个批次的序列长度为 4096。这意味着批次大小约为 1024 个token，假设在处理第一个数字后将 400 万个数字四舍五入到最接近的数字。 2.52 亿个token，他们将序列长度加倍到 8192。在训练过程中，在 2.87 万亿个token之后，他们再次将批量大小加倍。

此外，研究人员并没有在整个训练过程中保持数据组合不变，而是调整了训练过程中使用的数据组合，以优化模型的学习和性能。这种动态数据处理方法可能有助于提高模型对不同类型数据的泛化能力。

#### 4.2.2 预训练 II: 上下文加长的持续预训练

与单阶段增加上下文窗口的模型相比，Llama 3.1 上下文延长是一种更为渐进的方法：在这里，研究人员通过六个不同的阶段将上下文长度从 8,000 个标记增加到 128,000 个标记，这种逐步增量可能允许模型实现。更顺利地适应更大的环境。

此过程使用的训练集涉及 8000 亿个token，约占数据集总大小的 5%。

#### 4.2.3 预训练 III: 高质量数据退火

在第三个预训练阶段，研究人员在小型但高质量的混合数据上训练了模型，他们发现这有助于提高基准数据集的性能。例如，在 GSM8K 和 MATH 训练集上进行退火可以显着提高模型的性能。

在论文第 3.1.3 节中，研究人员表示，退火数据集大小为 400 亿个token（占数据集总大小的 0.02%）；他们在第 3.4.3 节中表示，这个 40B 退火数据集用于评估数据质量。实际退火仅针对 4000 万个token（退火数据的 0.1%）进行。

![img](Llama3.1预训练.png)

Llama 3.1 预训练技术总结

### 4.3 Llama 3.1 后训练

对于后训练过程，Meta AI 团队采用了一种相对简单的方法，包括监督微调 (SFT)、拒绝采样和直接偏好优化 (DPO)。

他们观察到，与这些技术相比，带有 PPO 的 RLHF 等强化学习算法稳定性较差，规模化更具挑战性。值得注意的是，SFT 和 DPO 步骤是在多轮中迭代重复的，并结合了人类生成的数据和合成数据。

在描述更多细节之前，他们的工作流程如下图所示。

![img](Llama3.1后训练.png)

Llama 3.1 论文中的注释图描述了后训练过程

请注意，尽管他们使用了 DPO，但他们也像在 RLHF 中那样开发了一个奖励模型。最初，他们使用预训练阶段的检查点并利用人工注释的数据来训练奖励模型。用于拒绝采样过程，帮助选择适当的提示进行进一步训练。

在每一轮训练中，他们不仅将模型平均技术应用于奖励模型，还应用于 SFT 和 DPO 模型。这种平均涉及合并最近和以前模型的参数，以随着时间的推移稳定（并提高）性能。

总而言之，它的核心是一个相对标准的 SFT + DPO 阶段，但是，这个阶段会重复多轮，然后，他们还使用了拒绝采样的奖励模型（类似 Qwen2和 AFM）。他们也像Gemma一样使用了模型平均技术；然而，这不仅仅用于奖励模型，而是涉及所有参与的模型。

![img](Llama3.1后训练2.png)

Llama 3.1 后训练技术总结

### 4.4 Conclusion

Llama 3 模型仍然相当标准，与早期的 Llama2模型相似，但具有一些有趣的方法，值得注意的是，15 万亿个token训练集将 Llama 3 与其他模型区分开来。

与其他最近的大型语言模型相比，Llama 3 没有采用知识蒸馏技术，而是选择了更直接的模型开发路径。对于后训练，该模型利用直接偏好优化（DPO）而不是更复杂的强化学习策略。总体而言，这种选择很有趣，因为它表明专注于通过更简单（但经过验证）的方法来提高 LLM 性能。

## 5 主要要点

我们可以从本文讨论的四个模型：Alibaba Qwen 2、AFM、Google Gemma2和 Meta Llama 3 中学到什么？

所有四种模型都采用了一些不同的方法来进行预训练和后训练，当然，方法是重叠的，但对于预训练来说，一个共同的特点似乎是所有方法都使用多阶段。预训练流水线，其中一般核心预训练之后是上下文延长，有时是高质量退火步骤，下图再次显示了预训练中采用的不同方法。

![img](预训练所用技术.png)

预训练所用技术概述

在后训练阶段，似乎没有一个流程是完全相同的。看起来，拒绝采样现在已成为后训练过程中的常见元素。然而，当涉及到DPO或RLHF时，目前还没有达成共识或偏好。

![img](后训练所用技术.png)

后训练所用技术概述

总的来说，开发高性能的大型语言模型（LLMs）并没有统一配方，而是有多种途径。

最后，这四种模型的表现大致在同一水平范围内。不幸的是，其中一些模型尚未进入 LMSYS 和 AlpacaEval 排行榜，因此除了 MMLU 等多项选择基准测试的分数外，我们最终还没有直接比较。

## 扩展资料

<https://www.youtube.com/watch?v=kPGTx4wcm_w>

<https://sebastianraschka.com/pdf/slides/2024-build-llms.pdf>
