### 逻辑回归(Logistic Regression)

<https://www.bilibili.com/video/BV1Eb4y1W7XG?p=42>-<https://www.bilibili.com/video/BV1Eb4y1W7XG?p=43>
分类还是回归？逻辑回归是经典的二分算法

#### Sigmoid函数

$g(z)=\frac{1}{1+e^{-z}}$

分类转换：值->概率

预测函数

$h_\theta(x)=g()\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$

分类任务

$P(y|x;\theta)=(h_\theta(x))^{y}(1-h_\theta(x))^{1-y}$

- y=0 时 $(1-h_\theta(x))^{1-y}$
- y=1 时 $(h_\theta(x))^y$

似然函数

$L(\theta)=\displaystyle \prod_{i=1}^m P(y_i|x_i;\theta)=\displaystyle \prod_{i=1}^m (h_\theta(x_i))^{y_i}(1-h_\theta(x_i))^{1-y_i}$

对数似然

$l(\theta)=logL(\theta)=\displaystyle \sum_{i=1}^m ({y_i}h_\theta(x_i)+(1-y_i)log(1-h_\theta(x_i)))$



$L=\frac{\displaystyle \sum_{i=1}^N \alpha \cdot {T99}_i + \beta \cdot {T999}_i}{N}$

梯度下降

$J(\theta)= - \frac{1}{m}l(\theta)$

参数更新

$\theta_j:=\theta_j-\alpha\frac{1}{m}\displaystyle\sum_{i=1}^m(h_\theta(x_i)-y_i)x_i^j$

多分类 softmax

#### 编程
<https://www.bilibili.com/video/BV1Eb4y1W7XG?p=44>-<https://www.bilibili.com/video/BV1Eb4y1W7XG?p=55>

##### 分类训练 
[logistic_regression.py](logistic_regression/logistic_regression/logistic_regression.py)
##### 线性决策
鸢尾花分类 [logistic_regression_with_linear_boundary.py](logistic_regression/logistic_regression/logistic_regression_with_linear_boundary.py)
##### 非线性决策
[NonLinearBoundary.py](logistic_regression/logistic_regression/NonLinearBoundary.py)