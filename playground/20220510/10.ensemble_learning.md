### 集成学习(ensemble learning)
<https://www.bilibili.com/video/BV1Eb4y1W7XG?p=104>-<https://www.bilibili.com/video/BV1Eb4y1W7XG?p=107>

- 目的:让机器学习效果更好，单个不行，群殴走起
- Bagging:训练多个分类器取平均 $f(x)\frac{1}{M}\displaystyle \sum_{m=1}^Mf_m(x)$
- Boosting:从弱学习器开始加强，通过加权来进行训练 $F_m(x)=F_{m-1}(x)+argmin_h \displaystyle \sum_{i=1}^nL(y_i,F_{m-1}(x)+h(x_i))$
(加入一棵树，要比原来强) 
- Stacking:聚合多个分类或回归模型(可以分阶段来做)

#### Bagging模型
全称: bootstrap aggregation(说白了就是并行训练一堆分类器) 

最典型的代表就是**随机森林**啦
- 随机:数据采样随机，特征选择随机
- 森林:很多个决策树**并行**放在一起

保证树的多样性，以使平均有意义 -> 样本随机、特征随机 ->
 
由于二重随机性，使得每个树基本上都不会一样，最终的结果也会不一样

之所以要进行随机，是要保证泛化能力，如果树都一样，那就没意义了!

随机森林优势
- 它能够处理很高维度(feature很多)的数据，并且不用做特征选择
- 在训练完后，它能够给出哪些feature比较重要
- 容易做成并行化方法，速度比较快
- 可以进行可视化展示，便于分析 —— 相比而言 深度学习是黑盒 看不到过程 不好证明

KNN就不太适合，因为很难去随机让泛化能力变强!

理论上越多的树效果会越好，但实际上基本超过一定数量就差不多上下浮动了

#### Boosting模型
梯升算法 典型代表:**AdaBoost**， **Xgboost**

Adaboost会根据前一次的分类效果调整数据权重 

解释:如果某一个数据在这次分错了，那么在下一次我就会给它更大的权重
 
最终的结果:每个分类器根据自身的准确性来确定各自的权重，再合体

Adaboost工作流程
- 每一次切一刀! 
- 最终合在一起 
- 弱分类器这就升级了!

#### Stacking模型
- 堆叠:很暴力，拿来一堆直接上(各种分类器都来了) 
- 可以堆叠各种各样的分类器(KNN,SVM,RF等等) 
- 分阶段:第一阶段得出各自结果，第二阶段再用前一阶段结果训练
- 为了刷结果，不择手段!


堆叠在一起确实能使得准确率提升，但是速度是个问题 集成算法是竞赛与论文神器，当我们更关注于结果时不妨来试试!