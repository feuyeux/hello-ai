<!-- markdownlint-disable MD033 -->

# 线性回归(Linear Regression)算法

唐宇迪

- 理论：<https://www.bilibili.com/video/BV1Eb4y1W7XG?p=3>-<https://www.bilibili.com/video/BV1Eb4y1W7XG?p=9>
- 编程：<https://www.bilibili.com/video/BV1Eb4y1W7XG?p=10>-<https://www.bilibili.com/video/BV1Eb4y1W7XG?p=18>

## 理论

数据：

|       | 特征1(工资) | 特征2(年龄) | 标签(额度) |
| ----- | ----------- | ----------- | ---------- |
| 样本1 | 4000        | 25          | 20000      |
| 样本2 | 8000        | 30          | 70000      |
| 样本3 | 5000        | 28          | 35000      |

目标：预测银行会贷款给我多少钱(标签)

考虑：工资和年龄都会影响最终贷款额度，那么它们各自有多大影响呢？(参数)

----

描述x与y的关系的方程——**拟合平面**：

$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2(\theta_0是偏置项，使h_\theta上下浮动；其他项是权重项)$

为了转换为矩阵，需要进行转换——整合：

$h_\theta(x)=\displaystyle \sum_{i=0}^{n} \theta_ix_i=\theta^Tx$

误差

真实值=预测值+误差($\epsilon$)  对于每个样本：$y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}$

data+loss function(损失函数)

误差$\epsilon^{(i)}$是独立、具有相同分布，且服从均值($\mu$)为0、方差为$\sigma^2$的高斯分布

- 样本独立：数据不能有序或者处于同一类型，要进行shuffle
- 同分布：张三李四都是来这家银行贷款的
- 高斯分布(Gaussian distribution)：$p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})$ 即正态分布(Normal distribution)。均值为0=某些多贷款+某些少贷款，浮动不会很大

$p(y^{(i)}|x^{(i)}:\theta)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$

我们希望x与$\theta$的组合成为y的概率越大越好

### 似然函数(likelihood function)

 $L(\theta)=\displaystyle \prod_{i=1}^{m} p(y^{(i)}|x^{(i)}:\theta)=\displaystyle \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$

什么样的参数与<u>我们的数据</u>组合后恰好是真实值呢？

对数似然

$logL(\theta)=log\displaystyle \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$

乘法难解，转换成加法就容易了

展开化简

$\displaystyle \sum_{i=1}^m \vdots log \frac{1}{\sqrt{2\pi}\sigma}\vdots \exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\vdots$

=$\vdots mlog \frac{1}{\sqrt{2\pi}\sigma} \vdots \frac{1}{\sigma^2} \cdot \frac{1}{2}\displaystyle\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2\vdots$

让似然函数越大越好 极大值点

$J(\theta)=\frac{1}{2}\displaystyle\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2$ （**最小二乘法**） 越小越好

### 目标函数

$J(\theta)=\frac{1}{2}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2=\frac{1}{2}(X\theta-y)^T(X\theta-y)$

x是数据，是矩阵，矩阵的平方等于与其转置的乘积

希望最小(方向)，即对$\theta$求偏导，偏导为0时为最小极值点

$ \nabla_\theta J(\theta)=\nabla_\theta(\frac{1}{2}(X\theta-y)^T(X\theta-y))$

=$\nabla_\theta(\frac{1}{2}(\theta^TX^T-y^T)(X\theta-y))$

=$\nabla_\theta(\frac{1}{2}(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^Ty))$

> 存在$X^TX$，X必然是一个对称矩阵，$\nabla_\theta X^TX\theta=2X^TX\theta$

=$\frac{1}{2}(2X^TX\theta-X^Ty-(y^TX)^T)$

=$X^TX\theta-X^Ty$

> 矩阵与其逆矩阵相乘为1

$\theta=(X^TX)^{-1}X^Ty$

### 梯度下降(Gradient Descent)

我们有了目标函数后，如何进行求解？优化求解 无法直接求解

ML的套路是交给机器一堆数据，并告诉它什么样的学习方式是对的(目标函数)，然后让它朝着这个方向去做

目标函数：$J(\theta_0,\theta_1)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})$

从每一个$\theta$寻找山谷最低点，即目标函数的终点

$J(\theta)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(y^{(i)}-h_\theta(x^{(i)}))^2$

> 增加平方项，让结果差异更大

#### 批量梯度下降

$\frac{\partial J(\theta)}{\partial \theta_j}=-\frac{1}{m}\displaystyle \sum_{i=1}^{m}(y^i-h_\theta(x^i))x_j^i$

> i 样本 j 列

$\theta_j^{'}=\theta_j+\frac{1}{m}\displaystyle \sum_{i=1}^{m}(y^i-h_\theta(x^i))x_j^i$

每次都朝着下降方向走，容易得到最优解，但由于每次考虑所有样本，速度很慢(如果m=100w)

#### 随机梯度下降

$\theta_j^{'}=\theta_j+(y^i-h_\theta(x^i))x_j^i$

每次找一个样本，迭代速度快，但不一定每次朝着收敛方向走(存在离群点/噪音点)

**小批量(mini batch)梯度下降法**：$\theta_j^{'}=\theta_j-\alpha\frac{1}{64}\displaystyle \sum_{k=i}^{i+63}(h_\theta(x^k)-y^k)x_j^k$

每次更新选择一小部分数据来算

学习率$\alpha$(步长) LR=0.001

## 编程

### 计算幸福指数

单特征：`linear_regression/UnivariateLinearRegression.ipynb`

多特征：`linear_regression/MultivariateLinearRegression.ipynb`

非线性回归：`linear_regression/Non-linearRegression.ipynb`

1. 训练集-测试集
   - 初始化 *init* `linear_regression/linear_regression.py`
     - 预处理 *prepare_for_training* `linear_regression/utils/features/prepare_for_training.py`

2. 训练 *train* `linear_regression/linear_regression.py`

3. 预测

----

## 附录

### the-different-of-likelihood-and-probability

概率(probability)描述了给定模型参数后，描述结果的合理性，而不涉及任何观察到的数据。

> 抛一枚均匀的硬币，拋20次，问15次拋得正面的可能性有多大？
>
> - 这里的可能性就是”概率”
> - 均匀的硬币就是给定参数$\theta=0.5$
> - “拋20次15次正面”是观测值$O$
> - 求概率$P (H=15 | \theta=0.5) = ？$的概率。

似然(likelihood)描述了给定了特定观测值后，描述模型参数是否合理。

> 拋一枚硬币，拋20次，结果15次正面向上，问其为均匀的可能性？
>
> - 这里的可能性就是”似然”
> - “拋20次15次正面”为观测值$O$为已知
> - 参数$\theta=?$并不知道
> - 求$L(\theta | H=15) = P (H=15 | \theta=0.5)$的最大化下的$\theta$ 值。

$P(O|\theta)$ 输入有两个：$O$表示某一个具体的数据；$\theta$表示模型的参数。

- 如果$\theta$是已知确定的，$O$是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本$O$，其出现概率是多少。
- 如果$O$是已知确定的，$\theta$是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现x这个样本点的概率是多少。

### 非线性拟合

<https://www.bilibili.com/video/BV1Eb4y1W7XG?p=19>

线性——特征变换——非线性——曲线(需要注意过拟合问题)

- polynomial 多项式特征变换 linear_regression/utils/features/generate_polynomials.py
- sinusoidal 正弦波特征变换 linear_regression/utils/features/generate_sinusoids.py
