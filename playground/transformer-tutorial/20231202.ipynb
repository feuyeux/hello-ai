{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)\n",
    "                             * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(\n",
    "            d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "                len(src)).to(device)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Load and batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(\n",
    "    map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long)\n",
    "            for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "\n",
    "# ``train_iter`` was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "# shape ``[seq_len, batch_size]``\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to generate input and target sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Initiate an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        output = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        loss = criterion(output_flat, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 68.72 | loss  8.22 | ppl  3706.36\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 17.41 | loss  6.92 | ppl  1012.97\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 17.52 | loss  6.48 | ppl   650.85\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 17.60 | loss  6.32 | ppl   554.51\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 17.53 | loss  6.20 | ppl   493.39\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 17.58 | loss  6.16 | ppl   474.73\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 17.64 | loss  6.12 | ppl   454.05\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 17.79 | loss  6.11 | ppl   451.49\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 17.87 | loss  6.02 | ppl   412.96\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 17.67 | loss  6.02 | ppl   411.97\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 17.83 | loss  5.89 | ppl   362.74\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 17.73 | loss  5.97 | ppl   392.08\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 17.93 | loss  5.95 | ppl   382.71\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 18.01 | loss  5.88 | ppl   357.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 65.23s | valid loss  5.81 | valid ppl   334.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 17.78 | loss  5.87 | ppl   355.66\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 17.65 | loss  5.86 | ppl   349.74\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 17.53 | loss  5.67 | ppl   291.35\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 17.63 | loss  5.71 | ppl   300.55\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 17.54 | loss  5.66 | ppl   286.20\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 17.55 | loss  5.69 | ppl   295.94\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 17.56 | loss  5.70 | ppl   297.84\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 17.67 | loss  5.72 | ppl   305.98\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 17.52 | loss  5.67 | ppl   289.32\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 17.63 | loss  5.68 | ppl   291.90\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 17.69 | loss  5.56 | ppl   258.81\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 17.69 | loss  5.66 | ppl   285.89\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 17.68 | loss  5.66 | ppl   287.85\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 17.55 | loss  5.60 | ppl   269.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 54.38s | valid loss  5.68 | valid ppl   291.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 17.70 | loss  5.61 | ppl   273.96\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 17.68 | loss  5.64 | ppl   281.59\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 17.52 | loss  5.44 | ppl   230.41\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 17.59 | loss  5.50 | ppl   243.79\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 17.43 | loss  5.45 | ppl   232.00\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 17.60 | loss  5.49 | ppl   241.11\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 17.51 | loss  5.50 | ppl   244.84\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 17.49 | loss  5.53 | ppl   251.40\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 17.54 | loss  5.48 | ppl   239.52\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 17.54 | loss  5.49 | ppl   243.27\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 17.60 | loss  5.37 | ppl   214.25\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 17.62 | loss  5.47 | ppl   237.40\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 17.59 | loss  5.48 | ppl   239.25\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 17.73 | loss  5.41 | ppl   224.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 54.22s | valid loss  5.61 | valid ppl   272.54\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "              f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    # load best model states\n",
    "    model.load_state_dict(torch.load(best_model_params_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Evaluate the best model on the test datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.51 | test ppl   247.69\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | ' f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
