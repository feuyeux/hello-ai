{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# huggingface gpt2-large\n",
    "\n",
    "# git lfs install\n",
    "# git clone https://huggingface.co/gpt2-large\n",
    "# git clone https://gitee.com/hf-models/gpt2-large\n",
    "\n",
    "# pretrained_model_name_or_path = \"d:/datasets/huggingface/gpt2-large\"\n",
    "\n",
    "pretrained_model_name_or_path = \"gpt2-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'Benefits of Sleeping Early'\n",
    "# topic = 'The disadvantages of going to bed late'\n",
    "input_ids = tokenizer.encode(topic, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Blog\n",
    "# max_lenth-Number of Words in the Article\n",
    "# num_beams-Number of different combination of words that can be chained together\n",
    "# no_repeat_ngram_size-No of words that be combined together and repeated, example: ['benefits of sleeping' can be repeated 2 times but not more ]\n",
    "\n",
    "output = model.generate(input_ids, max_length=100, num_beams=30,\n",
    "                        no_repeat_ngram_size=2, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = output[0]\n",
    "print(tokenizer.decode(text, skip_special_tokens=True))\n",
    "\n",
    "article_en = tokenizer.decode(text, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "# https://huggingface.co/facebook/mbart-large-50-one-to-many-mmt\n",
    "\n",
    "mmt_pretrained_model_name_or_path = \"facebook/mbart-large-50-one-to-many-mmt\"\n",
    "\n",
    "# https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt\n",
    "# mmt_pretrained_model_name_or_path = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "# mmt_pretrained_model_name_or_path = \"d:/datasets/facebook/mbart-large-50-many-to-many-mmt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\n",
    "    mmt_pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MBart50TokenizerFast.from_pretrained(\n",
    "    mmt_pretrained_model_name_or_path, src_lang=\"en_XX\")\n",
    "\n",
    "model_inputs = tokenizer(text, return_tensors=\"pt\", max_length=100, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate from English to Chinese\n",
    "generated_tokens = model.generate(\n",
    "    **model_inputs,\n",
    "    forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    ")\n",
    "translation = tokenizer.batch_decode(\n",
    "    generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate from English to French\n",
    "generated_tokens = model.generate(\n",
    "    **model_inputs,\n",
    "    forced_bos_token_id=tokenizer.lang_code_to_id[\"fr_XX\"]\n",
    ")\n",
    "translation = tokenizer.batch_decode(\n",
    "    generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
