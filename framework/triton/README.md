# Triton Inference Server

## Quick start

### Server

<https://github.com/triton-inference-server>

![Triton Architecture](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/_images/arch.jpg)

Framework backends

1. [TensorRT](https://github.com/triton-inference-server/tensorrt_backend)
2. [TensorFlow](https://github.com/triton-inference-server/tensorflow_backend)
3. [PyTorch](https://github.com/triton-inference-server/pytorch_backend)
4. [Python](https://github.com/triton-inference-server/python_backend)
5. [ONNX Runtime](https://github.com/triton-inference-server/onnxruntime_backend)
6. [OpenVino](https://github.com/triton-inference-server/openvino_backend)

install the [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-docker) to use a GPU for inference

[quickstart](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html)

<https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver>

### Client

<https://github.com/triton-inference-server/client/tree/main/src/java>
